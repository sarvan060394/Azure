 Delete Stale Pods API ‚Äì User Guide
‚úÖ Purpose
In large OpenShift clusters, many stale pods remain due to job completions, crashes, or other issues. These unused pods can clutter the system or take up unnecessary resources.

To fix this, we‚Äôve developed a "Delete Stale Pods API" ‚Äî a smart cleanup system that:

Lists stale pods by their status

Identifies pod ownership (ReplicaSet, Deployment, etc.)

Deletes safely, without breaking running applications

üë©‚Äçüíª How It Works (User Perspective)
As a user, you can:

Access the FastAPI Web UI

Select pod statuses you want to clean up:

Examples: CrashLoopBackOff, Completed, Error, Terminating, Pending, Evicted (All statuses supported)

Choose namespaces to target

The API automatically filters only non-prod namespaces based on the label: env_type=nonprod

Review the list of matching stale pods

Confirm deletion to start cleanup

The API will:

Trace pod ownership (RS/RC ‚Üí Deployment/DC)

Delete associated resources only if safe

Log everything clearly for traceability



üß¨ Key Logic Scenarios Handled
Case 1: Pod has no owner
The pod is not part of any controller.

Action: Pod is deleted directly.

Case 2: Pod is owned by a ReplicaSet or ReplicationController (no higher controller)
RS/RC manages the pod, but no Deployment or DC manages the RS/RC.

Action: Pod and RS/RC are both deleted.

Case 3: Pod is owned by ReplicaSet ‚Üí Deployment
If the Deployment still has healthy running pods:

Skip deletion to avoid breaking the app

If no other pods are running:

Delete the Deployment

Delete the RS

Delete the pod

Case 4: Multiple stale pods from the same Deployment
Only one delete operation is triggered for the Deployment.

Duplicates are tracked and skipped using in-memory sets.

Avoids noisy Kubernetes errors like 404 Not Found

 Key Challenges & Bottlenecks
1. Complex Ownership Chains
In Kubernetes, pods are often part of a chain:

scss
Copy
Edit
Pod ‚Üí ReplicaSet ‚Üí Deployment (or Pod ‚Üí RC ‚Üí DC)
The API navigates this chain automatically, so users don‚Äôt have to.
------------------------------------------------

Stale Image Pruner API: Performance Improvement Strategy

üß≠ Overview

The Stale Image Pruner API is responsible for listing and deleting container images (with all tags) from JFrog Artifactory and identifying all OpenShift resources using those images. While functional, the current implementation suffers from significant performance issues, especially when handling large workloads across clusters.

This document outlines the performance bottlenecks observed, their root causes, and a detailed optimization plan to address them.

‚ö†Ô∏è Current Bottlenecks

1. Frequent and Redundant API Calls

Issue: The API repeatedly queries both JFrog and OpenShift for image and pod details, often for the same image or pod.

Impact: Excessive latency and unnecessary network traffic.

2. Re-authentication to OpenShift

Issue: The OpenShift authentication token and kubeconfig are fetched multiple times per request.

Impact: Wastes processing time and introduces redundant operations.

3. Linear Resource Traversal in OpenShift

Issue: Each pod is individually evaluated to determine its associated RS/DC or Deployment, and the loop is repeated for each image.

Impact: Results in high latency for clusters with many resources.

üîç Observations from Logs

Multiple API hits to JFrog per image and tag.

Frequent fetching of the same OpenShift pod, RS, and deployment info.

OpenShift login succeeds quickly, but image-to-resource matching dominates time.

‚úÖ Optimization Strategies

1. Batch Resource Fetching

What to do:

Fetch all pods, RS, RC, Deployments, and DCs upfront at once.

Store these in-memory dictionaries for quick local access.

Benefits:

Drastically reduces redundant API calls.

Enables rapid lookup and filtering.

Sample Code Logic:

all_pods = list_all_pods()
all_rs = list_all_replicasets()
# Then access via dict: (namespace, name) as key
pod_map = {(pod.metadata.namespace, pod.metadata.name): pod for pod in all_pods}

2. Threading for Parallel Execution

What to do:

Use ThreadPoolExecutor to parallelize:

Image-to-pod usage checks

Pod-to-controller resolution

Benefits:

Utilizes I/O waiting time efficiently

Reduces total duration by parallel processing

3. Local Caching of Ownership Chains

What to do:

Cache pod-to-owner mapping during execution:

owner_cache["pod-namespace"] = { "rs": "rs-name", "deployment": "deployment-name" }

Benefits:

Avoids recomputation for pods sharing same owners.

Streamlines dependency tracing.

4. Consolidate OpenShift Login / Kubeconfig Fetching

What to do:

Authenticate once at the beginning and reuse token and config.

Benefits:

Prevents repeated authentication logic.

Speeds up API calls that follow.
