Monitoring Solutions Documentation
This document describes two solutions for monitoring services using Python. The first solution uses
threads and Redis for state management, and the second solution uses a scheduler to manage the
monitoring jobs.
Solution 1: Using Threads and Redis for State Management
This solution uses Redis to keep track of which services are being monitored and ensures that only
one monitoring task
is running for each service in a deployment. The monitoring task runs in a separate thread and
periodically checks
the service status until it comes back online.
### Steps to Implement:
1. **Install Redis and Python Redis client library:**
 ```bash
 pip install redis
 ```
2. **Set up a Redis instance:**
 - You can set up a Redis instance locally or use a managed Redis service.
3. **Implement the monitoring solution:**
 - **State Management:** Use Redis to store the state of each monitoring job. The key is a
combination of `deployment_id` and `service_type`, and the value is either `active` or `inactive`.
 - **Monitoring Task:** Create a function to run in a separate thread that periodically checks the
service status. If the service is down, it logs a message and continues to monitor.
 - **Start Monitoring Endpoint:** This endpoint starts the monitoring task if it is not already running.
It sets the status in Redis to `active` and starts a new thread for the monitoring task.
 - **Stop Monitoring Endpoint:** This endpoint stops the monitoring task by setting the status in
Redis to `inactive`.
### Benefits:
- This solution is straightforward and does not require additional scheduling libraries.
- It allows for flexible and dynamic state management using Redis.
### Potential Drawbacks:
- Managing threads can become complex with a large number of services.
- Ensuring thread safety and handling exceptions within threads can add complexity

Solution 2: Using Scheduler and Redis for State Management
This solution uses a scheduler to manage the monitoring jobs. Each monitoring job periodically
checks the Redis status
until the service comes back online.
### Steps to Implement:
1. **Install necessary packages:**
 ```bash
 pip install redis flask apscheduler requests
 ```
2. **Set up a Redis instance:**
 - You can set up a Redis instance locally or use a managed Redis service.
3. **Implement the solution with a scheduler:**
 - **Scheduler Setup:** Use the `apscheduler` library to create and manage the scheduled jobs.
 - **State Management:** Use Redis to store the state of each monitoring job. The key is a
combination of `deployment_id` and `service_type`, and the value is either `active` or `inactive`.
 - **Monitoring Job:** Create a function that checks if the Redis service is online. If the Redis
service is online, it sets the job status to `inactive` in Redis and removes the job from the scheduler.
If the Redis service is still down, it logs a message and continues to monitor.
 - **Start Monitoring Endpoint:** This endpoint starts the monitoring job if it is not already running. It
sets the status in Redis to `active` and schedules a new job using the scheduler.
 - **Stop Monitoring Endpoint:** This endpoint stops the monitoring job by setting the status in
Redis to `inactive` and removing the job from the scheduler.
### Benefits:
- The scheduler handles the periodic execution of monitoring tasks, simplifying the implementation.
- Using a scheduler can improve the scalability and manageability of monitoring multiple services.
### Potential Drawbacks:
- Additional dependencies are required (`apscheduler` library).
- Handling the scheduler's lifecycle and job management can add complexity  

---------------------------------------------------------------------------------------


Broadcasting Requests to All Pods in a Kubernetes service
Overview
This document outlines two solutions for broadcasting a request to all pods under a headless
service in Kubernetes. The two approaches are:
1. Sidecar Pattern
2. Message Queue (RabbitMQ)
Both methods ensure that a request to the headless service results in the request being forwarded
to all pods.
Solution 1: Sidecar Pattern
Overview:
The sidecar pattern involves deploying a sidecar container alongside your main application
container. The sidecar handles the broadcasting of requests to all pods in the headless service.
Architecture:
1. Main Application: Receives the initial request and forwards it to the sidecar container.
2. Sidecar Container: Resolves the headless service to get the IPs of all pods and sends the request
to each pod, including a final self-call.
Advantages:
1. Simplicity: The sidecar pattern is straightforward to implement and doesn't require additional
infrastructure.
2. Direct Control: You have direct control over how and when requests are broadcasted.
Broadcasting Requests to All Pods in a service
3. Minimal Dependencies: Only requires Python standard library and a couple of third-party libraries
(Flask and requests).
Solution 2: Message Queue (RabbitMQ)
Overview:
Using a message queue like RabbitMQ allows for a more scalable and decoupled approach. The
main application publishes a message to a RabbitMQ queue, and each pod subscribes to the queue
to process the message.
Architecture:
1. Producer: The main application sends a message to the RabbitMQ queue.
2. Consumer: Each pod has a sidecar container that subscribes to the RabbitMQ queue and
processes messages.
Advantages:
1. Scalability: The message queue can handle a large number of messages and distribute them
efficiently.
2. Decoupling: The producer and consumers are decoupled, allowing for more flexible and
maintainable code.
3. Reliability: RabbitMQ provides robust messaging guarantees, ensuring that messages are
delivered even in the face of network or pod failures.

-----------------\\\\\\\\\\\\\\\--------------------------------

Performance Testing Documentation
Overview
This document outlines the performance testing conducted for our application using Apache JMeter. The objective of the performance testing is to ensure that the system can handle various load conditions effectively and to identify any potential bottlenecks or performance issues.

Test Configuration
Tool Used: Apache JMeter 5.0

Server: redis-poc-cp-1049543.apps.useast16.bofa.com

Endpoint: api/v1/kafka/event/publish/topic/phx73717_ceng_svc_abstraction_test_topic

HTTP Method: POST

Test Data:
The request body contains JSON data simulating events:

Json body---------

-----Json body

Test Scenarios
1. 1000 Users Per Second
Objective:
Evaluate the system's performance when subjected to a high load of 1000 users per second. The goal is to ensure that the server can handle this level of concurrency without significant performance degradation.

Configuration:
Number of Threads (users): 1000
Ramp-Up Period: 1 second
Loop Count: 1

Expected Outcome:
The system should handle the load without crashing.
Acceptable response times and low error rates.
High throughput.

Results:
Response Time: Average: 200ms, Max: 500ms
Error Rate: 0.1%
Throughput: 995 requests/second
Analysis:
The system performed well under the load of 1000 users per second. Response times remained within acceptable limits, and the error rate was minimal.
pipeline {
    agent any

    stages {
        stage('Use shared library') {
            steps {
                // Step 1: Start a shell session and set the `tracker` variable
                script {
                    def tracker = sh(script: '''#!/bin/bash
                    # Store the tracker variable in the shell
                    tracker=test

                    # Execute a Python command to use this variable
                    python3 -c "import os; print('Tracker from Python:', os.environ['tracker'])"

                    # Print the tracker variable from shell
                    echo "Tracker from shell: $tracker"
                    
                    # Export tracker variable to environment for future use
                    echo $tracker
                    ''', returnStdout: true).trim()
                }

                // Step 2: Print xyz in Groovy
                script {
                    echo "xyz"
                }

                // Step 3: Return to the shell session and print the `tracker` variable
                script {
                    sh '''
                    # Use the previously stored tracker variable
                    echo "Final tracker: $tracker"
                    '''
                }
            }
        }
    }
}

pipeline {
    agent any

    stages {
        stage('Carbon Tracker started') {
            steps {
                // Step 1: Start the tracker and serialize it to a file
                script {
                    sh """
                        python3 -c "
import os
import json
from codecarbon import OfflineEmissionsTracker
import pickle

tracker = OfflineEmissionsTracker(country_iso_code='CAN')
tracker.start()
# Serialize the tracker object to a file
with open('/tmp/tracker.pkl', 'wb') as f:
    pickle.dump(tracker, f)
"
                    """
                }

                // Step 2: Print xyz in Groovy
                script {
                    echo "xyz"
                }

                // Step 3: Deserialize the tracker and stop it
                script {
                    sh """
                        python3 -c "
import json
from codecarbon import OfflineEmissionsTracker
import pickle

# Deserialize the tracker object from the file
with open('/tmp/tracker.pkl', 'rb') as f:
    tracker = pickle.load(f)

tracker.stop()
"
                    """
                }
            }
        }
    }
}
pipeline {
    agent any
    environment {
        PID_FILE = 'process.pid'  // Define a file to store the PID
    }
    stages {
        stage('Start Process') {
            steps {
                script {
                    sh '''
                    # Start the process in the background (example: a sleep command)
                    sleep 300 &
                    
                    # Save the PID of the process
                    echo $! > ${PID_FILE}
                    
                    # Verify the process is running
                    echo "Started process with PID: $(cat ${PID_FILE})"
                    '''
                }
            }
        }
        stage('Stop Process') {
            steps {
                script {
                    sh '''
                    # Check if the PID file exists and read the PID
                    if [ -f ${PID_FILE} ]; then
                        PID=$(cat ${PID_FILE})
                        
                        # Stop the process
                        kill $PID
                        
                        # Optionally verify if the process was terminated
                        if ps -p $PID > /dev/null; then
                            echo "Process $PID is still running"
                        else
                            echo "Process $PID has been terminated"
                        fi
                    else
                        echo "PID file not found!"
                    fi
                    '''
                }
            }
        }
    }
}

₹₹₹₹₹₹₹₹₹₹₹₹
pipeline {
    agent any
    environment {
        TRACKER_STATE_FILE = 'tracker_state.pkl'  // File to store the tracker state
    }
    stages {
        stage('Start Tracker') {
            steps {
                script {
                    sh '''
                    # Start the tracker and serialize it to a file
                    python3 -c "
import pickle
from your_module import offlineEmissionTracker  # Import your tracker module

# Initialize and start the tracker
tracker = offlineEmissionTracker(country_iso_code='CAN')
tracker.start()

# Serialize tracker state
with open('${TRACKER_STATE_FILE}', 'wb') as f:
    pickle.dump(tracker, f)
"
                    '''
                }
            }
        }
        stage('Stop Tracker') {
            steps {
                script {
                    sh '''
                    # Deserialize the tracker object and stop it
                    python3 -c "
import pickle

# Deserialize tracker state
with open('${TRACKER_STATE_FILE}', 'rb') as f:
    tracker = pickle.load(f)

# Stop the tracker
tracker.stop()
"
                    '''
                }
            }
        }
    }
}
???????


### Confluence Page: Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

---

**Page Title:** Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

**Author:** [Your Name]

**Date:** [Date of Testing]

---

### Overview

This document captures the performance test results for the Kafka Proxy Service deployed on OpenShift with varying replica configurations and resource limits. The objective is to analyze the performance impact of scaling the service from 1 replica pod to 3 replica pods under different resource configurations and a defined traffic load.

---

### Test Environment

- **Deployment Platform:** OpenShift
- **Service:** Kubernetes Kafka Proxy
- **Test Scenarios:**
  - **Scenario 1:** 1 Replica Pod
  - **Scenario 2:** 3 Replica Pods
- **Pod Resource Configurations:**
  - **Configuration A:** Request 2Gi, Limit 4Gi
  - **Configuration B:** Request 2Gi, Limit 1Gi
- **Traffic Rate:** 2400 requests over 10 minutes (~4 requests per second)
- **Message Rate to Kafka:** [Specify if different]
- **Kafka Cluster:** [Details on the Kafka setup if applicable]

---

### Test Objectives

1. **Evaluate the system's ability to handle a sustained traffic load of 2400 requests in 10 minutes with 1 and 3 replica pods under different resource configurations.**
2. **Assess the impact of different resource configurations on key performance metrics, including CPU, memory usage, message throughput, response time, and success/failure rates.**
3. **Determine the most efficient configuration for optimal performance and resource utilization.**

---

### Test Setup

**1. Scenario 1: 1 Replica Pod**

   - **Replica Configuration:** 1 Pod
   - **Pod Resource Configurations:**
     - **Configuration A:** Request 2Gi, Limit 4Gi
     - **Configuration B:** Request 2Gi, Limit 1Gi
   - **Traffic Simulation:** 2400 requests in 10 minutes

**2. Scenario 2: 3 Replica Pods**

   - **Replica Configuration:** 3 Pods
   - **Pod Resource Configurations:**
     - **Configuration A:** Request 2Gi, Limit 4Gi
     - **Configuration B:** Request 2Gi, Limit 1Gi
   - **Traffic Simulation:** 2400 requests in 10 minutes

**Tooling:**

   - **Performance Testing Tool:** [Specify Tool, e.g., JMeter, Locust]
   - **Monitoring Tools:** [Specify Tools, e.g., Prometheus, Grafana]
   - **Data Collection:** Metrics collected via [Specify Tool or Method]

---

### Performance Metrics

1. **CPU Utilization:** Average and peak CPU usage during the test.
2. **Memory Usage:** Average and peak memory usage during the test.
3. **Response Time:**
   - **Average Response Time:** Mean response time for all requests.
   - **95th Percentile Response Time:** Response time below which 95% of requests fall.
4. **Throughput:** Number of successful Kafka messages per second.
5. **Error Rate:** Number of failed requests or messages.
6. **Pod Scaling Impact:** Comparison between 1 and 3 replicas in handling the load.
7. **Resource Configuration Impact:** Comparison between different resource configurations (2Gi/4Gi vs. 2Gi/1Gi) on system performance.

---

### Test Results

#### Scenario 1: 1 Replica Pod

- **Configuration A (2Gi/4Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as resource saturation, high latency, etc.]

- **Configuration B (2Gi/1Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as resource saturation, high latency, etc.]

#### Scenario 2: 3 Replica Pods

- **Configuration A (2Gi/4Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as improved performance, reduced latency, etc.]

- **Configuration B (2Gi/1Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as improved performance, reduced latency, etc.]

---

### Analysis

- **Performance Comparison:** Analyze the differences in performance metrics between the two scenarios and resource configurations. Discuss the impact of scaling from 1 to 3 pods and the effect of resource limits on handling the traffic load.
- **Bottlenecks:** Identify any bottlenecks observed during the tests and suggest possible optimizations.
- **Recommendations:** Provide recommendations for optimal scaling strategies, resource configuration tuning, or any other improvements based on the test results.

---

### Conclusion

Summarize the key findings from the performance tests, emphasizing the relationship between pod scaling, resource configuration, and system performance. Provide a final recommendation on the optimal configuration for handling the specified traffic load.

---

**Attachments:**

- [Links to detailed logs, charts, and raw data files]

---

**Reviewers:**

- [Reviewer Name 1]
- [Reviewer Name 2]

**Approvals:**

- [Approval Name 1]
- [Approval Name 2]

---

This page provides a comprehensive view of the performance test results and serves as a reference for future optimizations and scaling strategies.


/////////
### Confluence Page: Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

---

**Page Title:** Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

**Author:** [Your Name]

**Date:** [Date of Testing]

---

### Overview

This document captures the performance test results for the Kafka Proxy Service deployed on OpenShift under various load conditions. Multiple scenarios have been tested to assess the service's performance across different traffic rates, time durations, and replica configurations.

---

### Test Environment

- **Deployment Platform:** OpenShift
- **Service:** Kubernetes Kafka Proxy
- **Traffic Rates:**
  - 2400 requests over 10 minutes (~4 requests per second)
  - 4800 requests over 10 minutes (~8 requests per second)
  - 4800 requests over 30 minutes (~2.67 requests per second)
- **Replica Configurations:**
  - 1 Replica Pod
  - 3 Replica Pods
- **Resource Configurations:**
  - Request 2Gi, Limit 4Gi
  - Request 2Gi, Limit 1Gi
- **Kafka Cluster:** [Details on the Kafka setup if applicable]

---

### Use Cases and Test Scenarios

**1. Scenario 1: Expected Production Load**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Simulates the expected production load with 1 replica pod configured to handle the average traffic rate efficiently.

**2. Scenario 2: Doubling the Production Requirements**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Tests the system's scalability when the load is doubled, using 3 replica pods to manage the increased traffic.

**3. Scenario 3: Resource-Constrained Environment**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 1Gi
   - **Description:** Evaluates performance under resource constraints, with the pod limited to 1Gi of memory.

**4. Scenario 4: Stress Testing with Limited Resources**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 1Gi
   - **Description:** Stress tests the system with increased replicas and constrained resources.

**5. Scenario 5: High Load Over Short Duration**
   - **Traffic Rate:** 4800 requests in 10 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Tests how the system handles a high traffic load over a short duration with 1 replica pod.

**6. Scenario 6: Scaling with High Load**
   - **Traffic Rate:** 4800 requests in 10 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Evaluates system performance when scaled up to 3 pods under the same high load.

**7. Scenario 7: Prolonged High Load with Resource Constraints**
   - **Traffic Rate:** 4800 requests in 30 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 1Gi
   - **Description:** Tests the system's ability to handle a prolonged load with limited resources.

**8. Scenario 8: Prolonged High Load with Scaling**
   - **Traffic Rate:** 4800 requests in 30 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Assesses performance when the load is sustained over a longer period with multiple replicas.

---

### Test Results Summary

The results for each test scenario are summarized in the table below, capturing key performance metrics such as CPU utilization, memory usage, response time, throughput, and error rate.

| **Scenario**      



| **Traffic Rate**             | **Replica Configuration** | **




To document this in an elaborated Jira story comment, you could structure it as follows:

---

**Heap Dump Generation for Kubernetes Proxy Application Deployed in OpenShift**

As part of our ongoing analysis for the Kubernetes proxy application deployed in OpenShift, we have been investigating two different scenarios for generating heap dumps. These scenarios are critical for diagnosing memory issues and improving the performance and stability of our application. Below is a detailed breakdown of the current status and limitations of both approaches:

1. **Heap Dump Generation During Pod Termination:**
   - We have successfully configured the heap dump generation in scenarios where a pod is manually terminated (either by a user or through Kubernetes' automatic scaling/restarting mechanisms).
   - In this case, the heap dump is generated just before the pod is terminated, which helps us capture the state of the application during pod lifecycle events. This method provides valuable insights into the memory consumption and the state of objects in the heap at the time of termination.
   - This method is fully implemented and working as expected in our OpenShift environment.

2. **Heap Dump Generation on Out of Memory (OOM) Events:**
   - The second scenario focuses on generating a heap dump when the application encounters an Out of Memory (OOM) event. Such events are crucial for diagnosing memory leaks or excessive memory usage that lead to the application crashing.
   - However, at present, we are facing limitations in achieving this. Specifically, the base image used for the Kubernetes proxy app does not support automatic heap dump generation upon OOM. 
   - The current OpenShift and organizational infrastructure lacks the necessary features to configure this functionality, primarily due to limitations in the base image's capabilities, which prevent us from automatically capturing heap dumps during OOM conditions.
   - Implementing this would likely require a change in the base image to one that supports this functionality, or a modification in our deployment pipeline to accommodate this.

**Next Steps:**
- Investigate potential options for base image customization or explore alternative images that support heap dump generation on OOM.
- Engage with the DevOps and platform teams to evaluate whether adjustments can be made to our Kubernetes configuration or OpenShift environment to enable heap dump creation during OOM events.
- Document any findings and assess the feasibility of implementing this feature in future releases.

By addressing these two scenarios, we aim to enhance our application's resilience and gain better diagnostic capabilities, especially in high-memory-use situations.

--- 

This comment gives a clear overview of the situation, highlights the challenges, and outlines potential next steps.




-------////-----

Here’s a possible **Acceptance Criteria** in the **Given-When-Then** format for your story:

---

**Story:** Persist Prometheus metrics across pod restarts by enabling the metrics file in a Persistent Volume (PV).

**Acceptance Criteria:**

1. **Given** the Java application is deployed on OpenShift and uses the `meterRegistry` library for Prometheus metrics,  
   **When** a pod is restarted or deleted,  
   **Then** the metrics file should be persisted in a Persistent Volume (PV) and all previous metrics (like hit count, response time, etc.) should continue without resetting.

2. **Given** the Persistent Volume (PV) is configured and mounted correctly in the pod,  
   **When** metrics are written to the `meterRegistry`,  
   **Then** they should be stored in the PV to ensure data is available after pod restarts.

3. **Given** the `/actuator/Prometheus` endpoint is being accessed after a pod restart,  
   **When** the metrics data is retrieved,  
   **Then** it should reflect the accumulated metrics from before the restart, without starting from scratch. 

---

This covers the key requirements for persisting the Prometheus metrics.




If Grafana is not available in your organization, you can consider the following alternatives for visualizing Prometheus metrics on a single page:

### 1. **Prometheus Expression Browser**
   - **Description**: Prometheus itself has a basic UI called the **Expression Browser**, where you can manually run **PromQL** queries and visualize the results.
   - **How to Use**: 
     - Navigate to `http://<prometheus-server>/graph` in your browser.
     - Enter your PromQL queries and view graphs. You can run multiple queries side by side, though it's not as feature-rich as a full dashboard.
   - **Limitations**: Limited visualization capabilities and cannot combine metrics into a single dashboard.

### 2. **Thanos Querier UI** (if Thanos is available)
   - **Description**: Thanos provides a **Querier UI** that integrates with Prometheus and extends its capabilities.
   - **How to Use**: 
     - If Thanos is set up in your environment, access the **Querier** component, which allows you to aggregate data from multiple Prometheus instances and view the metrics on a single UI.
   - **Limitations**: Requires Thanos to be deployed, and still offers fewer visualization features compared to Grafana.

### 3. **Custom Web Application**
   - **Description**: You can build a simple web app using a framework like **Flask** (Python) or **Express** (Node.js) that pulls metrics from the Prometheus API and displays them on a single page.
   - **How to Use**: 
     - Query Prometheus' API using the `/api/v1/query` endpoint.
     - Fetch multiple metrics and render them in a table, chart, or other visual formats in your custom web application.
   - **Limitations**: Requires development effort but gives you full control over how the metrics are presented.

### 4. **Prometheus Console Templates**
   - **Description**: Prometheus supports **console templates**, where you can create custom HTML pages that visualize metrics using simple graphs or tables.
   - **How to Use**:
     - Create a **.libsonnet** or **.html** file under the `consoles/` directory of Prometheus.
     - Define the queries and the layout in the template, then view it by accessing `http://<prometheus-server>/consoles/your-template`.
   - **Limitations**: Basic visualization capabilities, but can be a useful option for aggregating metrics.

### 5. **Pushgateway for Aggregated Metrics**
   - **Description**: **Prometheus Pushgateway** can be used to collect and aggregate metrics from short-lived jobs or batch processes.
   - **How to Use**:
     - Set up Pushgateway and push metrics from your Prometheus clients.
     - These metrics can be aggregated in Prometheus and visualized through the Expression Browser or other custom setups.
   - **Limitations**: Mainly suited for batch jobs and needs some setup for continuous processes.

These options provide flexibility based on your needs, allowing you to consolidate your Prometheus metrics in one place for better monitoring.


------------------------

Confluence Page Update: Persisting Prometheus Metrics for Java Application


---

Page Title: Persisting Prometheus Metrics for Java Application


---

Overview

In modern application deployments, monitoring is a crucial aspect to ensure the health, performance, and reliability of applications. Java applications often leverage Prometheus MeterRegistry to expose detailed metrics about application performance. However, these metrics are typically stored in-memory by default, which means that once an application or its associated pod restarts, the metric data is lost. To overcome this limitation and ensure continuity in metric data, we are implementing a solution to persist metrics using a custom script.


---

Objective

The primary objective of this initiative is:

To persist Prometheus metrics that are exposed through the /actuator/prometheus endpoint, which are currently stored in-memory.

To develop a custom script that runs at regular intervals, collects the required metrics, calculates their changes, and saves the aggregated data to a Persistent Volume (PV).

This script will allow metrics to retain their values even after a pod restart, thus providing continuous insights into the application’s performance over time.



---

Why Are We Doing This?

1. Current Challenge with In-Memory Metrics:

Java applications that expose metrics using the Prometheus MeterRegistry store these metrics in-memory.

When a pod restarts (e.g., due to a deployment update or failure), the in-memory metrics are reset, causing loss of valuable historical data.

As a result, aggregated metrics such as request counts, JVM memory usage, and custom business metrics start from zero, which makes it difficult to get a continuous view of application performance.



2. Need for Metric Persistence:

Continuous Monitoring: Persistent storage of metrics ensures that data is not lost upon restarts, providing a continuous monitoring view.

Accurate Reporting: It helps in maintaining accurate reports and dashboards over time, as the data is cumulative and accounts for previous values.

Historical Analysis: With persisted metrics, historical data analysis becomes possible, enabling the team to observe trends over time, such as memory usage patterns, API response times, and error rates.



3. Benefits for the Operations Team:

The persistence mechanism simplifies the troubleshooting process, allowing operators to have a consistent baseline of metrics.

It reduces the need for manual interventions to restart applications or rely on external monitoring solutions that may also face data sync challenges during restarts.





---

Theory Behind the Solution

The solution involves the development of a script that can be triggered by the application itself at regular intervals. Here is how the approach works:

1. Metrics Collection:

The Java application exposes metrics via the /actuator/prometheus endpoint.

The script is designed to fetch these metrics from the endpoint at predefined intervals (e.g., every 5 minutes).


2. Selective Metrics Extraction:

The script filters out only the relevant metrics that are critical for monitoring and are defined in a configuration file (e.g., memory usage, request counts, etc.).

This ensures that only the required metrics are processed and stored, reducing storage overhead.


3. Aggregation of Metrics:

The script then compares the current metrics with previously stored values in the PV.

It calculates the difference between the current and previous values to derive the incremental changes.

If the application has restarted and the metric values have reset to zero, the script recognizes this and handles the difference calculation accordingly.


4. Persistence to a Persistent Volume (PV):

The script then writes the aggregated metrics to a text file in a Persistent Volume (PV).

This PV is mounted to the application's pod, ensuring that even after restarts, the file containing metrics data remains intact.


5. Continuous Update:

The script runs at regular intervals, allowing it to continuously update and persist new metrics.

This ensures that the application’s metrics are always up-to-date in the persistent storage, providing accurate data for external systems or dashboards to consume.



---

Parallel Evaluation: SRE and Engineering Team Investigation

While we are developing this script-based approach to persist metrics, we are also collaborating with the SRE and engineering teams to explore potential solutions on the Prometheus side itself. Specifically, we are investigating:

Whether Prometheus can natively persist metrics data in our bank’s infrastructure without relying solely on the application-side script.

Evaluating Prometheus retention policies and configuration options that could allow us to store metric data in a more seamless and centralized manner.

Potential impact on Prometheus performance and resource usage when enabling persistence directly in the monitoring stack, as opposed to application-side solutions.


This parallel approach ensures that we explore all available options for metrics persistence, allowing us to adopt the most efficient and scalable solution for our environment. The goal is to find a solution that meets our requirements for data retention, accuracy, and operational simplicity.


---

Implementation Summary

Tools Used:

Prometheus MeterRegistry for exposing metrics.

Shell script for reading, extracting, and persisting metrics.

Persistent Volume (PV) for storing metrics data across pod restarts.


Script Behavior:

Reads metrics from /actuator/prometheus.

Extracts specific metrics based on a configuration file.

Aggregates changes and stores the result in a file within the PV.

Handles application restarts by comparing new metrics with previously persisted values.


Collaboration with SRE and Engineering:

Evaluate options for native Prometheus persistence in our infrastructure.

Assess configuration changes and their feasibility.




---

Expected Outcome

By implementing this solution and evaluating alternative options with the SRE team, we aim to:

Retain historical metrics data across pod restarts, leading to improved insights into application performance.

Enhance reliability of the monitoring process by ensuring that metrics are not lost during redeployments.

Enable the operations team to analyze trends over a longer duration without losing continuity in the data.

Optimize resource usage and potentially simplify the architecture if a Prometheus-based solution proves viable.



---

Next Steps

Development and testing of the script in the staging environment.

Validation of the metrics persistence across multiple restart scenarios.

Parallel discussions with the SRE and engineering teams to explore the viability of Prometheus-side persistence.

Decision on the preferred solution based on testing results and feedback from the SRE team.


!!!!!!!

Based on the performance test results you provided, here’s a brief observation and conclusion for your email:


---

Observation:
For sustained traffic of 4 events/second (7200 events over 10 minutes) and 8 events/second (14,400 events over 10 minutes):

CPU Usage: Initial usage was low (Pod1: 6 mcores, Pod2: 68 mcores), with the maximum observed CPU remaining within limits (Pod1: 101 mcores, Pod2: 110 mcores for 4 TPS, and slightly higher for 8 TPS).

Memory Usage: Memory usage was stable, with a maximum of around 378 Mi observed, staying comfortably below the limit of 3 Gi.

Throughput: The system maintained throughput rates of 4 to 8 messages per second, confirming that it can handle sustained high loads.


Conclusion:
The current resource configuration (CPU request: 500 mcore, CPU limit: 1 core, Memory request: 2 Gi, Memory limit: 3 Gi) is sufficient to handle the expected load scenarios, with no significant issues observed in CPU or memory usage. We are good to proceed with this resource configuration.


---

Would this summary work for your email?



