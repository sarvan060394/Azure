
 
---------------------------------------------------------------------------------------


Broadcasting Requests to All Pods in a Kubernetes service
Overview
This document outlines two solutions for broadcasting a request to all pods under a headless
service in Kubernetes. The two approaches are:
1. Sidecar Pattern
2. Message Queue (RabbitMQ)
Both methods ensure that a request to the headless service results in the request being forwarded
to all pods.
Solution 1: Sidecar Pattern
Overview:
The sidecar pattern involves deploying a sidecar container alongside your main application
container. The sidecar handles the broadcasting of requests to all pods in the headless service.
Architecture:
1. Main Application: Receives the initial request and forwards it to the sidecar container.
2. Sidecar Container: Resolves the headless service to get the IPs of all pods and sends the request
to each pod, including a final self-call.
Advantages:
1. Simplicity: The sidecar pattern is straightforward to implement and doesn't require additional
infrastructure.
2. Direct Control: You have direct control over how and when requests are broadcasted.
Broadcasting Requests to All Pods in a service
3. Minimal Dependencies: Only requires Python standard library and a couple of third-party libraries
(Flask and requests).
Solution 2: Message Queue (RabbitMQ)
Overview:
Using a message queue like RabbitMQ allows for a more scalable and decoupled approach. The
main application publishes a message to a RabbitMQ queue, and each pod subscribes to the queue
to process the message.
Architecture:
1. Producer: The main application sends a message to the RabbitMQ queue.
2. Consumer: Each pod has a sidecar container that subscribes to the RabbitMQ queue and
processes messages.
Advantages:
1. Scalability: The message queue can handle a large number of messages and distribute them
efficiently.
2. Decoupling: The producer and consumers are decoupled, allowing for more flexible and
maintainable code.
3. Reliability: RabbitMQ provides robust messaging guarantees, ensuring that messages are
delivered even in the face of network or pod failures.

-----------------\\\\\\\\\\\\\\\--------------------------------

Performance Testing Documentation
Overview
This document outlines the performance testing conducted for our application using Apache JMeter. The objective of the performance testing is to ensure that the system can handle various load conditions effectively and to identify any potential bottlenecks or performance issues.

Test Configuration
Tool Used: Apache JMeter 5.0

Server: redis-poc-cp-1049543.apps.useast16.bofa.com

Endpoint: api/v1/kafka/event/publish/topic/phx73717_ceng_svc_abstraction_test_topic

HTTP Method: POST

Test Data:
The request body contains JSON data simulating events:

Json body---------

-----Json body

Test Scenarios
1. 1000 Users Per Second
Objective:
Evaluate the system's performance when subjected to a high load of 1000 users per second. The goal is to ensure that the server can handle this level of concurrency without significant performance degradation.

Configuration:
Number of Threads (users): 1000
Ramp-Up Period: 1 second
Loop Count: 1

Expected Outcome:
The system should handle the load without crashing.
Acceptable response times and low error rates.
High throughput.

Results:
Response Time: Average: 200ms, Max: 500ms
Error Rate: 0.1%
Throughput: 995 requests/second
Analysis:
The system performed well under the load of 1000 users per second. Response times remained within acceptable limits, and the error rate was minimal.
pipeline {
    agent any

    stages {
        stage('Use shared library') {
            steps {
                // Step 1: Start a shell session and set the `tracker` variable
                script {
                    def tracker = sh(script: '''#!/bin/bash
                    # Store the tracker variable in the shell
                    tracker=test

                    # Execute a Python command to use this variable
                    python3 -c "import os; print('Tracker from Python:', os.environ['tracker'])"

                    # Print the tracker variable from shell
                    echo "Tracker from shell: $tracker"
                    
                    # Export tracker variable to environment for future use
                    echo $tracker
                    ''', returnStdout: true).trim()
                }

                // Step 2: Print xyz in Groovy
                script {
                    echo "xyz"
                }

                // Step 3: Return to the shell session and print the `tracker` variable
                script {
                    sh '''
                    # Use the previously stored tracker variable
                    echo "Final tracker: $tracker"
                    '''
                }
            }
        }
    }
}

pipeline {
    agent any

    stages {
        stage('Carbon Tracker started') {
            steps {
                // Step 1: Start the tracker and serialize it to a file
                script {
                    sh """
                        python3 -c "
import os
import json
from codecarbon import OfflineEmissionsTracker
import pickle

tracker = OfflineEmissionsTracker(country_iso_code='CAN')
tracker.start()
# Serialize the tracker object to a file
with open('/tmp/tracker.pkl', 'wb') as f:
    pickle.dump(tracker, f)
"
                    """
                }

                // Step 2: Print xyz in Groovy
                script {
                    echo "xyz"
                }

                // Step 3: Deserialize the tracker and stop it
                script {
                    sh """
                        python3 -c "
import json
from codecarbon import OfflineEmissionsTracker
import pickle

# Deserialize the tracker object from the file
with open('/tmp/tracker.pkl', 'rb') as f:
    tracker = pickle.load(f)

tracker.stop()
"
                    """
                }
            }
        }
    }
}
pipeline {
    agent any
    environment {
        PID_FILE = 'process.pid'  // Define a file to store the PID
    }
    stages {
        stage('Start Process') {
            steps {
                script {
                    sh '''
                    # Start the process in the background (example: a sleep command)
                    sleep 300 &
                    
                    # Save the PID of the process
                    echo $! > ${PID_FILE}
                    
                    # Verify the process is running
                    echo "Started process with PID: $(cat ${PID_FILE})"
                    '''
                }
            }
        }
        stage('Stop Process') {
            steps {
                script {
                    sh '''
                    # Check if the PID file exists and read the PID
                    if [ -f ${PID_FILE} ]; then
                        PID=$(cat ${PID_FILE})
                        
                        # Stop the process
                        kill $PID
                        
                        # Optionally verify if the process was terminated
                        if ps -p $PID > /dev/null; then
                            echo "Process $PID is still running"
                        else
                            echo "Process $PID has been terminated"
                        fi
                    else
                        echo "PID file not found!"
                    fi
                    '''
                }
            }
        }
    }
}

₹₹₹₹₹₹₹₹₹₹₹₹
pipeline {
    agent any
    environment {
        TRACKER_STATE_FILE = 'tracker_state.pkl'  // File to store the tracker state
    }
    stages {
        stage('Start Tracker') {
            steps {
                script {
                    sh '''
                    # Start the tracker and serialize it to a file
                    python3 -c "
import pickle
from your_module import offlineEmissionTracker  # Import your tracker module

# Initialize and start the tracker
tracker = offlineEmissionTracker(country_iso_code='CAN')
tracker.start()

# Serialize tracker state
with open('${TRACKER_STATE_FILE}', 'wb') as f:
    pickle.dump(tracker, f)
"
                    '''
                }
            }
        }
        stage('Stop Tracker') {
            steps {
                script {
                    sh '''
                    # Deserialize the tracker object and stop it
                    python3 -c "
import pickle

# Deserialize tracker state
with open('${TRACKER_STATE_FILE}', 'rb') as f:
    tracker = pickle.load(f)

# Stop the tracker
tracker.stop()
"
                    '''
                }
            }
        }
    }
}
???????


### Confluence Page: Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

---

**Page Title:** Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

**Author:** [Your Name]

**Date:** [Date of Testing]

---

### Overview

This document captures the performance test results for the Kafka Proxy Service deployed on OpenShift with varying replica configurations and resource limits. The objective is to analyze the performance impact of scaling the service from 1 replica pod to 3 replica pods under different resource configurations and a defined traffic load.

---

### Test Environment

- **Deployment Platform:** OpenShift
- **Service:** Kubernetes Kafka Proxy
- **Test Scenarios:**
  - **Scenario 1:** 1 Replica Pod
  - **Scenario 2:** 3 Replica Pods
- **Pod Resource Configurations:**
  - **Configuration A:** Request 2Gi, Limit 4Gi
  - **Configuration B:** Request 2Gi, Limit 1Gi
- **Traffic Rate:** 2400 requests over 10 minutes (~4 requests per second)
- **Message Rate to Kafka:** [Specify if different]
- **Kafka Cluster:** [Details on the Kafka setup if applicable]

---

### Test Objectives

1. **Evaluate the system's ability to handle a sustained traffic load of 2400 requests in 10 minutes with 1 and 3 replica pods under different resource configurations.**
2. **Assess the impact of different resource configurations on key performance metrics, including CPU, memory usage, message throughput, response time, and success/failure rates.**
3. **Determine the most efficient configuration for optimal performance and resource utilization.**

---

### Test Setup

**1. Scenario 1: 1 Replica Pod**

   - **Replica Configuration:** 1 Pod
   - **Pod Resource Configurations:**
     - **Configuration A:** Request 2Gi, Limit 4Gi
     - **Configuration B:** Request 2Gi, Limit 1Gi
   - **Traffic Simulation:** 2400 requests in 10 minutes

**2. Scenario 2: 3 Replica Pods**

   - **Replica Configuration:** 3 Pods
   - **Pod Resource Configurations:**
     - **Configuration A:** Request 2Gi, Limit 4Gi
     - **Configuration B:** Request 2Gi, Limit 1Gi
   - **Traffic Simulation:** 2400 requests in 10 minutes

**Tooling:**

   - **Performance Testing Tool:** [Specify Tool, e.g., JMeter, Locust]
   - **Monitoring Tools:** [Specify Tools, e.g., Prometheus, Grafana]
   - **Data Collection:** Metrics collected via [Specify Tool or Method]

---

### Performance Metrics

1. **CPU Utilization:** Average and peak CPU usage during the test.
2. **Memory Usage:** Average and peak memory usage during the test.
3. **Response Time:**
   - **Average Response Time:** Mean response time for all requests.
   - **95th Percentile Response Time:** Response time below which 95% of requests fall.
4. **Throughput:** Number of successful Kafka messages per second.
5. **Error Rate:** Number of failed requests or messages.
6. **Pod Scaling Impact:** Comparison between 1 and 3 replicas in handling the load.
7. **Resource Configuration Impact:** Comparison between different resource configurations (2Gi/4Gi vs. 2Gi/1Gi) on system performance.

---

### Test Results

#### Scenario 1: 1 Replica Pod

- **Configuration A (2Gi/4Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as resource saturation, high latency, etc.]

- **Configuration B (2Gi/1Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as resource saturation, high latency, etc.]

#### Scenario 2: 3 Replica Pods

- **Configuration A (2Gi/4Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as improved performance, reduced latency, etc.]

- **Configuration B (2Gi/1Gi):**
  - **CPU Utilization:** [Detail the average and peak values]
  - **Memory Usage:** [Detail the average and peak values]
  - **Response Time:**
    - **Average:** [Value]
    - **95th Percentile:** [Value]
  - **Throughput:** [Value]
  - **Error Rate:** [Value]
  - **Observations:** [List any observations, such as improved performance, reduced latency, etc.]

---

### Analysis

- **Performance Comparison:** Analyze the differences in performance metrics between the two scenarios and resource configurations. Discuss the impact of scaling from 1 to 3 pods and the effect of resource limits on handling the traffic load.
- **Bottlenecks:** Identify any bottlenecks observed during the tests and suggest possible optimizations.
- **Recommendations:** Provide recommendations for optimal scaling strategies, resource configuration tuning, or any other improvements based on the test results.

---

### Conclusion

Summarize the key findings from the performance tests, emphasizing the relationship between pod scaling, resource configuration, and system performance. Provide a final recommendation on the optimal configuration for handling the specified traffic load.

---

**Attachments:**

- [Links to detailed logs, charts, and raw data files]

---

**Reviewers:**

- [Reviewer Name 1]
- [Reviewer Name 2]

**Approvals:**

- [Approval Name 1]
- [Approval Name 2]

---

This page provides a comprehensive view of the performance test results and serves as a reference for future optimizations and scaling strategies.


/////////
### Confluence Page: Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

---

**Page Title:** Performance Test Results for Kubernetes Kafka Proxy Service on OpenShift

**Author:** [Your Name]

**Date:** [Date of Testing]

---

### Overview

This document captures the performance test results for the Kafka Proxy Service deployed on OpenShift under various load conditions. Multiple scenarios have been tested to assess the service's performance across different traffic rates, time durations, and replica configurations.

---

### Test Environment

- **Deployment Platform:** OpenShift
- **Service:** Kubernetes Kafka Proxy
- **Traffic Rates:**
  - 2400 requests over 10 minutes (~4 requests per second)
  - 4800 requests over 10 minutes (~8 requests per second)
  - 4800 requests over 30 minutes (~2.67 requests per second)
- **Replica Configurations:**
  - 1 Replica Pod
  - 3 Replica Pods
- **Resource Configurations:**
  - Request 2Gi, Limit 4Gi
  - Request 2Gi, Limit 1Gi
- **Kafka Cluster:** [Details on the Kafka setup if applicable]

---

### Use Cases and Test Scenarios

**1. Scenario 1: Expected Production Load**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Simulates the expected production load with 1 replica pod configured to handle the average traffic rate efficiently.

**2. Scenario 2: Doubling the Production Requirements**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Tests the system's scalability when the load is doubled, using 3 replica pods to manage the increased traffic.

**3. Scenario 3: Resource-Constrained Environment**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 1Gi
   - **Description:** Evaluates performance under resource constraints, with the pod limited to 1Gi of memory.

**4. Scenario 4: Stress Testing with Limited Resources**
   - **Traffic Rate:** 2400 requests in 10 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 1Gi
   - **Description:** Stress tests the system with increased replicas and constrained resources.

**5. Scenario 5: High Load Over Short Duration**
   - **Traffic Rate:** 4800 requests in 10 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Tests how the system handles a high traffic load over a short duration with 1 replica pod.

**6. Scenario 6: Scaling with High Load**
   - **Traffic Rate:** 4800 requests in 10 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Evaluates system performance when scaled up to 3 pods under the same high load.

**7. Scenario 7: Prolonged High Load with Resource Constraints**
   - **Traffic Rate:** 4800 requests in 30 minutes
   - **Replica Configuration:** 1 Pod
   - **Resource Configuration:** Request 2Gi, Limit 1Gi
   - **Description:** Tests the system's ability to handle a prolonged load with limited resources.

**8. Scenario 8: Prolonged High Load with Scaling**
   - **Traffic Rate:** 4800 requests in 30 minutes
   - **Replica Configuration:** 3 Pods
   - **Resource Configuration:** Request 2Gi, Limit 4Gi
   - **Description:** Assesses performance when the load is sustained over a longer period with multiple replicas.

---

### Test Results Summary

The results for each test scenario are summarized in the table below, capturing key performance metrics such as CPU utilization, memory usage, response time, throughput, and error rate.

| **Scenario**      



| **Traffic Rate**             | **Replica Configuration** | **




To document this in an elaborated Jira story comment, you could structure it as follows:

---

**Heap Dump Generation for Kubernetes Proxy Application Deployed in OpenShift**

As part of our ongoing analysis for the Kubernetes proxy application deployed in OpenShift, we have been investigating two different scenarios for generating heap dumps. These scenarios are critical for diagnosing memory issues and improving the performance and stability of our application. Below is a detailed breakdown of the current status and limitations of both approaches:

1. **Heap Dump Generation During Pod Termination:**
   - We have successfully configured the heap dump generation in scenarios where a pod is manually terminated (either by a user or through Kubernetes' automatic scaling/restarting mechanisms).
   - In this case, the heap dump is generated just before the pod is terminated, which helps us capture the state of the application during pod lifecycle events. This method provides valuable insights into the memory consumption and the state of objects in the heap at the time of termination.
   - This method is fully implemented and working as expected in our OpenShift environment.

2. **Heap Dump Generation on Out of Memory (OOM) Events:**
   - The second scenario focuses on generating a heap dump when the application encounters an Out of Memory (OOM) event. Such events are crucial for diagnosing memory leaks or excessive memory usage that lead to the application crashing.
   - However, at present, we are facing limitations in achieving this. Specifically, the base image used for the Kubernetes proxy app does not support automatic heap dump generation upon OOM. 
   - The current OpenShift and organizational infrastructure lacks the necessary features to configure this functionality, primarily due to limitations in the base image's capabilities, which prevent us from automatically capturing heap dumps during OOM conditions.
   - Implementing this would likely require a change in the base image to one that supports this functionality, or a modification in our deployment pipeline to accommodate this.

**Next Steps:**
- Investigate potential options for base image customization or explore alternative images that support heap dump generation on OOM.
- Engage with the DevOps and platform teams to evaluate whether adjustments can be made to our Kubernetes configuration or OpenShift environment to enable heap dump creation during OOM events.
- Document any findings and assess the feasibility of implementing this feature in future releases.

By addressing these two scenarios, we aim to enhance our application's resilience and gain better diagnostic capabilities, especially in high-memory-use situations.

--- 

This comment gives a clear overview of the situation, highlights the challenges, and outlines potential next steps.




-------////-----

Here’s a possible **Acceptance Criteria** in the **Given-When-Then** format for your story:

---

**Story:** Persist Prometheus metrics across pod restarts by enabling the metrics file in a Persistent Volume (PV).

**Acceptance Criteria:**

1. **Given** the Java application is deployed on OpenShift and uses the `meterRegistry` library for Prometheus metrics,  
   **When** a pod is restarted or deleted,  
   **Then** the metrics file should be persisted in a Persistent Volume (PV) and all previous metrics (like hit count, response time, etc.) should continue without resetting.

2. **Given** the Persistent Volume (PV) is configured and mounted correctly in the pod,  
   **When** metrics are written to the `meterRegistry`,  
   **Then** they should be stored in the PV to ensure data is available after pod restarts.

3. **Given** the `/actuator/Prometheus` endpoint is being accessed after a pod restart,  
   **When** the metrics data is retrieved,  
   **Then** it should reflect the accumulated metrics from before the restart, without starting from scratch. 

---

This covers the key requirements for persisting the Prometheus metrics.




If Grafana is not available in your organization, you can consider the following alternatives for visualizing Prometheus metrics on a single page:

### 1. **Prometheus Expression Browser**
   - **Description**: Prometheus itself has a basic UI called the **Expression Browser**, where you can manually run **PromQL** queries and visualize the results.
   - **How to Use**: 
     - Navigate to `http://<prometheus-server>/graph` in your browser.
     - Enter your PromQL queries and view graphs. You can run multiple queries side by side, though it's not as feature-rich as a full dashboard.
   - **Limitations**: Limited visualization capabilities and cannot combine metrics into a single dashboard.

### 2. **Thanos Querier UI** (if Thanos is available)
   - **Description**: Thanos provides a **Querier UI** that integrates with Prometheus and extends its capabilities.
   - **How to Use**: 
     - If Thanos is set up in your environment, access the **Querier** component, which allows you to aggregate data from multiple Prometheus instances and view the metrics on a single UI.
   - **Limitations**: Requires Thanos to be deployed, and still offers fewer visualization features compared to Grafana.

### 3. **Custom Web Application**
   - **Description**: You can build a simple web app using a framework like **Flask** (Python) or **Express** (Node.js) that pulls metrics from the Prometheus API and displays them on a single page.
   - **How to Use**: 
     - Query Prometheus' API using the `/api/v1/query` endpoint.
     - Fetch multiple metrics and render them in a table, chart, or other visual formats in your custom web application.
   - **Limitations**: Requires development effort but gives you full control over how the metrics are presented.

### 4. **Prometheus Console Templates**
   - **Description**: Prometheus supports **console templates**, where you can create custom HTML pages that visualize metrics using simple graphs or tables.
   - **How to Use**:
     - Create a **.libsonnet** or **.html** file under the `consoles/` directory of Prometheus.
     - Define the queries and the layout in the template, then view it by accessing `http://<prometheus-server>/consoles/your-template`.
   - **Limitations**: Basic visualization capabilities, but can be a useful option for aggregating metrics.

### 5. **Pushgateway for Aggregated Metrics**
   - **Description**: **Prometheus Pushgateway** can be used to collect and aggregate metrics from short-lived jobs or batch processes.
   - **How to Use**:
     - Set up Pushgateway and push metrics from your Prometheus clients.
     - These metrics can be aggregated in Prometheus and visualized through the Expression Browser or other custom setups.
   - **Limitations**: Mainly suited for batch jobs and needs some setup for continuous processes.

These options provide flexibility based on your needs, allowing you to consolidate your Prometheus metrics in one place for better monitoring.


------------------------

Confluence Page Update: Persisting Prometheus Metrics for Java Application


---

Page Title: Persisting Prometheus Metrics for Java Application


---

Overview

In modern application deployments, monitoring is a crucial aspect to ensure the health, performance, and reliability of applications. Java applications often leverage Prometheus MeterRegistry to expose detailed metrics about application performance. However, these metrics are typically stored in-memory by default, which means that once an application or its associated pod restarts, the metric data is lost. To overcome this limitation and ensure continuity in metric data, we are implementing a solution to persist metrics using a custom script.


---

Objective

The primary objective of this initiative is:

To persist Prometheus metrics that are exposed through the /actuator/prometheus endpoint, which are currently stored in-memory.

To develop a custom script that runs at regular intervals, collects the required metrics, calculates their changes, and saves the aggregated data to a Persistent Volume (PV).

This script will allow metrics to retain their values even after a pod restart, thus providing continuous insights into the application’s performance over time.



---

Why Are We Doing This?

1. Current Challenge with In-Memory Metrics:

Java applications that expose metrics using the Prometheus MeterRegistry store these metrics in-memory.

When a pod restarts (e.g., due to a deployment update or failure), the in-memory metrics are reset, causing loss of valuable historical data.

As a result, aggregated metrics such as request counts, JVM memory usage, and custom business metrics start from zero, which makes it difficult to get a continuous view of application performance.



2. Need for Metric Persistence:

Continuous Monitoring: Persistent storage of metrics ensures that data is not lost upon restarts, providing a continuous monitoring view.

Accurate Reporting: It helps in maintaining accurate reports and dashboards over time, as the data is cumulative and accounts for previous values.

Historical Analysis: With persisted metrics, historical data analysis becomes possible, enabling the team to observe trends over time, such as memory usage patterns, API response times, and error rates.



3. Benefits for the Operations Team:

The persistence mechanism simplifies the troubleshooting process, allowing operators to have a consistent baseline of metrics.

It reduces the need for manual interventions to restart applications or rely on external monitoring solutions that may also face data sync challenges during restarts.





---

Theory Behind the Solution

The solution involves the development of a script that can be triggered by the application itself at regular intervals. Here is how the approach works:

1. Metrics Collection:

The Java application exposes metrics via the /actuator/prometheus endpoint.

The script is designed to fetch these metrics from the endpoint at predefined intervals (e.g., every 5 minutes).


2. Selective Metrics Extraction:

The script filters out only the relevant metrics that are critical for monitoring and are defined in a configuration file (e.g., memory usage, request counts, etc.).

This ensures that only the required metrics are processed and stored, reducing storage overhead.


3. Aggregation of Metrics:

The script then compares the current metrics with previously stored values in the PV.

It calculates the difference between the current and previous values to derive the incremental changes.

If the application has restarted and the metric values have reset to zero, the script recognizes this and handles the difference calculation accordingly.


4. Persistence to a Persistent Volume (PV):

The script then writes the aggregated metrics to a text file in a Persistent Volume (PV).

This PV is mounted to the application's pod, ensuring that even after restarts, the file containing metrics data remains intact.


5. Continuous Update:

The script runs at regular intervals, allowing it to continuously update and persist new metrics.

This ensures that the application’s metrics are always up-to-date in the persistent storage, providing accurate data for external systems or dashboards to consume.



---

Parallel Evaluation: SRE and Engineering Team Investigation

While we are developing this script-based approach to persist metrics, we are also collaborating with the SRE and engineering teams to explore potential solutions on the Prometheus side itself. Specifically, we are investigating:

Whether Prometheus can natively persist metrics data in our bank’s infrastructure without relying solely on the application-side script.

Evaluating Prometheus retention policies and configuration options that could allow us to store metric data in a more seamless and centralized manner.

Potential impact on Prometheus performance and resource usage when enabling persistence directly in the monitoring stack, as opposed to application-side solutions.


This parallel approach ensures that we explore all available options for metrics persistence, allowing us to adopt the most efficient and scalable solution for our environment. The goal is to find a solution that meets our requirements for data retention, accuracy, and operational simplicity.


---

Implementation Summary

Tools Used:

Prometheus MeterRegistry for exposing metrics.

Shell script for reading, extracting, and persisting metrics.

Persistent Volume (PV) for storing metrics data across pod restarts.


Script Behavior:

Reads metrics from /actuator/prometheus.

Extracts specific metrics based on a configuration file.

Aggregates changes and stores the result in a file within the PV.

Handles application restarts by comparing new metrics with previously persisted values.


Collaboration with SRE and Engineering:

Evaluate options for native Prometheus persistence in our infrastructure.

Assess configuration changes and their feasibility.




---

Expected Outcome

By implementing this solution and evaluating alternative options with the SRE team, we aim to:

Retain historical metrics data across pod restarts, leading to improved insights into application performance.

Enhance reliability of the monitoring process by ensuring that metrics are not lost during redeployments.

Enable the operations team to analyze trends over a longer duration without losing continuity in the data.

Optimize resource usage and potentially simplify the architecture if a Prometheus-based solution proves viable.



---

Next Steps

Development and testing of the script in the staging environment.

Validation of the metrics persistence across multiple restart scenarios.

Parallel discussions with the SRE and engineering teams to explore the viability of Prometheus-side persistence.

Decision on the preferred solution based on testing results and feedback from the SRE team.


!!!!!!!

Based on the performance test results you provided, here’s a brief observation and conclusion for your email:


---

Observation:
For sustained traffic of 4 events/second (7200 events over 10 minutes) and 8 events/second (14,400 events over 10 minutes):

CPU Usage: Initial usage was low (Pod1: 6 mcores, Pod2: 68 mcores), with the maximum observed CPU remaining within limits (Pod1: 101 mcores, Pod2: 110 mcores for 4 TPS, and slightly higher for 8 TPS).

Memory Usage: Memory usage was stable, with a maximum of around 378 Mi observed, staying comfortably below the limit of 3 Gi.

Throughput: The system maintained throughput rates of 4 to 8 messages per second, confirming that it can handle sustained high loads.


Conclusion:
The current resource configuration (CPU request: 500 mcore, CPU limit: 1 core, Memory request: 2 Gi, Memory limit: 3 Gi) is sufficient to handle the expected load scenarios, with no significant issues observed in CPU or memory usage. We are good to proceed with this resource configuration.


---

Would this summary work for your email?


,,,,,,

Here's a simpler explanation of how Dapr manages multiple components with one sidecar, using Redis and Kafka as examples:

What Dapr Does with Multiple Components

1. One Sidecar, Many Abilities:

When you use Dapr with your app, it adds a sidecar next to your app's container.

Think of the sidecar as a helpful assistant. It can manage many tools (components), but it’s still just one assistant working alongside your app.



2. How Dapr Knows What to Do:

You give Dapr a configuration file that lists the tools you want it to manage. For example, you might want a state storage using Redis and a messaging system using Kafka.

Dapr reads this configuration file when it starts, and it knows which tools (components) to set up.



3. Example Configuration Files:

You could have two files, one for Redis and one for Kafka:

Redis Configuration (state.yaml):

apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: statestore
spec:
  type: state.redis
  metadata:
  - name: redisHost
    value: "redis:6379"
  - name: redisPassword
    value: ""

Kafka Configuration (pubsub.yaml):

apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: pubsub
spec:
  type: pubsub.kafka
  metadata:
  - name: brokers
    value: "kafka:9092"
  - name: consumerGroup
    value: "myapp-consumer"




4. What Happens When Dapr Starts:

When Dapr starts up, it reads these configuration files and understands that it needs to manage:

A state storage using Redis.

A pub/sub system using Kafka.


But instead of creating two separate sidecars, it only starts one sidecar that can handle both Redis and Kafka.



5. How Your App Talks to Dapr:

Your app communicates with Dapr’s sidecar using simple HTTP or gRPC requests.

For example, if your app needs to store some data in Redis, it talks to the sidecar using a URL like:

POST /v1.0/state/statestore

Here, statestore is the name of the Redis storage Dapr set up.

If your app needs to publish a message to Kafka, it uses:

POST /v1.0/publish/pubsub

Here, pubsub refers to the Kafka system Dapr set up.



6. Dapr Keeps It Simple:

Even though Dapr might handle multiple tools for your app (like Redis and Kafka), it keeps everything inside that one sidecar. This makes it easier to manage and doesn’t use extra resources to run separate sidecars.

The Dapr sidecar is like a toolbox that holds many tools, but it’s still just one box that sits beside your app.




Summary

You have one app and one Dapr sidecar.

You give Dapr a list of tools (like Redis and Kafka) using configuration files.

Dapr sets up these tools inside the same sidecar.

Your app can easily talk to the sidecar to use the tools it needs.


This way, Dapr helps you manage different components without making things too complicated, and it does all this with just one sidecar for each app.


.,..........


Here's the detailed explanation in simple English about how Dapr manages multiple components with a single sidecar:


---

How Dapr Manages Multiple Components with a Single Sidecar

This document explains how Dapr can manage multiple components, like Redis and Kafka, with just one sidecar. It describes the process step by step in simple terms.


---

Step 1: One Sidecar, Many Abilities

When you use Dapr with your app, it adds a sidecar next to your app's container.

Think of the sidecar as a helpful assistant. It can manage many tools (components), but it’s still just one assistant working alongside your app.



---

Step 2: How Dapr Knows What to Do

You give Dapr a configuration file that lists the tools you want it to manage.

For example, you might want a state storage using Redis and a messaging system using Kafka.

Dapr reads this configuration file when it starts, and it knows which tools (components) to set up.



---

Step 3: Example Configuration Files

You could have two files, one for Redis and one for Kafka:

Redis Configuration (state.yaml):

apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: statestore
spec:
  type: state.redis
  metadata:
  - name: redisHost
    value: "redis:6379"
  - name: redisPassword
    value: ""

Kafka Configuration (pubsub.yaml):

apiVersion: dapr.io/v1alpha1
kind: Component
metadata:
  name: pubsub
spec:
  type: pubsub.kafka
  metadata:
  - name: brokers
    value: "kafka:9092"
  - name: consumerGroup
    value: "myapp-consumer"



---

Step 4: What Happens When Dapr Starts

When Dapr starts up, it reads these configuration files and understands that it needs to manage:

A state storage using Redis.

A pub/sub system using Kafka.


But instead of creating two separate sidecars, it only starts one sidecar that can handle both Redis and Kafka.



---

Step 5: How Your App Talks to Dapr

Your app communicates with Dapr’s sidecar using simple HTTP or gRPC requests. For example:

If your app needs to store some data in Redis, it talks to the sidecar using a URL like:

POST /v1.0/state/statestore

Here, statestore is the name of the Redis storage Dapr set up.

If your app needs to publish a message to Kafka, it uses:

POST /v1.0/publish/pubsub

Here, pubsub refers to the Kafka system Dapr set up.




---

Step 6: Dapr Keeps It Simple

Even though Dapr might handle multiple tools for your app (like Redis and Kafka), it keeps everything inside that one sidecar.

This makes it easier to manage and doesn’t use extra resources to run separate sidecars.

The Dapr sidecar is like a toolbox that holds many tools, but it’s still just one box that sits beside your app.



---

Summary

You have one app and one Dapr sidecar.

You give Dapr a list of tools (like Redis and Kafka) using configuration files.

Dapr sets up these tools inside the same sidecar.

Your app can easily talk to the sidecar to use the tools it needs.


This way, Dapr helps you manage different components without making things too complicated, and it does all this with just one sidecar for each app.


---

This explanation provides a step-by-step view of how Dapr manages multiple components with a single sidecar, making it easier to understand and share with your team.
............



Comment:

We’ve tried three ways to save Prometheus metrics data, but each has had issues:

1. Using application settings to save metrics directly to the PV

We tried configuring the app to write metrics to the PV, but it didn’t work as expected.



2. Sending metrics to /var/logs for Splunk to collect

We tried to save metrics in /var/logs so Splunk could collect it, but we don’t have permission to create files in /var/logs.



3. Creating a script utility

We’re working on a script to get metrics from /actuator/prometheus, save it to the PV, and add up the metrics even after a restart. We’re still improving this script.




Next Steps:

Keep working on the script to make sure it correctly saves and adds up metrics in the PV.

Look into setting up Splunk to collect logs from a different path.


Let me know if this is clear or needs any changes!


.......


Here’s a Jira story based on the performance testing details provided:

Story: Execute Performance Testing with Production Settings and VPA Enabled

Description:

To validate the application’s performance and stability under different traffic per second (TPS) loads, we need to conduct performance testing with both the current production settings and after enabling the Vertical Pod Autoscaler (VPA). The test should verify the application’s ability to handle the specified TPS loads without restarting pods during execution.

Acceptance Criteria:

Given the application is deployed with 3 instances,
When traffic is continuously applied at various TPS levels (16, 20, and 25 TPS),
Then the system should handle the load without pod restarts and maintain performance within acceptable thresholds.

Test Scenarios:

1. With Current Production Settings:

	•	Test Case 1: 16 TPS with 3 instances for continuous 30 minutes and 1 hour.
	•	Test Case 2: 20 TPS with 3 instances for continuous 30 minutes and 1 hour.
	•	Test Case 3: 25 TPS with 3 instances for continuous 30 minutes and 1 hour.

2. Enable the VPA:

	•	Test Case 4: 16 TPS with 3 instances for continuous 30 minutes and 1 hour.
	•	Test Case 5: 20 TPS with 3 instances for continuous 30 minutes and 1 hour.
	•	Test Case 6: 25 TPS with 3 instances for continuous 30 minutes and 1 hour.

Sub-Tasks:

	1.	Set Up Test Environment:
	•	Deploy the application with 3 instances in the test environment.
	•	Ensure VPA is enabled for the second set of test cases.
	•	Verify that pod restarts are disabled during test execution.
	2.	Execute and Capture Test Results:
	•	Run each test scenario for both production settings and VPA-enabled configurations.
	•	Capture key performance metrics like CPU, memory, TPS handling, and latency.
	•	Document any anomalies or performance issues.

Note: Ensure that the test namespace is configured correctly before beginning the tests.

