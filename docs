Image & Stale Resource Pruner
Overview
This Confluence page details the custom-built Image & Stale Resource Pruner application developed to manage and clean up unused container images in Artifactory and stale Kubernetes/OpenShift resources like pods, deployments, replica sets, and controllers.

The pruner application exposes REST APIs with a FastAPI UI, allowing users to:

List and selectively delete images from Artifactory, OpenShift, or both.

Identify and delete stale resources, particularly pods stuck in CrashLoopBackOff or Completed status, based on user input.

Module: Stale Resource (Pod) Deletion API
Functionality
The stale resource pruner includes a powerful API to delete pods from OpenShift by status. This is not limited to a fixed list of statuses.

Key Capabilities:
Automatically detects all pod statuses (e.g., CrashLoopBackOff, Completed, Evicted, Error, Pending, Terminating, etc.)

Displays a categorized list of pods grouped by status

Enables users to:

Select specific pod statuses (e.g., only Evicted or Error)

Review matching pods

Delete selected pods from OpenShift

Example Pod Statuses Supported
Pod Status	Description
Running	Actively running pods (not typically deleted)
Completed	Successfully completed jobs (safe to delete)
CrashLoopBackOff	Crashing repeatedly and restarting
Error	Pods that failed permanently
Evicted	Removed by Kubelet due to resource pressure
Pending	Scheduled but not yet running (e.g., resource constraints)
Terminating	Stuck or slow termination
Users have full control to filter and choose which pod statuses to delete.

Key Challenges & Edge Cases
1. Pods Recreated After Deletion
Even after stale pods are deleted by the API, some are immediately recreated due to ownership relationships:

Case A: Pods Owned by ReplicaSet or ReplicationController
Deleting the pod alone is not sufficient.

ReplicaSets/RCs will automatically reschedule and recreate the pods to maintain desired replica count.

Resolution:

The API includes an option to identify and delete the owning ReplicaSet or RC along with the pods.

Case B: ReplicaSet/RC Owned by DeploymentConfig (DC)
If the owner of the ReplicaSet or RC is a DC, the system will regenerate the ReplicaSet and, in turn, the pod.

Resolution:

The deletion logic traces the ownership tree:

Pod → RS/RC → DeploymentConfig

It prompts the user (in UI or via dry-run) that to stop complete regeneration, the entire tree needs to be deleted, starting from the DC.

Ownership Tracing Hierarchy
plaintext
Copy
Edit
Pod
└── ReplicaSet or ReplicationController
    └── DeploymentConfig (if applicable)
Ownership metadata is retrieved using metadata.ownerReferences.

Risk Mitigation and Environment Segregation
Environment Filtering
To ensure production environments remain untouched, a filtering mechanism is applied:

Resources are only considered if they are tagged with env_type: non-prod.

Known Limitation
All environments except production are currently tagged as non-prod.

This makes it difficult to differentiate between Dev, QA, UAT, SIT, etc.

Risk Implications:
A generic cleanup in non-prod environments might unintentionally affect critical testing workloads.

For example, QA or UAT environments, which are typically more stable and require persistence, might be impacted.

Recommendations & Next Steps
Short-Term Improvements
Enhance API to visualize ownership chains so users understand what will get recreated post-deletion.

Add a confirmation prompt before deleting owners like DC/RC/RS.

Mid-Term Strategy
Improve environment tagging:

Tag projects more granularly with env_type: dev, env_type: qa, env_type: uat, etc.

Introduce namespace-specific filters in the UI and API to allow selective targeting.

Long-Term Enhancements
Integrate with GitOps or config management tools to:

Reconcile deleted DCs automatically (if needed).

Ensure audit logs and traceability.

Sample Workflow Diagram
plaintext
Copy
Edit
User selects pods to delete based on status
        │
        ├── Check if Pod has ownerReferences
        │     └── If ReplicaSet/RC found
        │             └── Check if RS/RC is owned by DeploymentConfig
        │                     └── Prompt user to confirm deletion of higher-level owner
        └── Delete the pod(s) and selected owners
Conclusion
The stale resource pruner provides a powerful, controlled interface to clean up unused workloads in OpenShift. However, due to the complexity of resource ownership and tagging limitations in non-prod environments, caution is essential to prevent unintended deletions. Future enhancements aim to provide better visibility, control, and granularity in targeting stale resources.
