 Delete Stale Pods API ‚Äì User Guide
‚úÖ Purpose
In large OpenShift clusters, many stale pods remain due to job completions, crashes, or other issues. These unused pods can clutter the system or take up unnecessary resources.

To fix this, we‚Äôve developed a "Delete Stale Pods API" ‚Äî a smart cleanup system that:

Lists stale pods by their status

Identifies pod ownership (ReplicaSet, Deployment, etc.)

Deletes safely, without breaking running applications

üë©‚Äçüíª How It Works (User Perspective)
As a user, you can:

Access the FastAPI Web UI

Select pod statuses you want to clean up:

Examples: CrashLoopBackOff, Completed, Error, Terminating, Pending, Evicted (All statuses supported)

Choose namespaces to target

The API automatically filters only non-prod namespaces based on the label: env_type=nonprod

Review the list of matching stale pods

Confirm deletion to start cleanup

The API will:

Trace pod ownership (RS/RC ‚Üí Deployment/DC)

Delete associated resources only if safe

Log everything clearly for traceability



üß¨ Key Logic Scenarios Handled
Case 1: Pod has no owner
The pod is not part of any controller.

Action: Pod is deleted directly.

Case 2: Pod is owned by a ReplicaSet or ReplicationController (no higher controller)
RS/RC manages the pod, but no Deployment or DC manages the RS/RC.

Action: Pod and RS/RC are both deleted.

Case 3: Pod is owned by ReplicaSet ‚Üí Deployment
If the Deployment still has healthy running pods:

Skip deletion to avoid breaking the app

If no other pods are running:

Delete the Deployment

Delete the RS

Delete the pod

Case 4: Multiple stale pods from the same Deployment
Only one delete operation is triggered for the Deployment.

Duplicates are tracked and skipped using in-memory sets.

Avoids noisy Kubernetes errors like 404 Not Found

 Key Challenges & Bottlenecks
1. Complex Ownership Chains
In Kubernetes, pods are often part of a chain:

scss
Copy
Edit
Pod ‚Üí ReplicaSet ‚Üí Deployment (or Pod ‚Üí RC ‚Üí DC)
The API navigates this chain automatically, so users don‚Äôt have to.
------------------------------------------------

Stale Image Pruner API: Performance Improvement Strategy

üß≠ Overview

The Stale Image Pruner API is responsible for listing and deleting container images (with all tags) from JFrog Artifactory and identifying all OpenShift resources using those images. While functional, the current implementation suffers from significant performance issues, especially when handling large workloads across clusters.

This document outlines the performance bottlenecks observed, their root causes, and a detailed optimization plan to address them.

‚ö†Ô∏è Current Bottlenecks

1. Frequent and Redundant API Calls

Issue: The API repeatedly queries both JFrog and OpenShift for image and pod details, often for the same image or pod.

Impact: Excessive latency and unnecessary network traffic.

2. Re-authentication to OpenShift

Issue: The OpenShift authentication token and kubeconfig are fetched multiple times per request.

Impact: Wastes processing time and introduces redundant operations.

3. Linear Resource Traversal in OpenShift

Issue: Each pod is individually evaluated to determine its associated RS/DC or Deployment, and the loop is repeated for each image.

Impact: Results in high latency for clusters with many resources.

üîç Observations from Logs

Multiple API hits to JFrog per image and tag.

Frequent fetching of the same OpenShift pod, RS, and deployment info.

OpenShift login succeeds quickly, but image-to-resource matching dominates time.

‚úÖ Optimization Strategies

1. Batch Resource Fetching

What to do:

Fetch all pods, RS, RC, Deployments, and DCs upfront at once.

Store these in-memory dictionaries for quick local access.

Benefits:

Drastically reduces redundant API calls.

Enables rapid lookup and filtering.

Sample Code Logic:

all_pods = list_all_pods()
all_rs = list_all_replicasets()
# Then access via dict: (namespace, name) as key
pod_map = {(pod.metadata.namespace, pod.metadata.name): pod for pod in all_pods}

2. Threading for Parallel Execution

What to do:

Use ThreadPoolExecutor to parallelize:

Image-to-pod usage checks

Pod-to-controller resolution

Benefits:

Utilizes I/O waiting time efficiently

Reduces total duration by parallel processing

3. Local Caching of Ownership Chains

What to do:

Cache pod-to-owner mapping during execution:

owner_cache["pod-namespace"] = { "rs": "rs-name", "deployment": "deployment-name" }

Benefits:

Avoids recomputation for pods sharing same owners.

Streamlines dependency tracing.

4. Consolidate OpenShift Login / Kubeconfig Fetching

What to do:

Authenticate once at the beginning and reuse token and config.

Benefits:

Prevents repeated authentication logic.

Speeds up API calls that follow.



------


Description:
Currently, the image pruner APIs fetch tokens for OpenShift and Artifactory with every request. To improve efficiency and reduce authentication overhead, implement a caching mechanism to store these tokens and reuse them until expiry. The cache should be aware of token TTL and refresh tokens as needed.

‚∏ª

Acceptance Criteria (AC)

Given the image pruner app requires authentication with OpenShift and Artifactory,
When a token is already cached and still valid,
Then the API should use the cached token instead of generating or fetching a new one.


Design token cache structure with TTL tracking
	‚Ä¢	Define a simple in-memory cache (e.g., dictionary with token and expiry time)
	‚Ä¢	Plan for separate cache entries for OpenShift and Artifactory
	2.	Implement OpenShift token caching logic
	‚Ä¢	Fetch and cache token with associated TTL
	‚Ä¢	Reuse token until TTL expires, then refresh

---------------

To improve performance and reduce redundant API calls, implement a caching mechanism within the image pruner application that stores results from OpenShift and Artifactory queries. Cached data should be used wherever possible during API responses, and the cache should be configurable and refreshable.

‚∏ª

Acceptance Criteria (AC)

Given the image pruner app interacts with OpenShift and Artifactory,
When the API is invoked repeatedly for listing images or resources,
Then the app should serve results from the cache instead of making fresh calls every time,
And the cache should support TTL (Time To Live) and refresh mechanisms.

‚∏ª

Definition of Done (DoD)
	‚Ä¢	In-memory or persistent caching implemented for both OpenShift and Artifactory API data
	‚Ä¢	Cache is configurable (e.g., TTL, max size, refresh interval)
	‚Ä¢	Cache is used for listing APIs, with fallback to live calls when needed
	‚Ä¢	Cache refreshes after TTL expiry or via manual trigger
	‚Ä¢	Logging in place to indicate when cache hits or misses occur
	‚Ä¢	Documentation updated for cache configuration and behavior

‚∏ª

Sub-Tasks
	1.	Design caching strategy and select cache backend (e.g., in-memory, Redis)
	‚Ä¢	Decide on the caching layer (e.g., cachetools, functools.lru_cache, or Redis)
	‚Ä¢	Identify which data from OpenShift and Artifactory should be cached
	2.	Implement caching for Artifactory image list API
	‚Ä¢	Store list of repositories and tags per image
	‚Ä¢	Ensure proper cache invalidation and TTL
	3.	Implement caching for OpenShift pod/deployment image usage
	‚Ä¢	Cache pod and deployment metadata for each namespace
	‚Ä¢	Include filtering logic for dry-run and deletion paths


As part of the optimization work on the image pruner API, we have:
	‚Ä¢	Implemented parallel processing for JFrog Artifactory operations to enhance performance and reduce latency.
	‚Ä¢	Introduced threading for OpenShift-related operations to further improve the API response time.
	‚Ä¢	Added a caching mechanism to store results from OpenShift and Artifactory, reducing redundant API calls.
	‚Ä¢	Ensured the cache supports TTL (Time To Live) and provides a refresh mechanism as specified in the Acceptance Criteria.

All Acceptance Criteria have been met and verified.

Next Steps:
	‚Ä¢	Apply the above optimization to the remaining two APIs.
	‚Ä¢	Validate image tagging is performed in sequence.
	‚Ä¢	Explore APIs from the Panther app to interact with the CM Portal.
	‚Ä¢	Create a dedicated service account for the image pruner.

‚∏ª

Updated Acceptance Criteria (AC):
	‚Ä¢	Given the image pruner app interacts with OpenShift and Artifactory,
	‚Ä¢	When the API is invoked repeatedly for listing images or resources,
	‚Ä¢	Then the app should serve results from the cache instead of making fresh calls every time,
	‚Ä¢	And the cache should support TTL (Time To Live) and refresh mechanisms.
	‚Ä¢	Additionally, the app should use parallel processing for Artifactory and threading for OpenShift operations to optimize response time.

‚∏ª

Updated Description (DC):

PLATFORM: Code refactoring and optimization in Image Pruner GET & DELETE APIs.

To improve performance and reduce redundant API calls, we have:
	‚Ä¢	Implemented a caching mechanism to store results from OpenShift and Artifactory queries.
	‚Ä¢	Used parallel processing for JFrog Artifactory operations and threading for OpenShift operations to increase API response time.
	‚Ä¢	Ensured that cached data is configurable and refreshable, supporting TTL and manual refresh triggers.






----////-----

Acceptance Criteria (AC):
	‚Ä¢	Given the image pruner DELETE API is responsible for removing images from OpenShift and JFrog Artifactory,
	‚Ä¢	When the DELETE operation is triggered,
	‚Ä¢	Then the system should execute JFrog Artifactory deletions using parallel processing for multiple images,
	‚Ä¢	And OpenShift deletions should be handled using threading to improve concurrency and reduce overall response time,
	‚Ä¢	And the DELETE operation should remain idempotent and safe even when run in parallel,
	‚Ä¢	And proper error handling and logging should be implemented to capture any failures during deletion.

‚∏ª

Description (DC):

PLATFORM: Code refactoring and optimization in Image Pruner DELETE APIs.

To improve the performance of the image pruning process, especially when dealing with multiple images or resources, the DELETE APIs have been enhanced to:
	‚Ä¢	Use parallel processing for handling JFrog Artifactory image deletions, allowing multiple operations to run concurrently.
	‚Ä¢	Apply threading for OpenShift resource deletions to improve API response time and reduce latency.
	‚Ä¢	Ensure robustness through appropriate error handling, logging, and retries where applicable.

Implement Parallel Processing for Artifactory DELETE Calls
	‚Ä¢	Use concurrent.futures or equivalent async/threading tools for efficient deletion.
	‚Ä¢	Implement retries and failure tracking.
	4.	Add Threading for OpenShift Deletion Operations
	‚Ä¢	Introduce threading to handle multiple resource deletions simultaneously.
	‚Ä¢	Ensure Kubernetes/OpenShift API interactions are thread-safe.

