Monitoring Solutions Documentation
This document describes two solutions for monitoring services using Python. The first solution uses
threads and Redis for state management, and the second solution uses a scheduler to manage the
monitoring jobs.
Solution 1: Using Threads and Redis for State Management
This solution uses Redis to keep track of which services are being monitored and ensures that only
one monitoring task
is running for each service in a deployment. The monitoring task runs in a separate thread and
periodically checks
the service status until it comes back online.
### Steps to Implement:
1. **Install Redis and Python Redis client library:**
 ```bash
 pip install redis
 ```
2. **Set up a Redis instance:**
 - You can set up a Redis instance locally or use a managed Redis service.
3. **Implement the monitoring solution:**
 - **State Management:** Use Redis to store the state of each monitoring job. The key is a
combination of `deployment_id` and `service_type`, and the value is either `active` or `inactive`.
 - **Monitoring Task:** Create a function to run in a separate thread that periodically checks the
service status. If the service is down, it logs a message and continues to monitor.
 - **Start Monitoring Endpoint:** This endpoint starts the monitoring task if it is not already running.
It sets the status in Redis to `active` and starts a new thread for the monitoring task.
 - **Stop Monitoring Endpoint:** This endpoint stops the monitoring task by setting the status in
Redis to `inactive`.
### Benefits:
- This solution is straightforward and does not require additional scheduling libraries.
- It allows for flexible and dynamic state management using Redis.
### Potential Drawbacks:
- Managing threads can become complex with a large number of services.
- Ensuring thread safety and handling exceptions within threads can add complexity

Solution 2: Using Scheduler and Redis for State Management
This solution uses a scheduler to manage the monitoring jobs. Each monitoring job periodically
checks the Redis status
until the service comes back online.
### Steps to Implement:
1. **Install necessary packages:**
 ```bash
 pip install redis flask apscheduler requests
 ```
2. **Set up a Redis instance:**
 - You can set up a Redis instance locally or use a managed Redis service.
3. **Implement the solution with a scheduler:**
 - **Scheduler Setup:** Use the `apscheduler` library to create and manage the scheduled jobs.
 - **State Management:** Use Redis to store the state of each monitoring job. The key is a
combination of `deployment_id` and `service_type`, and the value is either `active` or `inactive`.
 - **Monitoring Job:** Create a function that checks if the Redis service is online. If the Redis
service is online, it sets the job status to `inactive` in Redis and removes the job from the scheduler.
If the Redis service is still down, it logs a message and continues to monitor.
 - **Start Monitoring Endpoint:** This endpoint starts the monitoring job if it is not already running. It
sets the status in Redis to `active` and schedules a new job using the scheduler.
 - **Stop Monitoring Endpoint:** This endpoint stops the monitoring job by setting the status in
Redis to `inactive` and removing the job from the scheduler.
### Benefits:
- The scheduler handles the periodic execution of monitoring tasks, simplifying the implementation.
- Using a scheduler can improve the scalability and manageability of monitoring multiple services.
### Potential Drawbacks:
- Additional dependencies are required (`apscheduler` library).
- Handling the scheduler's lifecycle and job management can add complexity  

---------------------------------------------------------------------------------------


Broadcasting Requests to All Pods in a Kubernetes service
Overview
This document outlines two solutions for broadcasting a request to all pods under a headless
service in Kubernetes. The two approaches are:
1. Sidecar Pattern
2. Message Queue (RabbitMQ)
Both methods ensure that a request to the headless service results in the request being forwarded
to all pods.
Solution 1: Sidecar Pattern
Overview:
The sidecar pattern involves deploying a sidecar container alongside your main application
container. The sidecar handles the broadcasting of requests to all pods in the headless service.
Architecture:
1. Main Application: Receives the initial request and forwards it to the sidecar container.
2. Sidecar Container: Resolves the headless service to get the IPs of all pods and sends the request
to each pod, including a final self-call.
Advantages:
1. Simplicity: The sidecar pattern is straightforward to implement and doesn't require additional
infrastructure.
2. Direct Control: You have direct control over how and when requests are broadcasted.
Broadcasting Requests to All Pods in a service
3. Minimal Dependencies: Only requires Python standard library and a couple of third-party libraries
(Flask and requests).
Solution 2: Message Queue (RabbitMQ)
Overview:
Using a message queue like RabbitMQ allows for a more scalable and decoupled approach. The
main application publishes a message to a RabbitMQ queue, and each pod subscribes to the queue
to process the message.
Architecture:
1. Producer: The main application sends a message to the RabbitMQ queue.
2. Consumer: Each pod has a sidecar container that subscribes to the RabbitMQ queue and
processes messages.
Advantages:
1. Scalability: The message queue can handle a large number of messages and distribute them
efficiently.
2. Decoupling: The producer and consumers are decoupled, allowing for more flexible and
maintainable code.
3. Reliability: RabbitMQ provides robust messaging guarantees, ensuring that messages are
delivered even in the face of network or pod failures.

-----------------\\\\\\\\\\\\\\\--------------------------------

Performance Testing Documentation
Overview
This document outlines the performance testing conducted for our application using Apache JMeter. The objective of the performance testing is to ensure that the system can handle various load conditions effectively and to identify any potential bottlenecks or performance issues.

Test Configuration
Tool Used: Apache JMeter 5.0

Server: redis-poc-cp-1049543.apps.useast16.bofa.com

Endpoint: api/v1/kafka/event/publish/topic/phx73717_ceng_svc_abstraction_test_topic

HTTP Method: POST

Test Data:
The request body contains JSON data simulating events:

Json body---------

-----Json body

Test Scenarios
1. 1000 Users Per Second
Objective:
Evaluate the system's performance when subjected to a high load of 1000 users per second. The goal is to ensure that the server can handle this level of concurrency without significant performance degradation.

Configuration:
Number of Threads (users): 1000
Ramp-Up Period: 1 second
Loop Count: 1

Expected Outcome:
The system should handle the load without crashing.
Acceptable response times and low error rates.
High throughput.

Results:
Response Time: Average: 200ms, Max: 500ms
Error Rate: 0.1%
Throughput: 995 requests/second
Analysis:
The system performed well under the load of 1000 users per second. Response times remained within acceptable limits, and the error rate was minimal.
