def has_running_pods(k8s_client, namespace, parent_name, version, kind):
    dyn_client = k8s_client
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pod_list = pods.get(namespace=namespace)
        for pod in pod_list.items:
            owners = pod.metadata.owner_references or []
            for owner in owners:
                if owner.kind == kind and owner.name == parent_name:
                    if pod.status.phase == "Running":
                        return True
    except Exception as e:
        logger.warning(f"Error checking running pods for {kind} {parent_name}: {e}")
    return False


def handle_replica_set(k8s_client, rs_obj, dep_api, rs_api, pods_api, namespace, pod_name, deleted):
    rs_name = rs_obj.metadata.name
    deployment_name = rs_obj.metadata.owner_references[0].name if rs_obj.metadata.owner_references else None

    if not deployment_name:
        logger.warning(f"ReplicaSet {rs_name} has no owning Deployment.")
        return

    # Check for running pods under the entire deployment
    has_running = has_running_pods(
        k8s_client=k8s_client,
        namespace=namespace,
        parent_name=deployment_name,
        version="v1",
        kind="Deployment"
    )

    if not has_running:
        try:
            logger.info(f"No running pods found under Deployment {deployment_name}. Deleting Deployment, RS, and Pod.")
            dep_api.delete(name=deployment_name, namespace=namespace)
            logger.info(f"Deleted Deployment {deployment_name}")
        except Exception as e:
            logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")

        try:
            rs_api.delete(name=rs_name, namespace=namespace)
            logger.info(f"Deleted ReplicaSet {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete ReplicaSet {rs_name}: {e}")

        try:
            pods_api.delete(name=pod_name, namespace=namespace)
            logger.info(f"Deleted Pod {pod_name}")
            deleted.append(pod_name)
        except Exception as e:
            logger.warning(f"Failed to delete Pod {pod_name}: {e}")
    else:
        logger.info(f"Deployment {deployment_name} still has running pods. Skipping deletion.")


---///-------

def process_stale_pods(k8s_client, stale_pods):
    pods_api = dynclient.resources.get(api_version="v1", kind="Pod")
    rs_api = dynclient.resources.get(api_version="apps/v1", kind="ReplicaSet")
    dep_api = dynclient.resources.get(api_version="apps/v1", kind="Deployment")
    rc_api = dynclient.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dynclient.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    deleted = []

    for pod in stale_pods:
        namespace = pod["namespace"]
        name = pod["name"]
        try:
            pod_obj = pods_api.get(namespace=namespace, name=name)
            owners = pod_obj.metadata.ownerReferences or []

            if not owners:
                # No owner → delete pod
                pods_api.delete(namespace=namespace, name=name)
                logger.info(f"Deleted pod {name} in {namespace} with no owners.")
                deleted.append({"pod": name, "namespace": namespace})
                continue

            for owner in owners:
                kind = owner.kind
                owner_name = owner.name

                if kind == "ReplicaSet":
                    rs_obj = rs_api.get(namespace=namespace, name=owner_name)
                    handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, name, rs_obj, deleted)

                elif kind == "ReplicationController":
                    rc_obj = rc_api.get(namespace=namespace, name=owner_name)
                    handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, name, rc_obj, deleted)

        except Exception as e:
            logger.error(f"Error handling pod {name} in {namespace}: {e}")

    return deleted


def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted):
    rs_name = rs_obj.metadata.name
    rs_owners = rs_obj.metadata.ownerReferences or []

    if not rs_owners:
        # RS is orphaned → delete RS and pod
        try:
            rs_api.delete(namespace=namespace, name=rs_name)
            logger.info(f"Deleted orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete RS {rs_name}: {e}")

        try:
            pods_api.delete(namespace=namespace, name=pod_name)
            logger.info(f"Deleted pod {pod_name} owned by orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete pod {pod_name}: {e}")

        deleted.append({"replicaset": rs_name, "pod": pod_name, "namespace": namespace})
        return

    for owner in rs_owners:
        if owner.kind == "Deployment":
            deployment_name = owner.name

            # Check if any RS under the deployment has a running pod
            if has_running_pods(k8s_client, namespace, deployment_name, "apps/v1", "Deployment"):
                logger.info(f"Deployment {deployment_name} has other healthy pods. Skipping deletion.")
                return

            # No running pods under this deployment → safe to delete
            try:
                dep_api.delete(namespace=namespace, name=deployment_name)
                logger.info(f"Deleted Deployment {deployment_name}, which removed RS {rs_name} and pod {pod_name}")
                deleted.append({
                    "deployment": deployment_name,
                    "replicaset": rs_name,
                    "pod": pod_name,
                    "namespace": namespace
                })
            except Exception as e:
                logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")
            return
