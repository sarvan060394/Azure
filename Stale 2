def has_running_pods(k8s_client, namespace, parent_name, version, kind):
    dyn_client = k8s_client
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pod_list = pods.get(namespace=namespace)
        for pod in pod_list.items:
            owners = pod.metadata.owner_references or []
            for owner in owners:
                if owner.kind == kind and owner.name == parent_name:
                    if pod.status.phase == "Running":
                        return True
    except Exception as e:
        logger.warning(f"Error checking running pods for {kind} {parent_name}: {e}")
    return False


def handle_replica_set(k8s_client, rs_obj, dep_api, rs_api, pods_api, namespace, pod_name, deleted):
    rs_name = rs_obj.metadata.name
    deployment_name = rs_obj.metadata.owner_references[0].name if rs_obj.metadata.owner_references else None

    if not deployment_name:
        logger.warning(f"ReplicaSet {rs_name} has no owning Deployment.")
        return

    # Check for running pods under the entire deployment
    has_running = has_running_pods(
        k8s_client=k8s_client,
        namespace=namespace,
        parent_name=deployment_name,
        version="v1",
        kind="Deployment"
    )

    if not has_running:
        try:
            logger.info(f"No running pods found under Deployment {deployment_name}. Deleting Deployment, RS, and Pod.")
            dep_api.delete(name=deployment_name, namespace=namespace)
            logger.info(f"Deleted Deployment {deployment_name}")
        except Exception as e:
            logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")

        try:
            rs_api.delete(name=rs_name, namespace=namespace)
            logger.info(f"Deleted ReplicaSet {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete ReplicaSet {rs_name}: {e}")

        try:
            pods_api.delete(name=pod_name, namespace=namespace)
            logger.info(f"Deleted Pod {pod_name}")
            deleted.append(pod_name)
        except Exception as e:
            logger.warning(f"Failed to delete Pod {pod_name}: {e}")
    else:
        logger.info(f"Deployment {deployment_name} still has running pods. Skipping deletion.")


---///-------

def process_stale_pods(k8s_client, stale_pods):
    pods_api = dynclient.resources.get(api_version="v1", kind="Pod")
    rs_api = dynclient.resources.get(api_version="apps/v1", kind="ReplicaSet")
    dep_api = dynclient.resources.get(api_version="apps/v1", kind="Deployment")
    rc_api = dynclient.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dynclient.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    deleted = []

    for pod in stale_pods:
        namespace = pod["namespace"]
        name = pod["name"]
        try:
            pod_obj = pods_api.get(namespace=namespace, name=name)
            owners = pod_obj.metadata.ownerReferences or []

            if not owners:
                # No owner → delete pod
                pods_api.delete(namespace=namespace, name=name)
                logger.info(f"Deleted pod {name} in {namespace} with no owners.")
                deleted.append({"pod": name, "namespace": namespace})
                continue

            for owner in owners:
                kind = owner.kind
                owner_name = owner.name

                if kind == "ReplicaSet":
                    rs_obj = rs_api.get(namespace=namespace, name=owner_name)
                    handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, name, rs_obj, deleted)

                elif kind == "ReplicationController":
                    rc_obj = rc_api.get(namespace=namespace, name=owner_name)
                    handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, name, rc_obj, deleted)

        except Exception as e:
            logger.error(f"Error handling pod {name} in {namespace}: {e}")

    return deleted


def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted):
    rs_name = rs_obj.metadata.name
    rs_owners = rs_obj.metadata.ownerReferences or []

    if not rs_owners:
        # RS is orphaned → delete RS and pod
        try:
            rs_api.delete(namespace=namespace, name=rs_name)
            logger.info(f"Deleted orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete RS {rs_name}: {e}")

        try:
            pods_api.delete(namespace=namespace, name=pod_name)
            logger.info(f"Deleted pod {pod_name} owned by orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete pod {pod_name}: {e}")

        deleted.append({"replicaset": rs_name, "pod": pod_name, "namespace": namespace})
        return

    for owner in rs_owners:
        if owner.kind == "Deployment":
            deployment_name = owner.name

            # Check if any RS under the deployment has a running pod
            if has_running_pods(k8s_client, namespace, deployment_name, "apps/v1", "Deployment"):
                logger.info(f"Deployment {deployment_name} has other healthy pods. Skipping deletion.")
                return

            # No running pods under this deployment → safe to delete
            try:
                dep_api.delete(namespace=namespace, name=deployment_name)
                logger.info(f"Deleted Deployment {deployment_name}, which removed RS {rs_name} and pod {pod_name}")
                deleted.append({
                    "deployment": deployment_name,
                    "replicaset": rs_name,
                    "pod": pod_name,
                    "namespace": namespace
                })
            except Exception as e:
                logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")
            return


def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted):
    rs_name = rs_obj.metadata.name
    rs_owners = rs_obj.metadata.owner_references or []

    if not rs_owners:
        # RS has no Deployment owner — delete RS and pod
        try:
            rs_api.delete(namespace=namespace, name=rs_name)
            logger.info(f"Deleted RS {rs_name} in {namespace}")
        except ApiException as e:
            if e.status == 404:
                logger.warning(f"RS {rs_name} in {namespace} already deleted.")
            else:
                logger.error(f"Failed to delete RS {rs_name} in {namespace}: {str(e)}")

        try:
            pods_api.delete(namespace=namespace, name=pod_name)
            logger.info(f"Deleted pod {pod_name} in {namespace}")
            deleted.append({"pod": pod_name, "namespace": namespace})
        except ApiException as e:
            logger.error(f"Failed to delete pod {pod_name}: {str(e)}")
        return

    for owner in rs_owners:
        if owner.kind == "Deployment":
            deployment_name = owner.name
            try:
                deployment = dep_api.get(name=deployment_name, namespace=namespace)
                label_selector = ",".join([f"{k}={v}" for k, v in deployment.spec.selector.match_labels.items()])
                rs_list = rs_api.get(namespace=namespace)["items"]

                active_running_pods = []
                for rs in rs_list:
                    labels = rs.spec.selector.match_labels or {}
                    rs_label_selector = ",".join([f"{k}={v}" for k, v in labels.items()])
                    pods = pods_api.get(namespace=namespace, label_selector=rs_label_selector)["items"]
                    for p in pods:
                        if p.status.phase == "Running" and p.metadata.name != pod_name:
                            active_running_pods.append(p.metadata.name)

                if not active_running_pods:
                    # Safe to delete Deployment
                    dep_api.delete(name=deployment_name, namespace=namespace)
                    logger.info(f"Deleted Deployment {deployment_name} in {namespace}")
                    deleted.append({"deployment": deployment_name, "namespace": namespace})
                else:
                    logger.info(f"Deployment {deployment_name} has other active pods: {active_running_pods} — skipping deletion.")
            except ApiException as e:
                logger.error(f"Failed to process Deployment {deployment_name}: {str(e)}")


.....???.....

def process_stale_pods(k8s_client, stale_pods):
    pods_api = dynclient.resources.get(api_version="v1", kind="Pod")
    rs_api = dynclient.resources.get(api_version="apps/v1", kind="ReplicaSet")
    dep_api = dynclient.resources.get(api_version="apps/v1", kind="Deployment")
    rc_api = dynclient.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dynclient.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    deleted = []
    deleted_deployments = set()

    for pod in stale_pods:
        namespace = pod["namespace"]
        name = pod["name"]
        try:
            pod_obj = pods_api.get(namespace=namespace, name=name)
            owners = pod_obj.metadata.ownerReferences or []

            if not owners:
                # No owner → delete pod
                try:
                    pods_api.delete(namespace=namespace, name=name)
                    logger.info(f"Deleted pod {name} in {namespace} with no owners.")
                    deleted.append({"pod": name, "namespace": namespace})
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"Pod {name} in {namespace} already deleted.")
                    else:
                        logger.error(f"Failed to delete pod {name} in {namespace}: {str(e)}")
                continue

            for owner in owners:
                kind = owner.kind
                owner_name = owner.name

                if kind == "ReplicaSet":
                    rs_obj = rs_api.get(namespace=namespace, name=owner_name)
                    handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, name, rs_obj, deleted, deleted_deployments)

                elif kind == "ReplicationController":
                    rc_obj = rc_api.get(namespace=namespace, name=owner_name)
                    handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, name, rc_obj, deleted)

        except Exception as e:
            logger.error(f"Error handling pod {name} in {namespace}: {e}")

---------------- currently I have this --------------------
def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted, deleted_deployments):
    rs_name = rs_obj.metadata.name
    rs_owners = rs_obj.metadata.ownerReferences or []

    if not rs_owners:
        # RS is orphaned → delete RS and pod
        try:
            rs_api.delete(namespace=namespace, name=rs_name)
            logger.info(f"Deleted orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete RS {rs_name}: {e}")

        try:
            pods_api.delete(namespace=namespace, name=pod_name)
            logger.info(f"Deleted pod {pod_name} owned by orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete pod {pod_name}: {e}")

        deleted.append({"replicaset": rs_name, "pod": pod_name, "namespace": namespace})
        return

    for owner in rs_owners:
        if owner.kind == "Deployment":
            deployment_name = owner.name

            if deployment_name in deleted_deployments:
                logger.info(f"Deployment {deployment_name} already deleted earlier. Skipping.")
                return

            # Check if any RS under the deployment has a running pod
            if has_running_pods(k8s_client, namespace, deployment_name, "apps/v1", "Deployment"):
                logger.info(f"Deployment {deployment_name} has other healthy pods. Skipping deletion.")
                return

            # No running pods under this deployment → safe to delete
            try:
                dep_api.delete(namespace=namespace, name=deployment_name)
                logger.info(f"Deleted Deployment {deployment_name}, which removed RS {rs_name} and pod {pod_name}")
                deleted.append({
                    "deployment": deployment_name,
                    "replicaset": rs_name,
                    "pod": pod_name,
                    "namespace": namespace
                })
                deleted_deployments.add(deployment_name)  # Track deletion
            except Exception as e:
                logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")
            return

...............

import time

def timeit(logger=None, label=""):
    def decorator(func):
        def wrapper(*args, **kwargs):
            start = time.time()
            result = func(*args, **kwargs)
            end = time.time()
            msg = f"{label or func.__name__} took {end - start:.2f} seconds"
            if logger:
                logger.info(msg)
            else:
                print(msg)
            return result
        return wrapper
    return decorator


@timeit(logger=logger, label="remove_image_identifier")


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!-------Working code------!!!!!!!!!!!!!!!!!!!!!!!!!!!

def process_stale_pods(k8s_client, stale_pods):
    dynclient = DynamicClient(k8s_client)
    pods_api = dynclient.resources.get(api_version="v1", kind="Pod")
    rs_api = dynclient.resources.get(api_version="apps/v1", kind="ReplicaSet")
    dep_api = dynclient.resources.get(api_version="apps/v1", kind="Deployment")
    rc_api = dynclient.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dynclient.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    deleted = []
    deleted_deployments = set()
    deleted_deploymentconfigs = set()

    for pod in stale_pods:
        namespace = pod["namespace"]
        name = pod["name"]
        try:
            pod_obj = pods_api.get(namespace=namespace, name=name)
            owners = pod_obj.metadata.ownerReferences or []

            if not owners:
                try:
                    pods_api.delete(namespace=namespace, name=name)
                    logger.info(f"Deleted pod {name} in {namespace} with no owners.")
                    deleted.append({"pod": name, "namespace": namespace})
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"Pod {name} in {namespace} already deleted.")
                    else:
                        logger.error(f"Failed to delete pod {name} in {namespace}: {str(e)}")
                continue

            for owner in owners:
                kind = owner.kind
                owner_name = owner.name

                if kind == "ReplicaSet":
                    rs_obj = rs_api.get(namespace=namespace, name=owner_name)
                    handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, name, rs_obj, deleted, deleted_deployments)

                elif kind == "ReplicationController":
                    rc_obj = rc_api.get(namespace=namespace, name=owner_name)
                    handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, name, rc_obj, deleted, deleted_deploymentconfigs)

        except Exception as e:
            logger.error(f"Error handling pod {name} in {namespace}: {e}")


def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted, deleted_deployments):
    rs_name = rs_obj.metadata.name
    rs_owners = rs_obj.metadata.ownerReferences or []

    if not rs_owners:
        try:
            rs_api.delete(namespace=namespace, name=rs_name)
            logger.info(f"Deleted orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete RS {rs_name}: {e}")

        try:
            pods_api.delete(namespace=namespace, name=pod_name)
            logger.info(f"Deleted pod {pod_name} owned by orphaned RS {rs_name}")
        except Exception as e:
            logger.warning(f"Failed to delete pod {pod_name}: {e}")

        deleted.append({"replicaset": rs_name, "pod": pod_name, "namespace": namespace})
        return

    for owner in rs_owners:
        if owner.kind == "Deployment":
            deployment_name = owner.name

            if deployment_name in deleted_deployments:
                logger.info(f"Deployment {deployment_name} already deleted earlier. Skipping.")
                return

            if has_running_pods(k8s_client, namespace, deployment_name, "apps/v1", "Deployment"):
                logger.info(f"Deployment {deployment_name} has other healthy pods. Skipping deletion.")
                return

            try:
                dep_api.delete(namespace=namespace, name=deployment_name)
                logger.info(f"Deleted Deployment {deployment_name}, which removed RS {rs_name} and pod {pod_name}")
                deleted.append({
                    "deployment": deployment_name,
                    "replicaset": rs_name,
                    "pod": pod_name,
                    "namespace": namespace
                })
                deleted_deployments.add(deployment_name)
            except Exception as e:
                logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")
            return


def handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, pod_name, rc_obj, deleted, deleted_deploymentconfigs):
    rc_name = rc_obj.metadata.name
    rc_owners = rc_obj.metadata.ownerReferences or []

    if not rc_owners:
        try:
            rc_api.delete(namespace=namespace, name=rc_name)
            logger.info(f"Deleted orphaned RC {rc_name}")
        except Exception as e:
            logger.warning(f"Failed to delete RC {rc_name}: {e}")

        try:
            pods_api.delete(namespace=namespace, name=pod_name)
            logger.info(f"Deleted pod {pod_name} owned by orphaned RC {rc_name}")
        except Exception as e:
            logger.warning(f"Failed to delete pod {pod_name}: {e}")

        deleted.append({"replicationcontroller": rc_name, "pod": pod_name, "namespace": namespace})
        return

    for owner in rc_owners:
        if owner.kind == "DeploymentConfig":
            dc_name = owner.name

            if dc_name in deleted_deploymentconfigs:
                logger.info(f"DeploymentConfig {dc_name} already deleted earlier. Skipping.")
                return

            if has_running_pods(k8s_client, namespace, dc_name, "apps.openshift.io/v1", "DeploymentConfig"):
                logger.info(f"DeploymentConfig {dc_name} has other healthy pods. Skipping deletion.")
                return

            try:
                dc_api.delete(namespace=namespace, name=dc_name)
                logger.info(f"Deleted DeploymentConfig {dc_name}, which removed RC {rc_name} and pod {pod_name}")
                deleted.append({
                    "deploymentconfig": dc_name,
                    "replicationcontroller": rc_name,
                    "pod": pod_name,
                    "namespace": namespace
                })
                deleted_deploymentconfigs.add(dc_name)
            except Exception as e:
                logger.warning(f"Failed to delete DeploymentConfig {dc_name}: {e}")
            return

77777777777777777777777 ---------try--------77777777777777777777777777777777


def process_stale_pods(k8s_client, stale_pods):
    pods_api = dynclient.resources.get(api_version="v1", kind="Pod")
    rs_api = dynclient.resources.get(api_version="apps/v1", kind="ReplicaSet")
    dep_api = dynclient.resources.get(api_version="apps/v1", kind="Deployment")
    rc_api = dynclient.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dynclient.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    deleted = []
    deleted_deployments = set()
    deleted_deploymentconfigs = set()

    for pod in stale_pods:
        namespace = pod["namespace"]
        name = pod["name"]

        if any(dc_name in name for dc_name in deleted_deploymentconfigs):
            logger.info(f"Skipping pod {name} in {namespace} - belongs to already deleted DeploymentConfig")
            continue
        if any(dep_name in name for dep_name in deleted_deployments):
            logger.info(f"Skipping pod {name} in {namespace} - belongs to already deleted Deployment")
            continue

        try:
            pod_obj = pods_api.get(namespace=namespace, name=name)
        except ApiException as e:
            if e.status == 404:
                logger.warning(f"Pod {name} in {namespace} not found. Skipping...")
            else:
                logger.error(f"Error fetching pod {name} in {namespace}: {e}")
            continue

        owners = pod_obj.metadata.ownerReferences or []

        if not owners:
            try:
                pods_api.delete(namespace=namespace, name=name)
                logger.info(f"Deleted standalone pod {name} in {namespace}")
                deleted.append({"pod": name, "namespace": namespace})
            except ApiException as e:
                if e.status == 404:
                    logger.warning(f"Pod {name} already deleted")
                else:
                    logger.error(f"Failed to delete pod {name} in {namespace}: {str(e)}")
            continue

        for owner in owners:
            kind = owner.kind
            owner_name = owner.name

            if kind == "ReplicaSet":
                try:
                    rs_obj = rs_api.get(namespace=namespace, name=owner_name)
                    handle_replica_set(
                        k8s_client, rs_api, dep_api, pods_api, namespace, name,
                        rs_obj, deleted, deleted_deployments
                    )
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"ReplicaSet {owner_name} in {namespace} not found. Skipping...")
                    else:
                        logger.error(f"Error fetching ReplicaSet {owner_name} in {namespace}: {e}")

            elif kind == "ReplicationController":
                try:
                    rc_obj = rc_api.get(namespace=namespace, name=owner_name)
                    handle_replication_controller(
                        k8s_client, rc_api, dc_api, pods_api, namespace, name,
                        rc_obj, deleted, deleted_deploymentconfigs
                    )
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"ReplicationController {owner_name} in {namespace} not found. Skipping...")
                    else:
                        logger.error(f"Error fetching RC {owner_name} in {namespace}: {e}")


def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted, deleted_deployments):
    owner_refs = rs_obj.metadata.ownerReferences or []
    deployment_name = None

    for owner in owner_refs:
        if owner.kind == "Deployment":
            deployment_name = owner.name
            break

    if deployment_name:
        try:
            dep_obj = dep_api.get(namespace=namespace, name=deployment_name)
            selector = dep_obj.spec.selector.matchLabels
            pods = pods_api.get(namespace=namespace, label_selector=','.join([f"{k}={v}" for k, v in selector.items()]))
            healthy_pods = [p for p in pods.items if p.status.phase == "Running"]
            if not healthy_pods:
                dep_api.delete(namespace=namespace, name=deployment_name)
                rs_api.delete(namespace=namespace, name=rs_obj.metadata.name)
                pods_api.delete(namespace=namespace, name=pod_name)
                deleted_deployments.add(deployment_name)
                logger.info(f"Deleted Deployment {deployment_name}, RS {rs_obj.metadata.name}, and Pod {pod_name}")
        except ApiException as e:
            if e.status == 404:
                logger.warning(f"Deployment {deployment_name} already deleted")
            else:
                logger.error(f"Error handling Deployment {deployment_name}: {e}")


def handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, pod_name, rc_obj, deleted, deleted_deploymentconfigs):
    owner_refs = rc_obj.metadata.ownerReferences or []
    dc_name = None

    for owner in owner_refs:
        if owner.kind == "DeploymentConfig":
            dc_name = owner.name
            break

    if dc_name:
        try:
            dc_obj = dc_api.get(namespace=namespace, name=dc_name)
            selector = dc_obj.spec.selector
            pods = pods_api.get(namespace=namespace, label_selector=','.join([f"{k}={v}" for k, v in selector.items()]))
            healthy_pods = [p for p in pods.items if p.status.phase == "Running"]
            if not healthy_pods:
                dc_api.delete(namespace=namespace, name=dc_name)
                rc_api.delete(namespace=namespace, name=rc_obj.metadata.name)
                pods_api.delete(namespace=namespace, name=pod_name)
                deleted_deploymentconfigs.add(dc_name)
                logger.info(f"Deleted DeploymentConfig {dc_name}, RC {rc_obj.metadata.name}, and Pod {pod_name}")
        except ApiException as e:
            if e.status == 404:
                logger.warning(f"DeploymentConfig {dc_name} already deleted")
            else:
                logger.error(f"Error handling DeploymentConfig {dc_name}: {e}")

------------------------------------- trying  dry run 1---------------------

def process_stale_pods(k8s_client, stale_pods, dry_run=False):
    pods_api = dynclient.resources.get(api_version="v1", kind="Pod")
    rs_api = dynclient.resources.get(api_version="apps/v1", kind="ReplicaSet")
    dep_api = dynclient.resources.get(api_version="apps/v1", kind="Deployment")
    rc_api = dynclient.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dynclient.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    deleted = []
    deleted_deployments = set()
    deleted_deploymentconfigs = set()

    for pod in stale_pods:
        namespace = pod["namespace"]
        name = pod["name"]

        if any(dc_name in name for dc_name in deleted_deploymentconfigs):
            logger.info(f"Skipping pod {name} as it belongs to an already deleted DeploymentConfig.")
            deleted.append({"pod": name, "namespace": namespace, "status": "skipped - part of deleted DeploymentConfig"})
            continue

        try:
            pod_obj = pods_api.get(namespace=namespace, name=name)
        except ApiException as e:
            if e.status == 404:
                logger.warning(f"Pod {name} in {namespace} not found. Possibly already deleted.")
            else:
                logger.error(f"Failed to fetch pod {name} in {namespace}: {e}")
            continue

        owners = pod_obj.metadata.ownerReferences or []

        if not owners:
            if dry_run:
                logger.info(f"[Dry Run] Would delete pod {name} in {namespace} (no owners).")
                deleted.append({"pod": name, "namespace": namespace, "status": "dry-run"})
            else:
                try:
                    pods_api.delete(namespace=namespace, name=name)
                    logger.info(f"Deleted pod {name} in {namespace} with no owners.")
                    deleted.append({"pod": name, "namespace": namespace})
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"Pod {name} in {namespace} already deleted.")
                    else:
                        logger.error(f"Failed to delete pod {name} in {namespace}: {str(e)}")
            continue

        for owner in owners:
            kind = owner.kind
            owner_name = owner.name

            if kind == "ReplicaSet":
                try:
                    rs_obj = rs_api.get(namespace=namespace, name=owner_name)
                    handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, name,
                                       rs_obj, deleted, deleted_deployments, dry_run)
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"ReplicaSet {owner_name} in {namespace} not found. Possibly already deleted.")
                    else:
                        logger.error(f"Failed to fetch ReplicaSet {owner_name} in {namespace}: {e}")

            elif kind == "ReplicationController":
                try:
                    rc_obj = rc_api.get(namespace=namespace, name=owner_name)
                    handle_replication_controller(k8s_client, rc_api, dc_api, pods_api, namespace, name,
                                                  rc_obj, deleted, deleted_deploymentconfigs, dry_run)
                except ApiException as e:
                    if e.status == 404:
                        logger.warning(f"ReplicationController {owner_name} in {namespace} not found. Possibly already deleted.")
                    else:
                        logger.error(f"Failed to fetch ReplicationController {owner_name} in {namespace}: {e}")

    return deleted

def handle_replica_set(k8s_client, rs_api, dep_api, pods_api, namespace, pod_name, rs_obj, deleted, deleted_deployments, dry_run):
    rs_name = rs_obj.metadata.name
    rs_owners = rs_obj.metadata.ownerReferences or []

    if not rs_owners:
        # RS is orphaned → delete RS and pod
        if dry_run:
            logger.info(f"[Dry Run] Would delete orphaned RS {rs_name} and its pod {pod_name} in {namespace}")
        else:
            try:
                rs_api.delete(namespace=namespace, name=rs_name)
                logger.info(f"Deleted orphaned RS {rs_name}")
            except Exception as e:
                logger.warning(f"Failed to delete RS {rs_name}: {e}")

            try:
                pods_api.delete(namespace=namespace, name=pod_name)
                logger.info(f"Deleted pod {pod_name} owned by orphaned RS {rs_name}")
            except Exception as e:
                logger.warning(f"Failed to delete pod {pod_name}: {e}")

        deleted.append({
            "replicaset": rs_name,
            "pod": pod_name,
            "namespace": namespace,
            "status": "dry-run" if dry_run else "deleted"
        })
        return

    for owner in rs_owners:
        if owner.kind == "Deployment":
            deployment_name = owner.name

            if deployment_name in deleted_deployments:
                logger.info(f"Deployment {deployment_name} already deleted earlier. Skipping.")
                deleted.append({
                    "deployment": deployment_name,
                    "replicaset": rs_name,
                    "pod": pod_name,
                    "namespace": namespace,
                    "status": "skipped - already deleted"
                })
                return

            if has_running_pods(k8s_client, namespace, deployment_name, "apps/v1", "Deployment"):
                logger.info(f"Deployment {deployment_name} has other healthy pods. Skipping deletion.")
                deleted.append({
                    "deployment": deployment_name,
                    "replicaset": rs_name,
                    "pod": pod_name,
                    "namespace": namespace,
                    "status": "skipped - healthy pods exist"
                })
                return

            # No running pods under this deployment → safe to delete
            if dry_run:
                logger.info(f"[Dry Run] Would delete Deployment {deployment_name}, RS {rs_name}, and pod {pod_name} in {namespace}")
            else:
                try:
                    dep_api.delete(namespace=namespace, name=deployment_name)
                    logger.info(f"Deleted Deployment {deployment_name}, which removed RS {rs_name} and pod {pod_name}")
                except Exception as e:
                    logger.warning(f"Failed to delete Deployment {deployment_name}: {e}")

            deleted.append({
                "deployment": deployment_name,
                "replicaset": rs_name,
                "pod": pod_name,
                "namespace": namespace,
                "status": "dry-run" if dry_run else "deleted"
            })
            deleted_deployments.add(deployment_name)
            return
from fastapi import Query

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str,
    username: str,
    password: str,
    statuses: List[str] = Query(["CrashLoopBackOff", "ImagePullBackOff", "Error"]),
    namespaces: Optional[List[str]] = Query(None),
    dry_run: bool = Query(False, description="Set to true to simulate deletion without performing it")
):
    try:
        k8s_client = get_openshift_client(openshift_cluster_url, username, password)
        dyn_client = DynamicClient(k8s_client)

        if namespaces is None:
            all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                proj.metadata.name for proj in all_projects.items
                if is_non_prod_namespace(k8s_client, proj.metadata.name)
            ]

        stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
        deleted = process_stale_pods(k8s_client, stale_pods, dry_run=dry_run)

        return {
            "dry_run": dry_run,
            "deleted": deleted
        }

    except Exception as e:
        logger.error(f"Failed to delete stale pods: {str(e)}")
        return {"error": str(e)}

from fastapi import Query, Form
from fastapi.openapi.models import OAuthFlows as OAuthFlowsModel
from pydantic import SecretStr
from typing import List, Optional

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str = Form(
        ...,
        title="OpenShift Cluster URL",
        description="Select the OpenShift cluster to connect",
        json_schema_extra={
            "enum": [
                "https://cluster1.example.com",
                "https://cluster2.example.com",
                "https://cluster3.example.com"
            ]
        }
    ),
    username: str = Form(
        ..., 
        title="Username",
        description="OpenShift username for authentication"
    ),
    password: SecretStr = Form(
        ..., 
        title="Password",
        description="OpenShift password (hidden input)"
    ),
    statuses: List[str] = Form(
        default=["CrashLoopBackOff", "ImagePullBackOff", "Error"],
        title="Pod Statuses",
        description="List of pod statuses to consider as stale"
    ),
    namespaces: Optional[List[str]] = Form(
        default=None,
        title="Namespaces",
        description="Optional list of namespaces to limit stale pod check. Leave empty for all non-prod namespaces."
    ),
    dry_run: bool = Form(
        default=False,
        title="Dry Run",
        description="Enable dry run mode to preview actions without performing actual deletions"
    )
):
    try:
        k8s_client = get_openshift_client(openshift_cluster_url, username, password.get_secret_value())
        dyn_client = DynamicClient(k8s_client)

        if namespaces is None:
            all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                proj.metadata.name for proj in all_projects.items
                if is_non_prod_namespace(k8s_client, proj.metadata.name)
            ]

        stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
        deleted = process_stale_pods(k8s_client, stale_pods, dry_run=dry_run)
        return {"dry_run": dry_run, "deleted": deleted}

    except Exception as e:
        logger.error(f"Failed to delete stale pods: {str(e)}")
        return {"error": str(e)}

from fastapi import FastAPI, Query
from pydantic import SecretStr
from typing import List, Optional
import logging

app = FastAPI()
logger = logging.getLogger(__name__)

@app.delete("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str = Query(
        "https://cluster1.example.com",
        title="OpenShift Cluster URL",
        description="Select the OpenShift cluster to connect"
    ),
    username: str = Query(
        "admin",
        title="Username",
        description="OpenShift username"
    ),
    password: str = Query(
        ...,  # Required
        title="Password",
        description="OpenShift password"
    ),
    statuses: List[str] = Query(
        default=["CrashLoopBackOff", "ImagePullBackOff", "Error"],
        title="Pod Statuses",
        description="List of pod statuses to consider as stale"
    ),
    namespaces: Optional[List[str]] = Query(
        default=None,
        title="Namespaces",
        description="Optional list of namespaces. Leave empty for all non-prod namespaces."
    ),
    dry_run: bool = Query(
        default=False,
        title="Dry Run",
        description="Enable dry run mode to preview actions"
    )
):
    try:
        # Get Kubernetes client
        k8s_client = get_openshift_client(openshift_cluster_url, username, password)
        dyn_client = DynamicClient(k8s_client)

        # Get namespaces (if not provided)
        if namespaces is None:
            all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                proj.metadata.name for proj in all_projects.items
                if is_non_prod_namespace(k8s_client, proj.metadata.name)
            ]

        # List and delete stale pods
        stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
        deleted = process_stale_pods(k8s_client, stale_pods, dry_run=dry_run)

        return {"dry_run": dry_run, "deleted": deleted}

    except Exception as e:
        logger.error(f"Failed to delete stale pods: {str(e)}")
        return {"error": str(e)}


