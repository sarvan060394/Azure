curl -k \
  -H "Authorization: Bearer <access_token>" \
  -H "Accept: application/vnd.docker.distribution.manifest.v2+json" \
  "https://<registry_url>/v2/<repo>/<image_name>/manifests/<tag>"

curl -X GET \
  "https://.com/v2/73717/image/tags/list?n=100&details=true" \
  -H "Accept: application/json" \
  -H "Authorization: Bearer <YOUR_TOKEN>"


import requests
import logging
from fastapi import HTTPException

logger = logging.getLogger(__name__)

def get_image_created_date_from_registry(image, registry_url, registry_token, verify_ssl=False):
    """
    Returns the created date of the given image from the registry.
    Supports only Bearer token authentication.
    """
    try:
        # --- Ensure registry_url has scheme ---
        if not registry_url.startswith("http"):
            registry_url = f"https://{registry_url}"

        # --- Parse image name and tag using existing helper ---
        image_name, tag = parse_image_name(image)
        logger.info(f"Fetched image_name: {image_name}, tag: {tag} for image: {image}")

        # --- Prepare headers ---
        headers = {
            "Accept": "application/vnd.docker.distribution.manifest.v2+json",
            "Authorization": f"Bearer {registry_token}"
        }

        # --- Step 1: Get manifest ---
        manifest_url = f"{registry_url}/v2/{image_name}/manifests/{tag}"
        logger.info(f"Fetching manifest from {manifest_url}")
        resp = requests.get(manifest_url, headers=headers, verify=verify_ssl)

        if resp.status_code != 200:
            logger.error(f"Failed to get manifest for {image}: {resp.text}")
            raise HTTPException(status_code=resp.status_code, detail=f"Manifest fetch failed: {resp.text}")

        manifest_data = resp.json()

        # --- Step 2: Extract config digest ---
        try:
            config_digest = manifest_data["config"]["digest"]
        except KeyError:
            logger.error(f"No config digest found in manifest for {image}")
            raise HTTPException(status_code=500, detail="Invalid manifest format: missing config digest")

        # --- Step 3: Get config blob (contains created date) ---
        config_url = f"{registry_url}/v2/{image_name}/blobs/{config_digest}"
        logger.info(f"Fetching config blob from {config_url}")
        resp = requests.get(config_url, headers=headers, verify=verify_ssl)

        if resp.status_code != 200:
            logger.error(f"Failed to get config blob for {image}: {resp.text}")
            raise HTTPException(status_code=resp.status_code, detail=f"Config blob fetch failed: {resp.text}")

        config_data = resp.json()
        created_date = config_data.get("created")

        if not created_date:
            logger.warning(f"No 'created' field found in config blob for {image}")
            raise HTTPException(status_code=500, detail="No created date found in image config")

        logger.info(f"Image {image} was created at {created_date}")
        return created_date

    except Exception as e:
        logger.exception(f"Exception in get_image_created_date_from_registry for {registry_url}/{image}: {e}")
        raise HTTPException(status_code=500, detail=str(e))


------

# Step: Process images to find older versions
old_images_map = {}

for image in all_images:
    older_images = []
    try:
        logger.info(f"Processing image: {image}")

        # --- Parse image into repo name and tag ---
        image_name, current_tag = parse_image_name(image)  
        logger.debug(f"Parsed image: repo={image_name}, tag={current_tag}")

        # --- Get all tags for the repo with details (one API call) ---
        tags_url = f"{registry_url}/v2/{image_name}/tags/list?n=100&details=true"
        logger.debug(f"Fetching tags from: {tags_url}")

        response = requests.get(tags_url, headers={"Authorization": f"Bearer {registry_token}"}, timeout=30)

        if response.status_code != 200:
            raise Exception(f"Failed to get tags for {image_name}, status: {response.status_code}, body: {response.text}")

        tags_data = response.json()
        logger.debug(f"Tags data received for {image_name}: {tags_data}")

        # --- Find creation date of the current running image ---
        current_created_date = None
        for tag_info in tags_data.get("tags", []):
            if tag_info["name"] == current_tag:
                current_created_date = tag_info.get("created")
                break

        if not current_created_date:
            raise Exception(f"Creation date not found for current tag {current_tag} in {image_name}")

        logger.info(f"Current created date for {image}: {current_created_date}")

        # --- Collect images older than the current one ---
        for tag_info in tags_data.get("tags", []):
            tag_created = tag_info.get("created")
            if tag_created and tag_created < current_created_date:
                older_images.append({
                    "image": f"{image_name}:{tag_info['name']}",
                    "created_date": tag_created
                })

        old_images_map[image] = older_images
        logger.info(f"Older images for {image}: {older_images}")

    except Exception as e:
        logger.error(f"Error processing image {image}: {str(e)}")
        skipped_images.append({
            "image": image,
            "error": f"Failed to get registry info: {str(e)}"
        })

# old_images_map will now have:
# {
#   "running_image:tag": [
#       {"image": "repo/image:oldtag", "created_date": "2023-05-12T08:15:00Z"},
#       ...
#   ]
# }


____________________________________________________________________________________________

import requests
from datetime import datetime
from fastapi import HTTPException

def parse_image_name(image: str) -> tuple[str, str, str]:
    """
    Parses an image string into (image_name, reference_type, reference_value).

    Handles:
        registry.com/repo/image:tag       -> ('repo/image', 'tag', 'v1.0.0')
        registry.com/repo/image@sha256:.. -> ('repo/image', 'digest', 'sha256:...')
        repo/image:tag                    -> ('repo/image', 'tag', 'v1.0.0')
        repo/image                        -> ('repo/image', 'tag', 'latest')
    """
    parts = image.split('/')
    # Remove registry prefix if present
    if len(parts) > 2:
        image_path = '/'.join(parts[1:])
    else:
        image_path = parts[-1]

    if '@' in image_path:
        image_name, digest = image_path.split('@', 1)
        return image_name, 'digest', digest
    elif ':' in image_path:
        image_name, tag = image_path.rsplit(':', 1)
        return image_name, 'tag', tag
    else:
        return image_path, 'tag', 'latest'


def get_image_created_date(registry_url: str, image_name: str, reference: str, token: str) -> datetime:
    """
    Gets the created date of a given image tag/digest from the registry.
    """
    headers = {"Authorization": f"Bearer {token}"}
    manifest_url = f"{registry_url}/v2/{image_name}/manifests/{reference}"

    resp = requests.get(manifest_url, headers=headers)
    if resp.status_code == 404:
        raise HTTPException(status_code=404, detail=f"Image {image_name}:{reference} not found in registry")
    resp.raise_for_status()

    manifest = resp.json()

    # Handle schema v2
    if "history" in manifest:  # schema v1 fallback
        history_item = manifest["history"][0]
        created_str = requests.utils.json.loads(history_item["v1Compatibility"])["created"]
    else:  # schema v2
        config_digest = manifest["config"]["digest"]
        config_url = f"{registry_url}/v2/{image_name}/blobs/{config_digest}"
        config_resp = requests.get(config_url, headers=headers)
        config_resp.raise_for_status()
        created_str = config_resp.json()["created"]

    return datetime.fromisoformat(created_str.replace("Z", "+00:00"))


def get_all_images_before_cutoff(registry_url: str, image_name: str, cutoff_date: datetime, token: str):
    """
    Returns a list of all images created before cutoff_date for the given repository.
    Handles pagination so no image is missed.
    """
    headers = {"Authorization": f"Bearer {token}"}
    older_images = []

    # Pagination
    next_url = f"{registry_url}/v2/{image_name}/tags/list?n=100&details=true"

    while next_url:
        resp = requests.get(next_url, headers=headers)
        if resp.status_code == 404:
            raise HTTPException(status_code=404, detail=f"Repository {image_name} not found in registry")
        resp.raise_for_status()

        data = resp.json()

        for tag_info in data.get("tags", []):
            created_str = tag_info.get("created")
            if created_str:
                created_date = datetime.fromisoformat(created_str.replace("Z", "+00:00"))
                if created_date < cutoff_date:
                    older_images.append({
                        "tag": tag_info["name"],
                        "created": created_str
                    })

        # Handle pagination via Link header
        link_header = resp.headers.get("Link")
        if link_header and 'rel="next"' in link_header:
            next_url = link_header.split("<")[1].split(">")[0]
        else:
            next_url = None

    return older_images


def process_images(all_images, registry_url: str, token: str):
    """
    For each image in all_images:
      1. Get created date.
      2. Find all images created before it.
    """
    for image in all_images:
        try:
            img_name, ref_type, ref_value = parse_image_name(image)
            created_date = get_image_created_date(registry_url, img_name, ref_value, token)
            older_images = get_all_images_before_cutoff(registry_url, img_name, created_date, token)

            print(f"\nImage: {image} (Created: {created_date})")
            print("Older images:")
            for img in older_images:
                print(f"  {img['tag']} - {img['created']}")

        except HTTPException as e:
            print(f"Error for {image}: {e.detail}")
        except Exception as e:
            print(f"Unexpected error for {image}: {e}")




------------


def _fetch_tags_list(url: str, headers: dict):
    """
    Fetch the list of tags for a given image from the registry.
    Always returns parsed JSON (dict) if successful.
    Raises HTTPException on error.
    """
    try:
        resp = requests.get(url, headers=headers, verify=False)
        logger.info(f"_fetch_tags_list: status_code={resp.status_code}, url={url}")

        if resp.status_code == 404:
            logger.error(f"Repository not found at {url}")
            raise HTTPException(status_code=404, detail=f"Repository not found: {url}")

        if resp.status_code == 406:
            logger.error(f"406 Not Acceptable for url: {url}")
            return None  # Caller will decide to break loop

        resp.raise_for_status()

        if "application/json" not in resp.headers.get("Content-Type", ""):
            logger.error(f"Expected JSON but got Content-Type={resp.headers.get('Content-Type')}")
            raise HTTPException(status_code=500, detail="Registry did not return JSON")

        try:
            json_result = resp.json()
            logger.debug(f"_fetch_tags_list: JSON keys={list(json_result.keys())}")
            return json_result
        except ValueError as e:
            logger.error(f"_fetch_tags_list: Failed to parse JSON from {url}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to parse JSON: {str(e)}")

    except requests.RequestException as e:
        logger.error(f"RequestException for url {url}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch tags: {str(e)}")



def get_all_images_before_cutoff(registry_url: str, image_name: str, cutoff_date: datetime, token: str):
    """
    Returns a list of all images created before cutoff_date for the given repository.
    Handles pagination so no image is missed.
    """
    logger.info(f"Getting all images for {image_name} in registry {registry_url} before cutoff date {cutoff_date}")
    headers = {"Authorization": f"Bearer {token}"}
    older_images = []
    next_url = f"{registry_url}/v2/{image_name}/tags/list?n=100&details=true"

    while next_url:
        data = _fetch_tags_list(next_url, headers)

        if data is None:  # 406 error case
            logger.warning(f"Skipping tags fetch due to 406 error for url: {next_url}")
            break

        tags_list = _extract_tags_list(data)

        for tag_info in tags_list:
            result = _parse_tag_info(tag_info, cutoff_date)
            if result:
                older_images.append(result)

        # Extract pagination link if present
        link_header = requests.head(next_url, headers=headers, verify=False).headers.get("Link")
        if link_header and 'rel="next"' in link_header:
            next_url = link_header.split("<")[1].split(">")[0]
            logger.debug(f"Pagination detected, next URL: {next_url}")
        else:
            next_url = None

    logger.info(f"Total older images found for {image_name}: {len(older_images)}")
    return older_images



______________________--AQL_________________________________

import aiohttp
import asyncio
import logging
from datetime import datetime

# ================== CONFIG ==================
ARTIFACTORY_URL = "https://jfrog.example.com/artifactory"
BEARER_TOKEN = "YOUR_BEARER_TOKEN"

all_images = [
    "registry.sdi.com/repo/image1:1.0.0",
    "registry.sdi.com/repo/image2:2.0.0"
]

# ================== LOGGING ==================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

# ================== HELPERS ==================
def parse_image_name(image: str):
    """
    Parses Docker image reference into:
    registry, repo_path, reference_type, reference_value
    Handles :tag and @digest formats.
    """
    try:
        parts = image.split('/')
        registry = parts[0]
        repo_and_ref = '/'.join(parts[1:])

        if '@' in repo_and_ref:
            repo_path, digest = repo_and_ref.split('@', 1)
            return registry, repo_path, "digest", digest
        elif ':' in repo_and_ref:
            repo_path, tag = repo_and_ref.rsplit(':', 1)
            return registry, repo_path, "tag", tag
        else:
            return registry, repo_and_ref, "tag", "latest"
    except Exception as e:
        logger.error(f"Error parsing image name '{image}': {e}")
        raise

# ================== ASYNC FUNCTIONS ==================
async def get_stale_images_aql(all_images, artifactory_url, bearer_token):
    """
    For each image in all_images:
    1. Get creation date (cutoff) of the running image.
    2. Get all older tags before cutoff date using one AQL query.
    """
    stale_images_result = {}
    headers = {"Authorization": f"Bearer {bearer_token}"}

    async with aiohttp.ClientSession(headers=headers) as session:
        for image in all_images:
            try:
                registry, repo_path, ref_type, ref_value = parse_image_name(image)
                logger.info(f"Processing image: {repo_path} ({ref_type}={ref_value})")

                repo_name = repo_path.split('/')[0]  # Artifactory repo
                image_subpath = repo_path.split('/', 1)[1] if '/' in repo_path else ""

                aql_query = f"""
items.find({{
    "repo": "{repo_name}",
    "path": {{"$match": "{image_subpath}/*"}},
    "name": "manifest.json"
}}).include("name", "created", "path")
"""

                async with session.post(f"{artifactory_url}/api/search/aql", data=aql_query) as resp:
                    if resp.status != 200:
                        text = await resp.text()
                        logger.error(f"AQL query failed for {image}: {resp.status} {text}")
                        continue

                    items = (await resp.json()).get("results", [])
                    if not items:
                        logger.warning(f"No tags found for {repo_path}")
                        continue

                # Find cutoff date for running image
                cutoff_date = None
                for item in items:
                    tag_name = item["path"].split('/')[-1]
                    if tag_name == ref_value:
                        cutoff_date = datetime.fromisoformat(item["created"].replace('Z', '+00:00'))
                        break

                if not cutoff_date:
                    logger.warning(f"Cutoff date not found for running image: {image}")
                    continue

                # Find stale tags
                stale_tags = []
                for item in items:
                    created_date = datetime.fromisoformat(item["created"].replace('Z', '+00:00'))
                    if created_date < cutoff_date:
                        stale_tags.append(item["path"].split('/')[-1])

                stale_images_result[image] = stale_tags
                logger.info(f"Found {len(stale_tags)} stale tags for {image}")

            except Exception as e:
                logger.exception(f"Error processing image {image}: {e}")

    return stale_images_result

# ================== MAIN ==================
async def main():
    results = await get_stale_images_aql(all_images, ARTIFACTORY_URL, BEARER_TOKEN)
    for img, stale in results.items():
        logger.info(f"{img} â†’ {stale}")

if __name__ == "__main__":
    asyncio.run(main())

