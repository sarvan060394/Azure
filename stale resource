
def is_non_prod_namespace(namespace: str) -> bool:
    return "prod" not in namespace.lower()
def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    dyn_client = DynamicClient(k8s_client)
    pods = dyn_client.resources.get(api_version='v1', kind='Pod')

    stale_pods = []

    for ns in namespaces:
        if not is_non_prod_namespace(ns):
            logger.warning(f"Skipping namespace '{ns}' as it is considered prod.")
            continue
        try:
            pod_list = pods.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "status": cs.state.waiting.reason
                        })
        except Exception as e:
            logger.error(f"Error retrieving pods from namespace {ns}: {e}")
    return stale_pods
def remove_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
    dyn_client = DynamicClient(k8s_client)
    deleted_pods = []

    for pod in stale_pods:
        try:
            dyn_client.resources.get(api_version="v1", kind="Pod").delete(
                namespace=pod["namespace"], name=pod["name"]
            )
            logger.info(f"Deleted pod {pod['name']} from namespace {pod['namespace']}")
            deleted_pods.append(pod)
        except Exception as e:
            logger.error(f"Error removing pod {pod['name']}: {e}")

    return {"deleted_pods": deleted_pods}
from fastapi import Query
from typing import Optional

DEFAULT_NON_PROD_NAMESPACES = ["dev", "qa", "test", "sandbox"]

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    statuses: List[str] = Query(["CrashLoopBackOff", "Completed"]),
    namespaces: Optional[List[str]] = Query(None)
):
    used_namespaces = namespaces if namespaces else DEFAULT_NON_PROD_NAMESPACES
    return remove_stale_pods(k8s_client, used_namespaces, statuses)


;;;;;;;;;;;;;;;;;;;:::::

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str,
    username: str,
    password: str,
    statuses: List[str] = Query(["CrashLoopBackOff", "Completed"]),
    namespaces: Optional[List[str]] = Query(None)
):
    """
    API to delete stale pods based on user-selected statuses.
    """
    k8s_client = get_openshift_client(openshift_cluster_url, username, password)

    if namespaces is None:
        dyn_client = DynamicClient(k8s_client)
        all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
        namespaces = [
            project.metadata.name
            for project in all_projects.items
            if is_non_prod_namespace(k8s_client, project.metadata.name)
        ]
    
    return remove_stale_pods(k8s_client, namespaces, statuses)

def remove_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
    dyn_client = DynamicClient(k8s_client)
    deleted_pods = []

    for pod in stale_pods:
        try:
            dyn_client.resources.get(api_version="v1", kind="Pod").delete(
                namespace=pod["namespace"], name=pod["name"]
            )
            logger.info(f"Deleted pod {pod['name']} from namespace {pod['namespace']}")
            deleted_pods.append(pod)
        except Exception as e:
            logger.error(f"Error removing pod {pod['name']}: {e}")

    return {"deleted_pods": deleted_pods}


def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    dyn_client = DynamicClient(k8s_client)
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")

    stale_pods = []

    for ns in namespaces:
        if not is_non_prod_namespace(k8s_client, ns):
            continue  # Skip prod namespaces
        
        try:
            pod_list = pods.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "status": cs.state.waiting.reason
                        })
        except Exception as e:
            logger.error(f"Error retrieving pods from namespace {ns}: {e}")
    
    return stale_pods

def is_non_prod_namespace(k8s_client, namespace: str) -> bool:
    """
    Checks if a namespace is labeled as non-prod in OpenShift.
    """
    dyn_client = DynamicClient(k8s_client)
    projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")

    try:
        project = projects.get(name=namespace)
        env_type = project.metadata.labels.get("env_type", "").lower()
        if env_type == "nonprod":
            return True
        logger.warning(f"Namespace {namespace} is labeled as '{env_type}', skipping deletion.")
    except Exception as e:
        logger.error(f"Error retrieving namespace metadata for {namespace}: {e}")

    return False


--------------------*********************-----------------------------------


def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    dyn_client = DynamicClient(k8s_client)
    v1_pods = dyn_client.resources.get(api_version="v1", kind="Pod")
    stale_pods = {}

    for ns in namespaces:
        try:
            pods = v1_pods.get(namespace=ns)
        except Exception as e:
            logger.error(f"Failed to fetch pods in namespace '{ns}': {str(e)}")
            continue

        for pod in pods.items:
            pod_name = pod.metadata.name
            pod_status = pod.status.phase
            container_statuses = pod.status.containerStatuses or []
            deletion_timestamp = pod.metadata.deletionTimestamp

            is_crashloop = any(
                cs.state.waiting and cs.state.waiting.reason == "CrashLoopBackOff"
                for cs in container_statuses
            )
            is_error = any(
                cs.state.waiting and cs.state.waiting.reason == "Error"
                for cs in container_statuses
            )
            is_pending = pod_status == "Pending"
            is_completed = pod_status == "Succeeded" and "Completed" in statuses
            is_terminating = deletion_timestamp is not None

            match = (
                ("CrashLoopBackOff" in statuses and is_crashloop) or
                ("Error" in statuses and is_error) or
                ("Pending" in statuses and is_pending) or
                ("Completed" in statuses and is_completed) or
                ("Terminating" in statuses and is_terminating)
            )

            if match:
                stale_pods.setdefault(ns, []).append(pod_name)
                logger.info(f"Identified stale pod '{pod_name}' with status '{pod_status}' in namespace '{ns}'")

    return stale_pods

from pydantic import BaseModel
from typing import List, Optional

class DeletePodRequest(BaseModel):
    openshift_cluster_url: str
    username: str
    password: str
    namespaces: Optional[List[str]] = []  # default namespaces to search
    statuses: List[str] = ["CrashLoopBackOff", "Completed"]  # default selected


def validate_namespaces_are_nonprod(k8s_client, namespaces: List[str]) -> List[str]:
    dyn_client = DynamicClient(k8s_client)
    v1_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")
    valid_namespaces = []

    for ns in namespaces:
        try:
            project = v1_projects.get(name=ns)
            labels = project.metadata.labels or {}
            if labels.get("env_type") == "nonprod":
                valid_namespaces.append(ns)
            else:
                logger.warning(f"Namespace '{ns}' is labeled as PROD or not labeled at all. Skipping.")
        except Exception as e:
            logger.error(f"Error validating namespace '{ns}': {str(e)}")

    return valid_namespaces
-------(((------

from fastapi import APIRouter
from pydantic import BaseModel
from typing import List, Optional
import logging

router = APIRouter()
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

class DeletePodsRequest(BaseModel):
    openshift_cluster_url: str
    username: str
    password: str
    namespaces: Optional[List[str]] = []
    statuses: List[str]

@router.post("/delete-stale-pods")
def delete_stale_pods(req: DeletePodsRequest):
    logger.info("Starting stale pod deletion request")

    try:
        k8s_client = get_openshift_client(req.openshift_cluster_url, req.username, req.password)
        logger.info("OpenShift client initialized")
    except Exception as e:
        logger.error(f"Failed to authenticate OpenShift client: {str(e)}")
        return {"error": str(e)}

    try:
        validated_namespaces = validate_namespaces_are_nonprod(k8s_client, req.namespaces)
        if not validated_namespaces:
            logger.warning("No nonprod namespaces found in request. Exiting.")
            return {"message": "No nonprod namespaces available for processing"}
    except Exception as e:
        logger.error(f"Namespace validation failed: {str(e)}")
        return {"error": str(e)}

    try:
        result = remove_stale_pods(k8s_client, validated_namespaces, req.statuses)
        logger.info("Stale pod deletion process completed")
        return result
    except Exception as e:
        logger.error(f"Stale pod deletion failed: {str(e)}")
        return {"error": str(e)}

def remove_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
    dyn_client = DynamicClient(k8s_client)
    v1_pods = dyn_client.resources.get(api_version="v1", kind="Pod")

    for ns, pods in stale_pods.items():
        for pod_name in pods:
            try:
                v1_pods.delete(name=pod_name, namespace=ns)
                logger.info(f"Deleted pod '{pod_name}' in namespace '{ns}'")
            except Exception as e:
                logger.error(f"Error deleting pod '{pod_name}' in namespace '{ns}': {str(e)}")

    return {"deleted_pods": stale_pods}



def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]) -> Dict[str, List[str]]:
    dyn_client = DynamicClient(k8s_client)
    v1_pods = dyn_client.resources.get(api_version="v1", kind="Pod")
    stale_pods = {}

    for ns in namespaces:
        try:
            pods = v1_pods.get(namespace=ns)
        except Exception as e:
            logger.error(f"Failed to fetch pods in namespace '{ns}': {str(e)}")
            continue

        for pod in pods.items:
            pod_name = pod.metadata.name
            pod_status = pod.status.phase
            container_statuses = pod.status.containerStatuses or []
            deletion_timestamp = pod.metadata.deletionTimestamp

            is_crashloop = any(
                cs.state.waiting and cs.state.waiting.reason == "CrashLoopBackOff"
                for cs in container_statuses
            )
            is_error = any(
                cs.state.waiting and cs.state.waiting.reason == "Error"
                for cs in container_statuses
            )
            is_pending = pod_status == "Pending"
            is_completed = pod_status == "Succeeded"
            is_terminating = deletion_timestamp is not None

            match = (
                ("CrashLoopBackOff" in statuses and is_crashloop) or
                ("Error" in statuses and is_error) or
                ("Pending" in statuses and is_pending) or
                ("Completed" in statuses and is_completed) or
                ("Terminating" in statuses and is_terminating)
            )

            if match:
                stale_pods.setdefault(ns, []).append(pod_name)
                logger.info(f"Identified stale pod '{pod_name}' in namespace '{ns}'")

    return stale_pods


from openshift.dynamic import DynamicClient

def validate_namespaces_are_nonprod(k8s_client, namespaces: List[str]) -> List[str]:
    dyn_client = DynamicClient(k8s_client)
    v1_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")
    valid_namespaces = []

    for ns in namespaces:
        try:
            project = v1_projects.get(name=ns)
            labels = project.metadata.labels or {}
            if labels.get("env_type") == "nonprod":
                valid_namespaces.append(ns)
            else:
                logger.warning(f"Namespace '{ns}' is not nonprod. Skipping.")
        except Exception as e:
            logger.error(f"Failed to validate namespace '{ns}': {str(e)}")

    return valid_namespaces

------""""------


import logging
from fastapi import FastAPI, Query, HTTPException
from typing import List, Optional
from kubernetes.dynamic import DynamicClient
from kubernetes.client import Configuration

# Configure logger
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str,
    username: str,
    password: str,
    statuses: List[str] = Query(["CrashLoopBackOff", "Completed"]),
    namespaces: Optional[List[str]] = Query(None)
):
    """
    API to delete stale pods based on user-selected statuses.
    """
    logger.info(f"Started deleting stale pods for statuses {statuses} and namespaces {namespaces}")

    try:
        k8s_client = get_openshift_client(openshift_cluster_url, username, password)
    except Exception as e:
        logger.error(f"Error initializing OpenShift client: {e}")
        raise HTTPException(status_code=500, detail="Failed to authenticate with OpenShift cluster")

    if namespaces is None:
        logger.info("No namespaces provided, listing non-prod namespaces.")
        try:
            dyn_client = DynamicClient(k8s_client)
            all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                project.metadata.name
                for project in all_projects.items
                if is_non_prod_namespace(k8s_client, project.metadata.name)
            ]
            logger.info(f"Found namespaces: {namespaces}")
        except Exception as e:
            logger.error(f"Error retrieving namespaces: {e}")
            raise HTTPException(status_code=500, detail="Failed to retrieve namespaces")

    try:
        result = remove_stale_pods(k8s_client, namespaces, statuses)
        logger.info(f"Completed stale pod deletion. Deleted pods: {result['deleted_pods']}")
        return result
    except Exception as e:
        logger.error(f"Error in removing stale pods: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete stale pods")

def remove_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    logger.info(f"Started removing stale pods from namespaces {namespaces} for statuses {statuses}")

    stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
    dyn_client = DynamicClient(k8s_client)
    deleted_pods = []

    for pod in stale_pods:
        try:
            logger.info(f"Attempting to delete pod {pod['name']} from namespace {pod['namespace']}")
            dyn_client.resources.get(api_version="v1", kind="Pod").delete(
                namespace=pod["namespace"], name=pod["name"]
            )
            logger.info(f"Successfully deleted pod {pod['name']} from namespace {pod['namespace']}")
            deleted_pods.append(pod)
        except Exception as e:
            logger.error(f"Error removing pod {pod['name']} from namespace {pod['namespace']}: {e}")

    logger.info(f"Finished removing stale pods. Total deleted: {len(deleted_pods)}")
    return {"deleted_pods": deleted_pods}

def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    logger.info(f"Listing stale pods from namespaces {namespaces} with statuses {statuses}")

    dyn_client = DynamicClient(k8s_client)
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")

    stale_pods = []

    for ns in namespaces:
        if not is_non_prod_namespace(k8s_client, ns):
            logger.info(f"Skipping prod namespace {ns}")
            continue  # Skip prod namespaces

        try:
            pod_list = pods.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "status": cs.state.waiting.reason
                        })
        except Exception as e:
            logger.error(f"Error retrieving pods from namespace {ns}: {e}")

    logger.info(f"Finished listing stale pods. Total found: {len(stale_pods)}")
    return stale_pods

def is_non_prod_namespace(k8s_client, namespace: str) -> bool:
    """
    Checks if a namespace is labeled as non-prod in OpenShift.
    """
    logger.info(f"Checking if namespace {namespace} is non-prod")

    dyn_client = DynamicClient(k8s_client)
    projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")

    try:
        project = projects.get(name=namespace)
        env_type = project.metadata.labels.get("env_type", "").lower()
        if env_type == "nonprod":
            logger.info(f"Namespace {namespace} is non-prod.")
            return True
        logger.warning(f"Namespace {namespace} is labeled as '{env_type}', skipping deletion.")
    except Exception as e:
        logger.error(f"Error retrieving namespace metadata for {namespace}: {e}")

    return False


from fastapi import FastAPI, Query, HTTPException, Form
from typing import List, Optional

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str = Form(..., description="The URL of the OpenShift cluster."),
    username: str = Form(..., description="The username to authenticate with OpenShift."),
    password: str = Form(..., description="The password to authenticate with OpenShift."),
    statuses: List[str] = Query(
        ["CrashLoopBackOff", "Completed"],
        description="The statuses of the pods to consider for deletion. You can select multiple statuses.",
        example=["CrashLoopBackOff", "Completed"]
    ),
    namespaces: Optional[List[str]] = Query(
        None,
        description="The namespaces to check for stale pods. You can select multiple namespaces. If not provided, it will default to non-prod namespaces.",
        example=["namespace1", "namespace2"]
    )
):
    """
    API to delete stale pods based on user-selected statuses and namespaces.
    """
    logger.info(f"Started deleting stale pods for statuses {statuses} and namespaces {namespaces}")

    try:
        k8s_client = get_openshift_client(openshift_cluster_url, username, password)
    except Exception as e:
        logger.error(f"Error initializing OpenShift client: {e}")
        raise HTTPException(status_code=500, detail="Failed to authenticate with OpenShift cluster")

    if namespaces is None:
        logger.info("No namespaces provided, listing non-prod namespaces.")
        try:
            dyn_client = DynamicClient(k8s_client)
            all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                project.metadata.name
                for project in all_projects.items
                if is_non_prod_namespace(k8s_client, project.metadata.name)
            ]
            logger.info(f"Found namespaces: {namespaces}")
        except Exception as e:
            logger.error(f"Error retrieving namespaces: {e}")
            raise HTTPException(status_code=500, detail="Failed to retrieve namespaces")

    try:
        result = remove_stale_pods(k8s_client, namespaces, statuses)
        logger.info(f"Completed stale pod deletion. Deleted pods: {result['deleted_pods']}")
        return result
    except Exception as e:
        logger.error(f"Error in removing stale pods: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete stale pods")




----@@@@@-----


from typing import List, Optional
from datetime import datetime
from fastapi import Query
from openshift.dynamic import DynamicClient
import logging

# Logger setup
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Function to list stale pods based on status
def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    """
    List stale pods based on the user-defined statuses.

    Parameters:
    - k8s_client: OpenShift client to interact with the cluster.
    - namespaces: List of namespaces to search for stale pods.
    - statuses: List of statuses (e.g., 'CrashLoopBackOff', 'ImagePullBackOff') to identify stale pods.

    Returns:
    - A list of stale pods matching the specified statuses.
    """
    dyn_client = DynamicClient(k8s_client)
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")

    stale_pods = []

    for ns in namespaces:
        try:
            pod_list = pods.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "status": cs.state.waiting.reason,
                            "container_name": cs.name,
                            "created_at": pod.metadata.creationTimestamp
                        })
        except Exception as e:
            logger.error(f"Error retrieving pods from namespace {ns}: {e}")

    return stale_pods

# Function to remove stale pods and associated resources like ReplicaSet and DeploymentConfig
def remove_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
    dyn_client = DynamicClient(k8s_client)
    deleted_pods = []

    for pod in stale_pods:
        try:
            # Remove the pod
            dyn_client.resources.get(api_version="v1", kind="Pod").delete(
                namespace=pod["namespace"], name=pod["name"]
            )
            logger.info(f"Deleted pod {pod['name']} from namespace {pod['namespace']}")
            deleted_pods.append(pod)

            # Check if the pod is associated with a ReplicaSet or ReplicaController
            associated_rs = get_associated_replicaset(k8s_client, pod["namespace"], pod["name"])
            if associated_rs:
                delete_associated_replicaset(k8s_client, pod["namespace"], associated_rs)
                
            # Check if the ReplicaSet is associated with a DeploymentConfig or Deployment
            associated_dc = get_associated_deploymentconfig(k8s_client, pod["namespace"], associated_rs)
            if associated_dc:
                delete_associated_deploymentconfig(k8s_client, pod["namespace"], associated_dc)

        except Exception as e:
            logger.error(f"Error removing pod {pod['name']}: {e}")

    return {"deleted_pods": deleted_pods}

# Function to get associated ReplicaSet for a pod
def get_associated_replicaset(k8s_client, namespace: str, pod_name: str):
    dyn_client = DynamicClient(k8s_client)
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")
    
    try:
        pod = pods.get(name=pod_name, namespace=namespace)
        for owner in pod.metadata.ownerReferences:
            if owner.kind == "ReplicaSet":
                return owner.name
    except Exception as e:
        logger.error(f"Error retrieving ReplicaSet for pod {pod_name}: {e}")
    return None

# Function to delete associated ReplicaSet
def delete_associated_replicaset(k8s_client, namespace: str, replicaset_name: str):
    dyn_client = DynamicClient(k8s_client)
    try:
        dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet").delete(
            namespace=namespace, name=replicaset_name
        )
        logger.info(f"Deleted ReplicaSet {replicaset_name} from namespace {namespace}")
    except Exception as e:
        logger.error(f"Error deleting ReplicaSet {replicaset_name}: {e}")

# Function to get associated DeploymentConfig for a ReplicaSet
def get_associated_deploymentconfig(k8s_client, namespace: str, replicaset_name: str):
    dyn_client = DynamicClient(k8s_client)
    deployments = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")

    try:
        deployment_list = deployments.get(namespace=namespace)
        for deployment in deployment_list.items:
            for rs in deployment.spec.replicasets:
                if rs.name == replicaset_name:
                    return deployment.metadata.name
    except Exception as e:
        logger.error(f"Error retrieving DeploymentConfig for ReplicaSet {replicaset_name}: {e}")
    return None

# Function to delete associated DeploymentConfig
def delete_associated_deploymentconfig(k8s_client, namespace: str, deploymentconfig_name: str):
    dyn_client = DynamicClient(k8s_client)
    try:
        dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig").delete(
            namespace=namespace, name=deploymentconfig_name
        )
        logger.info(f"Deleted DeploymentConfig {deploymentconfig_name} from namespace {namespace}")
    except Exception as e:
        logger.error(f"Error deleting DeploymentConfig {deploymentconfig_name}: {e}")

# Function to list stale images
def list_stale_images(k8s_client, image_pull_threshold: int = 30):
    """
    List stale images from the OpenShift registry or Artifactory.
    
    Parameters:
    - k8s_client: OpenShift client to interact with the cluster.
    - image_pull_threshold: Time in days since the image was last pulled to consider it stale.

    Returns:
    - A list of stale images.
    """
    dyn_client = DynamicClient(k8s_client)
    images = dyn_client.resources.get(api_version="image.openshift.io/v1", kind="ImageStream")
    
    stale_images = []

    try:
        # Get all image streams across namespaces
        image_stream_list = images.get()
        for image_stream in image_stream_list.items:
            for tag in image_stream.status.tags:
                image_tag = tag.get("tag")
                image_data = tag.get("items", [])
                if not image_data:
                    continue

                # Check if image has not been used recently
                for image in image_data:
                    image_pull_time = image.get("created", None)
                    if image_pull_time:
                        # Convert the 'created' timestamp to a datetime object
                        created_time = datetime.strptime(image_pull_time, "%Y-%m-%dT%H:%M:%SZ")
                        # Calculate the difference in days between current date and image creation date
                        days_since_pull = (datetime.utcnow() - created_time).days

                        if days_since_pull > image_pull_threshold:
                            stale_images.append({
                                "image_stream": image_stream.metadata.name,
                                "tag": image_tag,
                                "image_digest": image.imageDigest,
                                "created_at": created_time,
                                "days_since_pull": days_since_pull
                            })
                        
    except Exception as e:
        logger.error(f"Error listing stale images: {e}")

    return stale_images


----&&&------deletion logic for dc & RC-----

import logging
from typing import List, Optional
from fastapi import Query
from openshift.dynamic import DynamicClient

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Function to get associated ReplicaSet for a pod
def get_associated_replicaset(k8s_client, namespace: str, pod_name: str):
    dyn_client = DynamicClient(k8s_client)
    replicasets = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    replicaset_list = replicasets.get(namespace=namespace)
    
    for rs in replicaset_list.items:
        for pod in rs.spec.template.spec.containers:
            if pod.name == pod_name:
                logger.info(f"ReplicaSet {rs.metadata.name} is associated with pod {pod_name}.")
                return rs.metadata.name
    return None

# Function to delete associated ReplicaSet
def delete_associated_replicaset(k8s_client, namespace: str, replicaset_name: str):
    dyn_client = DynamicClient(k8s_client)
    try:
        dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet").delete(
            namespace=namespace, name=replicaset_name
        )
        logger.info(f"Deleted ReplicaSet {replicaset_name} from namespace {namespace}.")
    except Exception as e:
        logger.error(f"Error deleting ReplicaSet {replicaset_name}: {e}")

# Function to get associated ReplicaController for a pod
def get_associated_replicacontroller(k8s_client, namespace: str, pod_name: str):
    dyn_client = DynamicClient(k8s_client)
    replica_controllers = dyn_client.resources.get(api_version="v1", kind="ReplicationController")
    rc_list = replica_controllers.get(namespace=namespace)
    
    for rc in rc_list.items:
        for pod in rc.spec.template.spec.containers:
            if pod.name == pod_name:
                logger.info(f"ReplicationController {rc.metadata.name} is associated with pod {pod_name}.")
                return rc.metadata.name
    return None

# Function to delete associated ReplicaController
def delete_associated_replicacontroller(k8s_client, namespace: str, replicacontroller_name: str):
    dyn_client = DynamicClient(k8s_client)
    try:
        dyn_client.resources.get(api_version="v1", kind="ReplicationController").delete(
            namespace=namespace, name=replicacontroller_name
        )
        logger.info(f"Deleted ReplicationController {replicacontroller_name} from namespace {namespace}.")
    except Exception as e:
        logger.error(f"Error deleting ReplicationController {replicacontroller_name}: {e}")

# Function to get associated DeploymentConfig for a pod
def get_associated_deploymentconfig(k8s_client, namespace: str, pod_name: str):
    dyn_client = DynamicClient(k8s_client)
    dc = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")
    dc_list = dc.get(namespace=namespace)
    
    for deployment_config in dc_list.items:
        for pod in deployment_config.spec.template.spec.containers:
            if pod.name == pod_name:
                logger.info(f"DeploymentConfig {deployment_config.metadata.name} is associated with pod {pod_name}.")
                return deployment_config.metadata.name
    return None

# Function to delete associated DeploymentConfig
def delete_associated_deploymentconfig(k8s_client, namespace: str, deploymentconfig_name: str):
    dyn_client = DynamicClient(k8s_client)
    try:
        dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig").delete(
            namespace=namespace, name=deploymentconfig_name
        )
        logger.info(f"Deleted DeploymentConfig {deploymentconfig_name} from namespace {namespace}.")
    except Exception as e:
        logger.error(f"Error deleting DeploymentConfig {deploymentconfig_name}: {e}")

# Function to get associated Deployment for a pod
def get_associated_deployment(k8s_client, namespace: str, pod_name: str):
    dyn_client = DynamicClient(k8s_client)
    deployments = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
    deployment_list = deployments.get(namespace=namespace)
    
    for deployment in deployment_list.items:
        for rs in deployment.spec.template.spec.containers:
            if rs.name == get_associated_replicaset(k8s_client, namespace, pod_name):
                logger.info(f"Deployment {deployment.metadata.name} is associated with pod {pod_name}.")
                return deployment.metadata.name
    return None

# Function to delete associated Deployment
def delete_associated_deployment(k8s_client, namespace: str, deployment_name: str):
    dyn_client = DynamicClient(k8s_client)
    try:
        dyn_client.resources.get(api_version="apps/v1", kind="Deployment").delete(
            namespace=namespace, name=deployment_name
        )
        logger.info(f"Deleted Deployment {deployment_name} from namespace {namespace}.")
    except Exception as e:
        logger.error(f"Error deleting Deployment {deployment_name}: {e}")

# Function to list stale pods based on selected statuses
def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    dyn_client = DynamicClient(k8s_client)
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")

    stale_pods = []

    for ns in namespaces:
        if not is_non_prod_namespace(k8s_client, ns):
            continue  # Skip prod namespaces
        
        try:
            pod_list = pods.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "status": cs.state.waiting.reason
                        })
        except Exception as e:
            logger.error(f"Error retrieving pods from namespace {ns}: {e}")
    
    return stale_pods

# Function to check if the namespace is non-prod
def is_non_prod_namespace(k8s_client, namespace: str) -> bool:
    """
    Checks if a namespace is labeled as non-prod in OpenShift.
    """
    dyn_client = DynamicClient(k8s_client)
    projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")

    try:
        project = projects.get(name=namespace)
        env_type = project.metadata.labels.get("env_type", "").lower()
        if env_type == "nonprod":
            return True
        logger.warning(f"Namespace {namespace} is labeled as '{env_type}', skipping deletion.")
    except Exception as e:
        logger.error(f"Error retrieving namespace metadata for {namespace}: {e}")

    return False

# Main function to remove stale pods and associated resources
def remove_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
    dyn_client = DynamicClient(k8s_client)
    deleted_pods = []

    for pod in stale_pods:
        try:
            logger.info(f"Attempting to delete pod {pod['name']} from namespace {pod['namespace']}.")
            # Remove the pod
            dyn_client.resources.get(api_version="v1", kind="Pod").delete(
                namespace=pod["namespace"], name=pod["name"]
            )
            logger.info(f"Deleted pod {pod['name']} from namespace {pod['namespace']}.")
            deleted_pods.append(pod)

            # Check if the pod is associated with a ReplicaSet or ReplicaController
            associated_rs = get_associated_replicaset(k8s_client, pod["namespace"], pod["name"])
            if associated_rs:
                logger.info(f"Pod {pod['name']} is associated with ReplicaSet {associated_rs}.")
                delete_associated_replicaset(k8s_client, pod["namespace"], associated_rs)
            
            associated_rc = get_associated_replicacontroller(k8s_client, pod["namespace"], pod["name"])
            if associated_rc:
                logger.info(f"Pod {pod['name']} is associated with ReplicaController {associated_rc}.")
                delete_associated_replicacontroller(k8s_client, pod["namespace"], associated_rc)
                
            # Check if the pod is associated with a Deployment or DeploymentConfig
            associated_dc = get_associated_deploymentconfig(k8s_client, pod["namespace"], pod["name"])
            if associated_dc:
                logger.info(f"Pod {pod['name']} is associated with DeploymentConfig {associated_dc}.")
                delete_associated_deploymentconfig(k8s_client, pod["namespace"], associated_dc)

            # Check if the pod is associated with a Deployment
            associated_deployment = get_associated_deployment(k8s_client, pod["namespace"], pod["name"])
            if associated_deployment:
                logger.info(f"Pod {pod['name']} is associated with Deployment {associated_deployment}.")
                delete_associated_deployment(k8s_client, pod["namespace"], associated_deployment)

        except Exception as e:
            logger.error(f"Error removing pod {pod['name']}: {e}")

    return {"deleted_pods": deleted_pods}

-----::::::dc RC delete logic:::-----


from fastapi import FastAPI, Query, HTTPException
from typing import List, Optional
from kubernetes import client, config
from openshift.dynamic import DynamicClient
import logging

app = FastAPI()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Utility: OpenShift client
def get_openshift_client(cluster_url: str, username: str, password: str):
    config.load_kube_config()  # Replace with your auth logic
    return client.ApiClient()


# Utility: Check non-prod namespace
def is_non_prod_namespace(k8s_client, namespace: str) -> bool:
    dyn_client = DynamicClient(k8s_client)
    projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")
    try:
        project = projects.get(name=namespace)
        env_type = project.metadata.labels.get("env_type", "").lower()
        return env_type == "nonprod"
    except Exception as e:
        logger.error(f"Error checking non-prod label for namespace {namespace}: {e}")
        return False


# GET: List stale pods
def list_stale_pods(k8s_client, dyn_client, namespaces: List[str], statuses: List[str]):
    pods = dyn_client.resources.get(api_version="v1", kind="Pod")
    stale_pods = []

    for ns in namespaces:
        if not is_non_prod_namespace(k8s_client, ns):
            continue
        try:
            pod_list = pods.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state and cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "status": cs.state.waiting.reason
                        })
        except Exception as e:
            logger.error(f"Error listing pods in {ns}: {e}")
    return stale_pods


# Main API
@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str,
    username: str,
    password: str,
    statuses: List[str] = Query(["CrashLoopBackOff", "ImagePullBackOff", "Completed"]),
    namespaces: Optional[List[str]] = Query(None)
):
    try:
        k8s_client = get_openshift_client(openshift_cluster_url, username, password)
        dyn_client = DynamicClient(k8s_client)

        if namespaces is None:
            projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                project.metadata.name
                for project in projects.items
                if is_non_prod_namespace(k8s_client, project.metadata.name)
            ]

        stale_pods = list_stale_pods(k8s_client, dyn_client, namespaces, statuses)
        result = remove_stale_pods(k8s_client, dyn_client, stale_pods)
        return {"deleted": result}

    except Exception as e:
        logger.exception("Failed to delete stale pods")
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")


# Remove stale pods logic
def remove_stale_pods(k8s_client, dyn_client, stale_pods: List[dict]):
    deleted = []
    pods_resource = dyn_client.resources.get(api_version="v1", kind="Pod")

    for pod_info in stale_pods:
        ns = pod_info["namespace"]
        pod_name = pod_info["name"]

        try:
            pod = pods_resource.get(name=pod_name, namespace=ns)
            owner = get_owner(pod)

            if not owner:
                pods_resource.delete(name=pod_name, namespace=ns)
                logger.info(f"Deleted standalone pod: {pod_name} in {ns}")
                deleted.append({"namespace": ns, "pod": pod_name})
                continue

            owner_kind = owner["kind"]
            owner_name = owner["name"]

            if owner_kind == "ReplicaSet":
                deleted += handle_replica_set(dyn_client, ns, pod_name, owner_name)
            elif owner_kind == "ReplicationController":
                deleted += handle_replication_controller(dyn_client, ns, pod_name, owner_name)
            else:
                logger.info(f"Pod {pod_name} owner {owner_kind} not handled, deleting pod only.")
                pods_resource.delete(name=pod_name, namespace=ns)
                deleted.append({"namespace": ns, "pod": pod_name})

        except Exception as e:
            logger.error(f"Error deleting pod {pod_name} in {ns}: {e}")
    return deleted


# Helpers
def get_owner(obj):
    if obj.metadata.ownerReferences:
        return obj.metadata.ownerReferences[0].to_dict()
    return None


def handle_replica_set(dyn_client, namespace, pod_name, rs_name):
    deleted = []
    rs_resource = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    pods_resource = dyn_client.resources.get(api_version="v1", kind="Pod")

    pods_resource.delete(namespace=namespace, name=pod_name)
    deleted.append({"namespace": namespace, "pod": pod_name})
    logger.info(f"Deleted pod: {pod_name} in {namespace}")

    rs = rs_resource.get(name=rs_name, namespace=namespace)
    rs_owner = get_owner(rs)

    if rs_owner and rs_owner["kind"] == "Deployment":
        deployment_name = rs_owner["name"]
        if has_other_running_pods(dyn_client, namespace, deployment_name, "Deployment"):
            rs_resource.delete(name=rs_name, namespace=namespace)
            deleted.append({"namespace": namespace, "replica_set": rs_name})
            logger.info(f"Deleted ReplicaSet {rs_name} (Deployment has other running pods)")
        else:
            deployment_resource = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
            deployment_resource.delete(name=deployment_name, namespace=namespace)
            rs_resource.delete(name=rs_name, namespace=namespace)
            deleted.append({
                "namespace": namespace,
                "deployment": deployment_name,
                "replica_set": rs_name
            })
            logger.info(f"Deleted Deployment {deployment_name} and ReplicaSet {rs_name}")
    else:
        rs_resource.delete(name=rs_name, namespace=namespace)
        deleted.append({"namespace": namespace, "replica_set": rs_name})
        logger.info(f"Deleted ReplicaSet {rs_name}")
    return deleted


def handle_replication_controller(dyn_client, namespace, pod_name, rc_name):
    deleted = []
    rc_resource = dyn_client.resources.get(api_version="v1", kind="ReplicationController")
    pods_resource = dyn_client.resources.get(api_version="v1", kind="Pod")

    pods_resource.delete(namespace=namespace, name=pod_name)
    deleted.append({"namespace": namespace, "pod": pod_name})
    logger.info(f"Deleted pod: {pod_name} in {namespace}")

    rc = rc_resource.get(name=rc_name, namespace=namespace)
    rc_owner = get_owner(rc)

    if rc_owner and rc_owner["kind"] == "DeploymentConfig":
        dc_name = rc_owner["name"]
        if has_other_running_pods(dyn_client, namespace, dc_name, "DeploymentConfig"):
            rc_resource.delete(name=rc_name, namespace=namespace)
            deleted.append({"namespace": namespace, "replication_controller": rc_name})
            logger.info(f"Deleted ReplicationController {rc_name} (DC has other running pods)")
        else:
            dc_resource = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")
            dc_resource.delete(name=dc_name, namespace=namespace)
            rc_resource.delete(name=rc_name, namespace=namespace)
            deleted.append({
                "namespace": namespace,
                "deployment_config": dc_name,
                "replication_controller": rc_name
            })
            logger.info(f"Deleted DeploymentConfig {dc_name} and RC {rc_name}")
    else:
        rc_resource.delete(name=rc_name, namespace=namespace)
        deleted.append({"namespace": namespace, "replication_controller": rc_name})
        logger.info(f"Deleted ReplicationController {rc_name}")
    return deleted


def has_other_running_pods(dyn_client, namespace, parent_name, parent_kind):
    pods = dyn_client.resources.get(api_version="v1", kind="Pod").get(namespace=namespace).items
    for pod in pods:
        if pod.status.phase != "Running":
            continue
        if pod.metadata.ownerReferences:
            for owner in pod.metadata.ownerReferences:
                if owner.kind == "ReplicaSet" and parent_kind == "Deployment":
                    rs = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet").get(name=owner.name, namespace=namespace)
                    if rs.metadata.ownerReferences and rs.metadata.ownerReferences[0].name == parent_name:
                        return True
                elif owner.kind == "ReplicationController" and parent_kind == "DeploymentConfig":
                    rc = dyn_client.resources.get(api_version="v1", kind="ReplicationController").get(name=owner.name, namespace=namespace)
                    if rc.metadata.ownerReferences and rc.metadata.ownerReferences[0].name == parent_name:
                        return True
    return False


-----:::check this dc delete llgic code:-------

from fastapi import FastAPI, Query
from kubernetes import client, config
from openshift.dynamic import DynamicClient
from typing import List, Optional
import logging

app = FastAPI()
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def get_openshift_client(cluster_url: str, username: str, password: str):
    configuration = client.Configuration()
    configuration.host = cluster_url
    configuration.verify_ssl = False
    configuration.username = username
    configuration.password = password
    client.Configuration.set_default(configuration)
    return client.ApiClient(configuration)

@app.post("/delete-stale-pods")
def delete_stale_pods_endpoint(
    openshift_cluster_url: str,
    username: str,
    password: str,
    statuses: List[str] = Query(["CrashLoopBackOff", "ImagePullBackOff", "Error"]),
    namespaces: Optional[List[str]] = Query(None)
):
    """
    API to delete stale pods and their associated ReplicaSets/ReplicationControllers and Deployment/DeploymentConfig if needed.
    """
    try:
        k8s_client = get_openshift_client(openshift_cluster_url, username, password)
        dyn_client = DynamicClient(k8s_client)

        if namespaces is None:
            all_projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project").get()
            namespaces = [
                project.metadata.name
                for project in all_projects.items
                if is_non_prod_namespace(k8s_client, project.metadata.name)
            ]

        stale_pods = list_stale_pods(k8s_client, namespaces, statuses)
        deleted = process_stale_pods(k8s_client, stale_pods)
        return {"deleted": deleted}
    except Exception as e:
        logger.error(f"Failed to delete stale pods: {e}")
        return {"error": str(e)}

def is_non_prod_namespace(k8s_client, namespace: str) -> bool:
    dyn_client = DynamicClient(k8s_client)
    projects = dyn_client.resources.get(api_version="project.openshift.io/v1", kind="Project")
    try:
        project = projects.get(name=namespace)
        env_type = project.metadata.labels.get("env_type", "").lower()
        return env_type == "nonprod"
    except Exception as e:
        logger.warning(f"Could not determine env_type for namespace {namespace}: {e}")
        return False

def list_stale_pods(k8s_client, namespaces: List[str], statuses: List[str]):
    dyn_client = DynamicClient(k8s_client)
    pods_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    stale_pods = []

    for ns in namespaces:
        if not is_non_prod_namespace(k8s_client, ns):
            continue
        try:
            pod_list = pods_api.get(namespace=ns)
            for pod in pod_list.items:
                container_statuses = pod.status.containerStatuses or []
                for cs in container_statuses:
                    if cs.state.waiting and cs.state.waiting.reason in statuses:
                        stale_pods.append({
                            "namespace": ns,
                            "name": pod.metadata.name,
                            "uid": pod.metadata.uid,
                            "owner_references": pod.metadata.ownerReferences,
                        })
        except Exception as e:
            logger.error(f"Error listing pods in namespace {ns}: {e}")

    return stale_pods

def process_stale_pods(k8s_client, stale_pods: List[dict]):
    dyn_client = DynamicClient(k8s_client)
    deleted = []

    for pod in stale_pods:
        try:
            logger.info(f"Processing pod: {pod['name']} in namespace {pod['namespace']}")
            owner_refs = pod.get("owner_references", [])
            if not owner_refs:
                delete_pod(dyn_client, pod)
                deleted.append(pod)
                continue

            owner = owner_refs[0]
            if owner.kind == "ReplicaSet":
                deleted.extend(handle_replicaset(dyn_client, pod, owner))
            elif owner.kind == "ReplicationController":
                deleted.extend(handle_replicacontroller(dyn_client, pod, owner))
            else:
                delete_pod(dyn_client, pod)
                deleted.append(pod)
        except Exception as e:
            logger.error(f"Error processing pod {pod['name']}: {e}")

    return deleted

def handle_replicaset(dyn_client, pod, owner):
    rs_api = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    deploy_api = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
    namespace = pod["namespace"]
    deleted = []

    rs = rs_api.get(name=owner.name, namespace=namespace)
    rs_owner = rs.metadata.ownerReferences[0] if rs.metadata.ownerReferences else None

    if rs_owner and rs_owner.kind == "Deployment":
        deployment = deploy_api.get(name=rs_owner.name, namespace=namespace)
        all_rs = rs_api.get(namespace=namespace, label_selector=f"app={deployment.metadata.labels.get('app')}")

        running_pods = get_running_pods_by_deployment(dyn_client, namespace, rs_owner.name)
        if running_pods:
            logger.info(f"Deployment {rs_owner.name} has running pods. Deleting only RS {owner.name} and pod.")
            rs_api.delete(name=owner.name, namespace=namespace)
        else:
            logger.info(f"No running pods found. Deleting Deployment {rs_owner.name}, RS {owner.name}, and pod.")
            deploy_api.delete(name=rs_owner.name, namespace=namespace)
            rs_api.delete(name=owner.name, namespace=namespace)

    delete_pod(dyn_client, pod)
    deleted.append(pod)
    return deleted

def handle_replicacontroller(dyn_client, pod, owner):
    rc_api = dyn_client.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")
    namespace = pod["namespace"]
    deleted = []

    rc = rc_api.get(name=owner.name, namespace=namespace)
    rc_owner = rc.metadata.ownerReferences[0] if rc.metadata.ownerReferences else None

    if rc_owner and rc_owner.kind == "DeploymentConfig":
        dc = dc_api.get(name=rc_owner.name, namespace=namespace)
        running_pods = get_running_pods_by_dc(dyn_client, namespace, rc_owner.name)
        if running_pods:
            logger.info(f"DC {rc_owner.name} has running pods. Deleting only RC {owner.name} and pod.")
            rc_api.delete(name=owner.name, namespace=namespace)
        else:
            logger.info(f"No running pods found. Deleting DC {rc_owner.name}, RC {owner.name}, and pod.")
            dc_api.delete(name=rc_owner.name, namespace=namespace)
            rc_api.delete(name=owner.name, namespace=namespace)

    delete_pod(dyn_client, pod)
    deleted.append(pod)
    return deleted

def delete_pod(dyn_client, pod):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    logger.info(f"Deleting pod {pod['name']} in namespace {pod['namespace']}")
    pod_api.delete(name=pod["name"], namespace=pod["namespace"])

def get_running_pods_by_deployment(dyn_client, namespace: str, deployment_name: str):
    pods_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    pod_list = pods_api.get(namespace=namespace)
    running = []
    for pod in pod_list.items:
        if pod.metadata.ownerReferences:
            for owner in pod.metadata.ownerReferences:
                if owner.kind == "ReplicaSet" and deployment_name in owner.name:
                    if pod.status.phase == "Running":
                        running.append(pod)
    return running

def get_running_pods_by_dc(dyn_client, namespace: str, dc_name: str):
    pods_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    pod_list = pods_api.get(namespace=namespace)
    running = []
    for pod in pod_list.items:
        if pod.metadata.ownerReferences:
            for owner in pod.metadata.ownerReferences:
                if owner.kind == "ReplicationController" and dc_name in owner.name:
                    if pod.status.phase == "Running":
                        running.append(pod)
    return running


---------


def handle_replicaset(dyn_client, pod, owner):
    rs_api = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    deploy_api = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
    namespace = pod["namespace"]
    deleted = []

    rs = rs_api.get(name=owner.name, namespace=namespace)
    rs_owner = rs.metadata.ownerReferences[0] if rs.metadata.ownerReferences else None

    if rs_owner and rs_owner.kind == "Deployment":
        deployment = deploy_api.get(name=rs_owner.name, namespace=namespace)

        # Safely handle missing labels
        label_selector = None
        if deployment.metadata.labels and "app" in deployment.metadata.labels:
            label_selector = f"app={deployment.metadata.labels['app']}"

        if label_selector:
            all_rs = rs_api.get(namespace=namespace, label_selector=label_selector)
        else:
            all_rs = rs_api.get(namespace=namespace)

        running_pods = get_running_pods_by_deployment(dyn_client, namespace, rs_owner.name)
        if running_pods:
            logger.info(f"Deployment {rs_owner.name} has running pods. Deleting only RS {owner.name} and pod.")
            rs_api.delete(name=owner.name, namespace=namespace)
        else:
            logger.info(f"No running pods found. Deleting Deployment {rs_owner.name}, RS {owner.name}, and pod.")
            deploy_api.delete(name=rs_owner.name, namespace=namespace)
            rs_api.delete(name=owner.name, namespace=namespace)

    delete_pod(dyn_client, pod)
    deleted.append(pod)
    return deleted

........


import logging
from kubernetes.dynamic import DynamicClient

logger = logging.getLogger("stale-pod-cleaner")
logging.basicConfig(level=logging.INFO)


def handle_stale_pod(dyn_client, pod):
    namespace = pod["namespace"]
    pod_name = pod["name"]
    pod_obj = get_pod_object(dyn_client, namespace, pod_name)

    if not pod_obj:
        logger.warning(f"Pod {pod_name} not found in namespace {namespace}")
        return []

    owner_refs = pod_obj.metadata.ownerReferences or []
    if not owner_refs:
        logger.info(f"Pod {pod_name} has no owner. Deleting pod.")
        delete_pod(dyn_client, pod)
        return [pod]

    owner = owner_refs[0]
    if owner.kind == "ReplicaSet":
        return handle_replicaset(dyn_client, pod, owner)
    elif owner.kind == "ReplicationController":
        return handle_replicationcontroller(dyn_client, pod, owner)
    else:
        logger.info(f"Pod {pod_name} has unsupported owner kind: {owner.kind}. Skipping.")
        return []


def get_pod_object(dyn_client, namespace, pod_name):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        return pod_api.get(name=pod_name, namespace=namespace)
    except Exception as e:
        logger.error(f"Error retrieving pod {pod_name} in {namespace}: {e}")
        return None


def delete_pod(dyn_client, pod):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pod_api.delete(name=pod["name"], namespace=pod["namespace"])
        logger.info(f"Deleted pod {pod['name']} in namespace {pod['namespace']}")
    except Exception as e:
        logger.error(f"Failed to delete pod {pod['name']} in {pod['namespace']}: {e}")


def handle_replicaset(dyn_client, pod, owner):
    rs_api = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    deploy_api = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
    namespace = pod["namespace"]
    deleted = []

    try:
        rs = rs_api.get(name=owner.name, namespace=namespace)
        rs_owner = rs.metadata.ownerReferences[0] if rs.metadata.ownerReferences else None

        if rs_owner and rs_owner.kind == "Deployment":
            deploy = deploy_api.get(name=rs_owner.name, namespace=namespace)
            all_rs = rs_api.get(namespace=namespace)
            owned_rs = [r for r in all_rs.items if is_owned_by(r, deploy.metadata.uid)]

            if has_running_pods_in_sets(dyn_client, namespace, owned_rs):
                logger.info(f"Deployment {deploy.metadata.name} has running pods. Skipping deletion for pod {pod['name']}")
                return []
            else:
                logger.info(f"No running pods in Deployment {deploy.metadata.name}. Deleting deployment, rs and pod.")
                deploy_api.delete(name=deploy.metadata.name, namespace=namespace)
                rs_api.delete(name=rs.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
        else:
            logger.info(f"ReplicaSet {rs.metadata.name} is not part of a Deployment. Deleting RS and pod.")
            rs_api.delete(name=rs.metadata.name, namespace=namespace)
            delete_pod(dyn_client, pod)
            return [pod]

    except Exception as e:
        logger.error(f"Error handling ReplicaSet for pod {pod['name']}: {e}")
        return []


def handle_replicationcontroller(dyn_client, pod, owner):
    rc_api = dyn_client.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")
    namespace = pod["namespace"]
    deleted = []

    try:
        rc = rc_api.get(name=owner.name, namespace=namespace)
        dc_name = rc.metadata.annotations.get("openshift.io/deployment-config.name")
        if dc_name:
            dc = dc_api.get(name=dc_name, namespace=namespace)
            all_rcs = rc_api.get(namespace=namespace)
            owned_rcs = [
                r for r in all_rcs.items
                if r.metadata.annotations and r.metadata.annotations.get("openshift.io/deployment-config.name") == dc_name
            ]

            if has_running_pods_in_sets(dyn_client, namespace, owned_rcs):
                logger.info(f"DeploymentConfig {dc_name} has running pods. Skipping deletion for pod {pod['name']}")
                return []
            else:
                logger.info(f"No running pods in DC {dc_name}. Deleting DC, RC and pod.")
                dc_api.delete(name=dc_name, namespace=namespace)
                rc_api.delete(name=rc.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
        else:
            logger.info(f"RC {rc.metadata.name} not tied to any DC. Deleting RC and pod.")
            rc_api.delete(name=rc.metadata.name, namespace=namespace)
            delete_pod(dyn_client, pod)
            return [pod]

    except Exception as e:
        logger.error(f"Error handling RC for pod {pod['name']}: {e}")
        return []


def is_owned_by(resource, owner_uid):
    owner_refs = resource.metadata.ownerReferences or []
    return any(o.uid == owner_uid for o in owner_refs)


def has_running_pods_in_sets(dyn_client, namespace, resources):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pods = pod_api.get(namespace=namespace)
        for pod in pods.items:
            for o in pod.metadata.ownerReferences or []:
                if any(r.metadata.name == o.name for r in resources) and pod.status.phase == "Running":
                    return True
    except Exception as e:
        logger.error(f"Error checking for running pods in namespace {namespace}: {e}")
    return False



-------@@@@---)))------


import logging
from kubernetes.dynamic import DynamicClient

logger = logging.getLogger("stale-pod-cleaner")
logging.basicConfig(level=logging.INFO)


def handle_stale_pod(dyn_client, pod):
    namespace = pod["namespace"]
    pod_name = pod["name"]
    pod_obj = get_pod_object(dyn_client, namespace, pod_name)

    if not pod_obj:
        logger.warning(f"Pod {pod_name} not found in namespace {namespace}")
        return []

    owner_refs = pod_obj.metadata.ownerReferences or []
    if not owner_refs:
        logger.info(f"Pod {pod_name} has no owner. Deleting pod.")
        delete_pod(dyn_client, pod)
        return [pod]

    owner = owner_refs[0]
    if owner.kind == "ReplicaSet":
        return handle_replicaset(dyn_client, pod, owner)
    elif owner.kind == "ReplicationController":
        return handle_replicationcontroller(dyn_client, pod, owner)
    else:
        logger.info(f"Pod {pod_name} has unsupported owner kind: {owner.kind}. Skipping.")
        return []


def get_pod_object(dyn_client, namespace, pod_name):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        return pod_api.get(name=pod_name, namespace=namespace)
    except Exception as e:
        logger.error(f"Error retrieving pod {pod_name} in {namespace}: {e}")
        return None


def delete_pod(dyn_client, pod):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pod_api.delete(name=pod["name"], namespace=pod["namespace"])
        logger.info(f"Deleted pod {pod['name']} in namespace {pod['namespace']}")
    except Exception as e:
        logger.error(f"Failed to delete pod {pod['name']} in {pod['namespace']}: {e}")


def handle_replicaset(dyn_client, pod, owner):
    rs_api = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    deploy_api = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
    namespace = pod["namespace"]
    deleted = []

    try:
        rs = rs_api.get(name=owner.name, namespace=namespace)
        rs_owner = rs.metadata.ownerReferences[0] if rs.metadata.ownerReferences else None

        if rs_owner and rs_owner.kind == "Deployment":
            deploy = deploy_api.get(name=rs_owner.name, namespace=namespace)
            all_rs = rs_api.get(namespace=namespace)
            owned_rs = [r for r in all_rs.items if is_owned_by(r, deploy.metadata.uid)]

            if has_running_pods_in_sets(dyn_client, namespace, owned_rs):
                logger.info(f"Deployment {deploy.metadata.name} has running pods. Skipping deletion for pod {pod['name']}")
                return []
            else:
                logger.info(f"No running pods in Deployment {deploy.metadata.name}. Deleting deployment, rs and pod.")
                deploy_api.delete(name=deploy.metadata.name, namespace=namespace)
                rs_api.delete(name=rs.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
        else:
            logger.info(f"ReplicaSet {rs.metadata.name} is not part of a Deployment. Deleting RS and pod.")
            rs_api.delete(name=rs.metadata.name, namespace=namespace)
            delete_pod(dyn_client, pod)
            return [pod]

    except Exception as e:
        logger.error(f"Error handling ReplicaSet for pod {pod['name']}: {e}")
        return []


def handle_replicationcontroller(dyn_client, pod, owner):
    rc_api = dyn_client.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")
    namespace = pod["namespace"]
    deleted = []

    try:
        rc = rc_api.get(name=owner.name, namespace=namespace)
        dc_name = rc.metadata.annotations.get("openshift.io/deployment-config.name")
        if dc_name:
            dc = dc_api.get(name=dc_name, namespace=namespace)
            all_rcs = rc_api.get(namespace=namespace)
            owned_rcs = [
                r for r in all_rcs.items
                if r.metadata.annotations and r.metadata.annotations.get("openshift.io/deployment-config.name") == dc_name
            ]

            if has_running_pods_in_sets(dyn_client, namespace, owned_rcs):
                logger.info(f"DeploymentConfig {dc_name} has running pods. Skipping deletion for pod {pod['name']}")
                return []
            else:
                logger.info(f"No running pods in DC {dc_name}. Deleting DC, RC and pod.")
                dc_api.delete(name=dc_name, namespace=namespace)
                rc_api.delete(name=rc.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
        else:
            logger.info(f"RC {rc.metadata.name} not tied to any DC. Deleting RC and pod.")
            rc_api.delete(name=rc.metadata.name, namespace=namespace)
            delete_pod(dyn_client, pod)
            return [pod]

    except Exception as e:
        logger.error(f"Error handling RC for pod {pod['name']}: {e}")
        return []


def is_owned_by(resource, owner_uid):
    owner_refs = resource.metadata.ownerReferences or []
    return any(o.uid == owner_uid for o in owner_refs)


def has_running_pods_in_sets(dyn_client, namespace, resources):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pods = pod_api.get(namespace=namespace)
        for pod in pods.items:
            for o in pod.metadata.ownerReferences or []:
                if any(r.metadata.name == o.name for r in resources) and pod.status.phase == "Running":
                    return True
    except Exception as e:
        logger.error(f"Error checking for running pods in namespace {namespace}: {e}")
    return False



----with all func-----


import logging
from fastapi import FastAPI, HTTPException
from kubernetes import config
from kubernetes.dynamic import DynamicClient
from kubernetes.client import api_client
from pydantic import BaseModel
from typing import List

# Setup logging
logger = logging.getLogger("stale-pod-cleaner")
logging.basicConfig(level=logging.INFO)

# Initialize FastAPI app
app = FastAPI()

# Load kube config
k8s_client = config.new_client_from_config()
dyn_client = DynamicClient(k8s_client)

class StalePodInput(BaseModel):
    namespaces: List[str]
    statuses: List[str]

def is_nonprod_namespace(namespace_obj):
    labels = namespace_obj.metadata.labels or {}
    return labels.get("env_type") == "nonprod"

def get_nonprod_namespaces(dyn_client):
    v1_ns = dyn_client.resources.get(api_version="v1", kind="Namespace")
    all_ns = v1_ns.get()
    return [ns.metadata.name for ns in all_ns.items if is_nonprod_namespace(ns)]

def get_pod_object(dyn_client, namespace, pod_name):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        return pod_api.get(name=pod_name, namespace=namespace)
    except Exception as e:
        logger.error(f"Error retrieving pod {pod_name} in {namespace}: {e}")
        return None

def delete_pod(dyn_client, pod):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pod_api.delete(name=pod["name"], namespace=pod["namespace"])
        logger.info(f"Deleted pod {pod['name']} in namespace {pod['namespace']}")
    except Exception as e:
        logger.error(f"Failed to delete pod {pod['name']} in {pod['namespace']}: {e}")

def is_owned_by(resource, owner_uid):
    owner_refs = resource.metadata.ownerReferences or []
    return any(o.uid == owner_uid for o in owner_refs)

def has_running_pods_in_sets(dyn_client, namespace, resources):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    try:
        pods = pod_api.get(namespace=namespace)
        for pod in pods.items:
            for o in pod.metadata.ownerReferences or []:
                if any(r.metadata.name == o.name for r in resources) and pod.status.phase == "Running":
                    return True
    except Exception as e:
        logger.error(f"Error checking for running pods in namespace {namespace}: {e}")
    return False

def handle_stale_pod(dyn_client, pod):
    namespace = pod["namespace"]
    pod_name = pod["name"]
    pod_obj = get_pod_object(dyn_client, namespace, pod_name)

    if not pod_obj:
        logger.warning(f"Pod {pod_name} not found in namespace {namespace}")
        return []

    owner_refs = pod_obj.metadata.ownerReferences or []
    if not owner_refs:
        logger.info(f"Pod {pod_name} has no owner. Deleting pod.")
        delete_pod(dyn_client, pod)
        return [pod]

    owner = owner_refs[0]
    if owner.kind == "ReplicaSet":
        return handle_replicaset(dyn_client, pod, owner)
    elif owner.kind == "ReplicationController":
        return handle_replicationcontroller(dyn_client, pod, owner)
    else:
        logger.info(f"Pod {pod_name} has unsupported owner kind: {owner.kind}. Skipping.")
        return []

def handle_replicaset(dyn_client, pod, owner):
    rs_api = dyn_client.resources.get(api_version="apps/v1", kind="ReplicaSet")
    deploy_api = dyn_client.resources.get(api_version="apps/v1", kind="Deployment")
    namespace = pod["namespace"]

    try:
        rs = rs_api.get(name=owner.name, namespace=namespace)
        rs_owner = rs.metadata.ownerReferences[0] if rs.metadata.ownerReferences else None

        if rs_owner and rs_owner.kind == "Deployment":
            deploy = deploy_api.get(name=rs_owner.name, namespace=namespace)
            all_rs = rs_api.get(namespace=namespace)
            owned_rs = [r for r in all_rs.items if is_owned_by(r, deploy.metadata.uid)]

            if has_running_pods_in_sets(dyn_client, namespace, owned_rs):
                logger.info(f"Deployment {deploy.metadata.name} has running pods. Deleting RS {rs.metadata.name} and pod.")
                rs_api.delete(name=rs.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
            else:
                logger.info(f"No running pods in Deployment {deploy.metadata.name}. Deleting Deployment, RS and pod.")
                deploy_api.delete(name=deploy.metadata.name, namespace=namespace)
                rs_api.delete(name=rs.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
        else:
            logger.info(f"ReplicaSet {rs.metadata.name} not part of a Deployment. Deleting RS and pod.")
            rs_api.delete(name=rs.metadata.name, namespace=namespace)
            delete_pod(dyn_client, pod)
            return [pod]
    except Exception as e:
        logger.error(f"Error handling ReplicaSet for pod {pod['name']}: {e}")
        return []

def handle_replicationcontroller(dyn_client, pod, owner):
    rc_api = dyn_client.resources.get(api_version="v1", kind="ReplicationController")
    dc_api = dyn_client.resources.get(api_version="apps.openshift.io/v1", kind="DeploymentConfig")
    namespace = pod["namespace"]

    try:
        rc = rc_api.get(name=owner.name, namespace=namespace)
        dc_name = rc.metadata.annotations.get("openshift.io/deployment-config.name")
        if dc_name:
            dc = dc_api.get(name=dc_name, namespace=namespace)
            all_rcs = rc_api.get(namespace=namespace)
            owned_rcs = [
                r for r in all_rcs.items
                if r.metadata.annotations and r.metadata.annotations.get("openshift.io/deployment-config.name") == dc_name
            ]

            if has_running_pods_in_sets(dyn_client, namespace, owned_rcs):
                logger.info(f"DC {dc_name} has running pods. Deleting RC {rc.metadata.name} and pod.")
                rc_api.delete(name=rc.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
            else:
                logger.info(f"No running pods in DC {dc_name}. Deleting DC, RC and pod.")
                dc_api.delete(name=dc_name, namespace=namespace)
                rc_api.delete(name=rc.metadata.name, namespace=namespace)
                delete_pod(dyn_client, pod)
                return [pod]
        else:
            logger.info(f"RC {rc.metadata.name} not part of a DC. Deleting RC and pod.")
            rc_api.delete(name=rc.metadata.name, namespace=namespace)
            delete_pod(dyn_client, pod)
            return [pod]
    except Exception as e:
        logger.error(f"Error handling RC for pod {pod['name']}: {e}")
        return []

def list_stale_pods(dyn_client, namespaces, statuses):
    pod_api = dyn_client.resources.get(api_version="v1", kind="Pod")
    stale_pods = []

    for ns in namespaces:
        try:
            pods = pod_api.get(namespace=ns)
            for pod in pods.items:
                if pod.status.phase in statuses:
                    stale_pods.append({"name": pod.metadata.name, "namespace": ns})
        except Exception as e:
            logger.error(f"Error listing pods in namespace {ns}: {e}")
    return stale_pods

def process_stale_pods(dyn_client, pods):
    deleted = []
    for pod in pods:
        deleted += handle_stale_pod(dyn_client, pod)
    return deleted

@app.post("/delete-stale-pods")
def delete_stale_pods(input: StalePodInput):
    try:
        nonprod_namespaces = get_nonprod_namespaces(dyn_client)
        target_namespaces = [ns for ns in input.namespaces if ns in nonprod_namespaces]
        stale_pods = list_stale_pods(dyn_client, target_namespaces, input.statuses)
        deleted_pods = process_stale_pods(dyn_client, stale_pods)
        return {"deleted": deleted_pods}
    except Exception as e:
        logger.error(f"Error processing stale pods API: {e}")
        raise HTTPException(status_code=500, detail="Internal error occurred while processing stale pods.")

