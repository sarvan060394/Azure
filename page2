Sure! Here's the full Jira content with a **single Acceptance Criteria**, **description**, and **2 well-scoped subtasks** for the story:

---

### **Story**: Prometheus configuration - Kubes Data Connect monitoring and alerting

---

### **Description**:

The goal of this story is to configure monitoring and alerting for the *Kubes Data Connect* application, which is hosted in OpenShift (OCP).

This involves:

* Identifying and listing all key metrics (as provided by the application team) that require monitoring.
* Writing the required **PromQL expressions** for each metric.
* Creating corresponding **PrometheusRule** configurations with appropriate severity labels (e.g., `critical`, `warning`) to enable effective alerting.

The outcome will be an operational PrometheusRules manifest deployed to the OCP cluster, providing actionable monitoring for the application.

---

### **Acceptance Criteria** (Single, Given-When-Then format):

**Given** the list of metrics provided by the Kubes Data Connect team,
**When** the PrometheusRules are written and deployed with severity-based alerting,
**Then** alerts should be generated correctly in Prometheus when threshold conditions are met for those metrics.

---

### **Subtasks**:

1. **List metrics and write PromQL expressions for alerting**

   * Document all required metrics shared by the application team.
   * Write corresponding PromQL queries for each metric based on recommended thresholds.

2. **Create PrometheusRules YAML and configure alerting severity**

   * Implement the PrometheusRules configuration YAML including labels for `severity`.
   * Deploy the rules to the appropriate OCP namespace and verify that alerts trigger as expected.

---

Let me know if you also need an example PrometheusRule YAML for one of the metrics.


&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Sure! Here's the full Jira content with a **clear description**, a **single Given-When-Then acceptance criteria**, and **2 relevant subtasks** tailored to the story.

---

### **Story**: Prometheus configuration - Kubes Kafka Producer monitoring and alerting

---

### **Description**:

This story aims to configure monitoring and alerting for the **Kubes Kafka Producer** application, which is deployed in OpenShift (OCP).

The team has provided a list of key metrics that must be monitored to ensure application reliability and performance.
The tasks include:

* Reviewing the provided metrics and determining appropriate alert conditions.
* Writing **PromQL** queries for each metric.
* Creating **PrometheusRules** that include appropriate severity labels (`warning`, `critical`, etc.) to facilitate effective alerting.

The final configuration will be deployed to the cluster to enable real-time alerting for the Kafka producer component.

---

### **Acceptance Criteria** (Given-When-Then format):

**Given** the list of Kafka producer metrics provided by the team,
**When** PrometheusRules are created with valid PromQL expressions and severity levels,
**Then** alerts should trigger correctly based on the defined thresholds for each metric.

---

### **Subtasks**:

1. **Review and write PromQL for Kafka producer metrics**

   * Analyze the metrics shared by the application team.
   * Draft PromQL queries for each metric based on defined thresholds for alerting.

2. **Create and deploy PrometheusRules with severity-based alerts**

   * Define the PrometheusRules YAML including all PromQL queries.
   * Add severity labels (`warning`, `critical`) and deploy the rule set in the appropriate OpenShift namespace.
   * Validate alert generation via Prometheus/Alertmanager.

---

Let me know if you'd like to include examples or validation steps.

&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

Certainly! Here's the Jira content with a clear **description**, a **single Given-When-Then acceptance criteria**, and **2 actionable subtasks** for your story:

---

### **Story**: SourceGraph - Explore monitoring options for SourceGraph services

---

### **Description**:

This story involves exploring monitoring capabilities for the **SourceGraph** application, which is a vendor-managed service.
The primary objective is to assess whether **Dynatrace OneAgent** can be installed and configured to monitor SourceGraph services running in the environment.

If OneAgent integration is not feasible due to technical or vendor restrictions, the secondary goal is to evaluate and document alternative monitoring solutions that can offer visibility into SourceGraph's performance, availability, and service health.

This may include native Prometheus integration (if supported), OpenTelemetry, or other compatible observability tools.

---

### **Acceptance Criteria** (Given-When-Then format):

**Given** the requirement to monitor SourceGraph services,
**When** Dynatrace OneAgent feasibility is evaluated and alternative monitoring options are assessed,
**Then** a recommended monitoring approach should be documented with setup steps or next actions based on findings.

---

### **Subtasks**:

1. **Evaluate feasibility of Dynatrace OneAgent with SourceGraph**

   * Investigate if OneAgent can be deployed alongside SourceGraph services.
   * Check for technical compatibility, vendor documentation, and container access requirements.

2. **Identify and document alternative monitoring options**

   * If OneAgent cannot be used, explore other monitoring integrations such as Prometheus, OpenTelemetry, or built-in vendor tools.
   * Document pros, cons, and setup feasibility for each identified alternative.

---

Let me know if you'd like help drafting a summary table or evaluation matrix for your findings.



Here’s a single Acceptance Criteria (AC) written in the Gherkin style, with a description and two clear subtasks for the user story “Image pruner enhancements”:

⸻

Acceptance Criteria

Title: Retrieve running OpenShift images and enrich with CM portal vulnerability data

Description:
As a developer, I want to enhance the image pruner service by creating a GET API that lists all running images in OpenShift resources and enriches the data by fetching vulnerability reports from the CM portal, so that users can identify vulnerable images.

⸻

Given the OpenShift cluster is accessible and CM portal is reachable,
When a user calls the image pruner GET API,
Then the API should return a list of all running container images from OpenShift workloads (like Deployments, StatefulSets, DaemonSets, etc.),
And for each image, the API should include its vulnerability report and mark whether it is vulnerable based on the CM portal response.

⸻

Subtasks
	1.	Subtask 1:
Implement a function to collect all currently running container images from OpenShift resources such as Pods, Deployments, StatefulSets, and DaemonSets.
	2.	Subtask 2:
Integrate with the CM portal API to retrieve vulnerability reports for the collected images and merge the response with the image list.

⸻

Let me know if you want to define edge cases or add error handling as part of the subtasks.




Here is a well-structured Acceptance Criteria (AC), Definition of Done (DoD), and Subtasks for the story:

⸻

Story:

Kubes Producer Splunk Dashboard Setup for Horizon Team

⸻

Acceptance Criteria (AC)

Title: Setup Splunk dashboard for Horizon team to visualize Kubes Producer logs

Description:
As a DevOps engineer, I want to set up a Splunk dashboard for the Horizon team that visualizes logs from the Kubes Producer application, so the team can monitor system behavior, performance, and errors in real time.

⸻

Given that the Kubes Producer application is emitting logs and Splunk is configured to receive logs,
When the logs from the application are indexed into Splunk,
Then a new Splunk dashboard should be created with relevant panels to show log volume, error rates, service behavior trends, and custom search queries specific to Horizon team use cases.

⸻

Definition of Done (DoD)
	•	Splunk index for Kubes Producer logs is confirmed and receiving data.
	•	A Splunk dashboard is created under the Horizon team’s shared folder or project space.
	•	Dashboard contains panels such as:
	•	Log ingestion over time
	•	Error count by severity
	•	Top log sources
	•	Custom queries requested by Horizon team
	•	Dashboard has been reviewed and validated by at least one member of the Horizon team.
	•	Access is provided to all necessary Horizon team members.
	•	Setup documentation is updated in the team Confluence or internal wiki.

⸻

Subtasks
	1.	Subtask 1:
Configure and verify Splunk log ingestion for the Kubes Producer application (including index and sourcetype setup if required).
	2.	Subtask 2:
Create and publish a Splunk dashboard with relevant visualizations and filters tailored to the Horizon team’s operational needs.

⸻

Let me know if you’d like to break this further by environment (e.g., non-prod vs prod), or include alerting as part of the scope.
