Based on the uploaded images, you can structure your deck to explain the current process of creating a certificate in Venafi as follows:

Venafi Certificate Creation Process

1. Overview of Venafi
	‚Ä¢	Venafi is used for certificate management.
	‚Ä¢	Developers generate certificates for development environments.
	‚Ä¢	System engineers generate certificates for production environments.
	‚Ä¢	Certificates are signed by a Bank-approved Certificate Authority.

2. Steps to Create a Certificate in Venafi

Step 1: Log in to Venafi
	‚Ä¢	Access the Venafi portal using valid credentials.

Step 2: Find the Correct Policy Location
	‚Ä¢	Navigate to the policy directory where the certificate needs to be added.
	‚Ä¢	Example: TI - Middleware Engineering ‚Üí Container-test (actual policy depends on the use case).

Step 3: Add a New Server Certificate
	‚Ä¢	Right-click and select ‚ÄúServer Certificate‚Äù to create a new certificate.

Step 4: Populate Certificate Details
	‚Ä¢	Certificate Name and Common Name should be identical.
	‚Ä¢	Click ‚ÄúEdit‚Äù on Management Type and select ‚ÄúEnrollment‚Äù.

Step 5: Configure CSR and Hash Algorithm
	‚Ä¢	Choose Service Generated CSR.
	‚Ä¢	Use SHA-256 as the hash algorithm.

Step 6: Configure Subject Alternate Name (SAN)
	‚Ä¢	Add the Common Name to the SAN field.
	‚Ä¢	If multiple alternate names are required, specify them here.

Step 7: Review and Verify Other Information
	‚Ä¢	Ensure that the certificate type (e.g., G3B) is selected correctly.

Step 8: Fill in Custom Fields
	‚Ä¢	Mandatory fields include:
	‚Ä¢	AIT Number
	‚Ä¢	Certificate Owner DG & NBKID
	‚Ä¢	Certificate Owner Email
	‚Ä¢	Set the Deployment Environment as Non-Production unless specified otherwise.

Step 9: Save and Deploy
	‚Ä¢	Save all the details and submit the request for approval.
	‚Ä¢	Monitor the approval status and download the certificate once issued.

Key Components of Certificate Manager
	‚Ä¢	Certificate Issuance: Automated process for generating certificates.
	‚Ä¢	Renewal Policy: Ensures timely renewal before expiration.
	‚Ä¢	Revocation & Expiry Handling: Manages certificate deactivation when no longer needed.

Certificate Lifecycle in Certificate Manager
	1.	Request Initiation: User requests a certificate in Venafi.
	2.	Approval Workflow: Approval by the designated approvers.
	3.	Certificate Issuance: Certificate is generated and signed.
	4.	Deployment: Certificate is deployed in the required environment.
	5.	Monitoring & Renewal: Expiry tracking and automatic renewal reminders.
	6.	Revocation (if needed): Certificate is revoked in case of a security risk.

Would you like any additional details or modifications?


You can include these additional steps in your deck under ‚ÄúPre-requisites for Venafi Access‚Äù:

Pre-requisites for Venafi Access

Before a user can generate certificates in Venafi, they must complete the following steps:
	1.	Complete Required Trainings
	‚Ä¢	Users must undergo mandatory training to understand Venafi usage and security policies.
	‚Ä¢	Training completion is required before portal access is granted.
	2.	Raise an ARM Request for Policy Tree Access
	‚Ä¢	Users need to submit an Access Request Management (ARM) ticket.
	‚Ä¢	The request must specify the required policy tree to which access is needed.
	‚Ä¢	Approval is required before certificate creation can proceed.

This ensures only authorized personnel can manage certificates and follow security best practices.

Would you like to add any screenshots or diagrams to illustrate the process?



JIRA Story: Unified Repository for Kubernetes Application

Description:
Currently, our source code exists in both our repository and the client‚Äôs repository, where it is built and deployed within their namespace. This setup requires maintaining duplicate codebases and increases complexity.

To streamline the deployment process, we need to build a stable container image and push it to a common registry (Artifactory). The client-side repository will only contain deployment manifests to pull the image from the registry, eliminating the need to maintain source code in multiple places.

This approach will ensure consistency, reduce maintenance overhead, and simplify client-side deployments.

Acceptance Criteria:
	1.	Stable Image Creation:
	‚Ä¢	The application code should be built into a stable container image.
	‚Ä¢	The image should be tagged appropriately (e.g., versioned or latest).
	2.	Push Image to Artifactory:
	‚Ä¢	The container image should be successfully pushed to the common registry (Artifactory).
	‚Ä¢	The image should be accessible by client namespaces.
	3.	Update Client Repository:
	‚Ä¢	The client-side repository should contain only deployment manifests.
	‚Ä¢	The manifests should reference the image from Artifactory instead of building it locally.
	4.	Validation & Testing:
	‚Ä¢	Verify that the application deploys successfully in the client namespace using the new approach.
	‚Ä¢	Ensure the client-side pipeline pulls the correct image version from Artifactory.
	5.	Documentation & Communication:
	‚Ä¢	Update internal documentation to reflect the new image-based deployment approach.
	‚Ä¢	Inform client teams about the changes and provide necessary guidance.

Would you like to include any additional criteria or details specific to your environment?
.....

JIRA Comment to Close the Story:

The implementation for this story has been successfully completed. Below are the key updates made:
	1.	Stable Image Creation & Push to Shared Registry:
	‚Ä¢	We have built a stable container image for the application.
	‚Ä¢	The image has been successfully pushed to the shared registry (Artifactory).
	‚Ä¢	Proper tagging and versioning have been implemented to ensure consistency in deployments.
	2.	Client-Side Repository Updates:
	‚Ä¢	The client-side repository has been updated to contain only the necessary deployment manifests.
	‚Ä¢	The manifests are configured to pull the application image from the shared registry instead of building it locally.
	‚Ä¢	This ensures a streamlined and uniform deployment process across all client environments.
	3.	Jenkins Pipeline for Deployment:
	‚Ä¢	A Jenkinsfile has been added to the client-side repository to automate the deployment of the application in OpenShift.
	‚Ä¢	This allows for a standardized deployment process without requiring manual intervention.
	4.	Configuration Management:
	‚Ä¢	A properties file has been added to the client repository to allow for environment-specific configurations.
	‚Ä¢	This ensures that necessary application parameters can be dynamically set during deployment.
	5.	Validation & Testing:
	‚Ä¢	Verified that the application deploys successfully in the client namespace using the new setup.
	‚Ä¢	Tested the end-to-end deployment process through Jenkins to ensure the correct image is pulled and deployed.

With these changes, the story is now complete, and we can proceed with closing it. Let me know if any further updates or refinements are needed.


Given a pod exists in the cluster
When the delete_stale_pods API is invoked
Then the API should:
	1.	Case 1: If the pod has no owner (i.e., ownerReferences is empty), delete the pod immediately. Ôøº
	2.	Case 2: If the pod is owned by a ReplicaSet or ReplicationController, verify that the owner resource no longer exists. If the owner is absent, delete the pod.
	3.	Case 3: If the pod is owned by a ReplicaSet or ReplicationController that still exists, do not delete the pod. Ôøº

‚∏ª

üìå Definition of Done (DoD)
	‚Ä¢	The delete_stale_pods API correctly identifies and deletes pods without owners.
	‚Ä¢	The API checks for the existence of ReplicaSet or ReplicationController owners before deleting pods. Ôøº
	‚Ä¢	Pods with existing owners are not deleted. Ôøº
	‚Ä¢	Unit tests cover all three cases: no owner, owner deleted, and owner exists.
	‚Ä¢	Integration tests validate the API‚Äôs behavior in a live cluster environment.
	‚Ä¢	Documentation is updated to reflect the enhanced behavior of the delete_stale_pods API.


User Story 1: Delete Pods Solely Owned by a ReplicaSet

As a Kubernetes cluster administrator,
I want the delete_stale_pods API to identify and delete pods that are solely owned by a ReplicaSet,
So that orphaned pods and their associated ReplicaSets are efficiently cleaned up, freeing up cluster resources.

Acceptance Criteria:
	‚Ä¢	The API identifies pods with a single owner reference to a ReplicaSet.
	‚Ä¢	Upon identification, the API deletes both the pod and its associated ReplicaSet.
	‚Ä¢	Deletion operations respect Kubernetes finalizers and owner reference propagation policies to prevent orphaned resources.

‚∏ª

üìù User Story 2: Delete Pods Owned by ReplicaSet and Deployment When Deployment Has No Active Pods

As a Kubernetes cluster administrator,
I want the delete_stale_pods API to delete pods owned by both a ReplicaSet and a Deployment when the Deployment has no active running pods,
So that redundant resources are removed, maintaining cluster efficiency.

Acceptance Criteria:
	‚Ä¢	The API identifies pods with owner references to both a ReplicaSet and a Deployment.
	‚Ä¢	The API checks if the associated Deployment has any active running pods.
	‚Ä¢	If the Deployment has no active running pods:
	‚Ä¢	Delete the ReplicaSet.
	‚Ä¢	Delete the Deployment.
	‚Ä¢	Delete the pod. Ôøº
	‚Ä¢	If the Deployment has active running pods:
	‚Ä¢	Do not delete the pod, ReplicaSet, or Deployment.
